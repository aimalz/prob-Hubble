{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# `scippr` inference module\n",
    "\n",
    "This notebook outlines the `scippr` inference procedure based on hierarchical inference.  To improve performance, all probabilities will be calculated as log probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import astropy.cosmology as cosmology\n",
    "import scipy.optimize as spo\n",
    "import scipy.stats as sps\n",
    "from scipy.stats import norm\n",
    "import scipy.linalg as la\n",
    "import emcee\n",
    "import corner\n",
    "from datetime import datetime\n",
    "import hickle\n",
    "import bisect\n",
    "import daft\n",
    "import cProfile\n",
    "import StringIO\n",
    "import pstats\n",
    "import sys\n",
    "epsilon = sys.float_info.min\n",
    "log_epsilon = sys.float_info.min_exp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inline\n",
    "from matplotlib import rc\n",
    "rc(\"font\", family=\"serif\", size=12)\n",
    "rc(\"text\", usetex=True)\n",
    "colors = 'rbgcymk'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scippr` is based on a probabilistic graphical model, illustrated below.  The model has two types of observables, shown in shaded circles, supernova lightcurves $\\underline{\\ell}_{n}$ and host galaxy photometry $\\vec{m}_{n}$.  The parameters, which are by definition not directly observable, are shown in empty circles.  The latent variables of supernova type $t_{n}$, redshift $z_{n}$, and distance modulus $\\mu_{n}$ are parameters over which we will marginalize, without ever directly inferring them, and while all three of them influence $\\underline{\\ell}_{n}$, only $z_{n}$ affects $\\vec{m}_{n}$ in this model.  In other words, _we currently assume no relationship between supernova type and host galaxy photometry, an assumption we may revisit in the future_.  The selection functions parametrized by $\\vec{P}$, $\\vec{V}$, $\\vec{C}$, and $\\vec{M}$ are known constants of the survey symbolized by dots that influence the possible lightcurves and host galaxy photometry that are included in the sample.  The box indicates that the latent variables and the observables are generated independently $N$ times for each supernova in the sample.  The hyperparameters we would like to estimate are the redshift-dependent supernova type proportions $\\underline{\\phi}$ that determine $t_{n}$ and $z_{n}$ and the cosmological parameters $\\vec{\\theta}$ that relate $z_{n}$ to $\\mu_{n}$, which are shared by all $N$ supernovae in the observed sample.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#initialize the PGM\n",
    "pgm = daft.PGM([6, 4.5], origin=[0, 0])\n",
    "\n",
    "#desired hyperparameters\n",
    "pgm.add_node(daft.Node(\"cosmology\", r\"$\\vec{\\theta}$\", 2., 4.))\n",
    "pgm.add_node(daft.Node(\"dist\", r\"$\\underline{\\phi}$\", 3.5, 4.))\n",
    "#pgm.add_node(daft.Node(\"rates\", r\"$\\vec{R}$\", 3., 5.5, fixed=True))\n",
    "\n",
    "#latent variables/parameters\n",
    "pgm.add_node(daft.Node(\"distance\", r\"$\\mu_{n}$\", 2., 2.5))\n",
    "pgm.add_node(daft.Node(\"redshift\", r\"$z_{n}$\", 3., 3.))\n",
    "pgm.add_node(daft.Node(\"type\", r\"$t_{n}$\", 4., 2.5))\n",
    "\n",
    "#data\n",
    "pgm.add_node(daft.Node(\"lightcurve\", r\"$\\underline{\\ell}_{n}$\", 2.5, 1., observed=True))\n",
    "pgm.add_node(daft.Node(\"photometry\", r\"$\\vec{m}_{n}$\", 3.5, 1., observed=True))\n",
    "\n",
    "#known constant parameters\n",
    "pgm.add_node(daft.Node(\"lightcurve selection\", r\"$\\vec{P}, \\vec{V}, \\vec{C}$\", 1., 1.75, fixed=True))\n",
    "pgm.add_node(daft.Node(\"photometry selection\", r\"$\\vec{M}$\", 5., 1.75, fixed=True))\n",
    "\n",
    "# Add in the edges.\n",
    "pgm.add_edge(\"dist\", \"type\")\n",
    "pgm.add_edge(\"cosmology\", \"distance\")\n",
    "pgm.add_edge(\"dist\", \"redshift\")\n",
    "pgm.add_edge(\"redshift\", \"distance\")\n",
    "#pgm.add_edge(\"distance\", \"photometry\")\n",
    "pgm.add_edge(\"distance\", \"lightcurve\")\n",
    "pgm.add_edge(\"redshift\", \"photometry\")\n",
    "pgm.add_edge(\"redshift\", \"lightcurve\")\n",
    "pgm.add_edge(\"type\", \"lightcurve\")\n",
    "pgm.add_edge(\"photometry selection\", \"photometry\")\n",
    "pgm.add_edge(\"lightcurve selection\", \"lightcurve\")\n",
    "\n",
    "# plates\n",
    "pgm.add_plate(daft.Plate([1.5, 0.5, 3., 3.], label=r\"$n = 1, \\cdots, N$\"))\n",
    "\n",
    "# Render and save.\n",
    "pgm.render()\n",
    "pgm.figure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probabilistic graphical model will guide us in characterizing the posterior distribution $\\ln[p(\\vec{\\theta}, \\underline{\\phi} | \\{\\underline{\\ell}_{n}, \\vec{m}_{n}\\}_{N}, \\vec{P}, \\vec{V}, \\vec{C}, \\vec{M})]$ of the hyperparameters given the observed data, but as a sneak preview, this is the form it will take:\n",
    "\n",
    "\\begin{align}\n",
    "\\ln[p(\\vec{\\theta}, \\underline{\\phi} | \\{\\underline{\\ell}_{n}, \\vec{m}_{n}\\}_{N}, \\vec{P}, \\vec{V}, \\vec{C}, \\vec{M})] \\propto & \\ln[p(\\vec{\\theta}, \\underline{\\phi})]\\\\\n",
    "& +\\sum_{n}^{N}\\ \\ln[\\iiint\\ \\exp[\\ln[p(t_{n}, z_{n}, \\mu_{n} | \\underline{\\ell}_{n}, \\vec{m}_{n}, \\underline{\\xi}, \\vec{P}, \\vec{V}, \\vec{C}, \\vec{M})]+\\ln[p(t_{n}, z_{n}, \\mu_{n} | \\vec{\\theta}, \\underline{\\phi})]\\\\\n",
    "\\ \\ \\ \\ \\ & -\\ln[p(t_{n}, z_{n}, \\mu_{n} | \\underline{\\xi})]]\\ d\\mu_{n}\\ dz_{n}\\ dt_{n}]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the parameter space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we set up the parameter space for the latent variables of supernova type $t$, redshift $z$, and distance modulus $\\mu$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../data.hkl', 'r+') as in_file:\n",
    "    sim_info = hickle.load(in_file)\n",
    "    \n",
    "types = sim_info['types']\n",
    "n_types = len(types)\n",
    "\n",
    "z_bins = sim_info['z_bins']\n",
    "z_difs = z_bins[1:] - z_bins[:-1]\n",
    "z_mids = (z_bins[1:] + z_bins[:-1]) / 2.\n",
    "n_zs = len(z_difs)\n",
    "\n",
    "mu_bins = sim_info['mu_bins']\n",
    "mu_difs = mu_bins[1:] - mu_bins[:-1]\n",
    "mu_mids = (mu_bins[1:] + mu_bins[:-1]) / 2.\n",
    "n_mus = len(mu_difs)\n",
    "\n",
    "def safe_log(arr, threshold=epsilon):\n",
    "    shape = np.shape(arr)\n",
    "    flat = arr.flatten()\n",
    "    logged = np.log(np.array([max(a, threshold) for a in flat])).reshape(shape)\n",
    "    return logged\n",
    "\n",
    "def reg_vals(arr, threshold=log_epsilon):\n",
    "    arr[arr < threshold] = threshold\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing the log-interim posteriors and interim hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scippr` requires inputs in the form of catalogs $\\{\\ln[p(t_{n}, z_{n}, \\mu_{n} | \\underline{\\ell}_{n}, \\vec{m}_{n}, \\underline{\\xi}, \\vec{P}, \\vec{V}, \\vec{C}, \\vec{M})]\\}_{N}$ of interim log-posteriors expressed as `3D` arrays constituting probabilities over $t_{n}$, $z_{n}$, and $\\mu_{n}$, enabling rapid computation of the log-posterior $\\ln[p(\\underline{\\phi}, \\vec{\\theta} | \\{\\underline{\\ell}_{n}, \\vec{m}_{n}\\}_{N}, \\vec{P}, \\vec{V}, \\vec{C}, \\vec{M})$ over the hyperparameters $\\underline{\\phi}$ and $\\vec{\\theta}$ of scientific interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lninterimposteriors = sim_info['interim ln posteriors']\n",
    "(n_SNe, n_types, n_zs, n_mus) = np.shape(lninterimposteriors)\n",
    "\n",
    "# these are going to get a lot narrower\n",
    "fig = plt.figure(figsize=(n_types * len(colors), n_SNe * len(colors)))\n",
    "p = 0\n",
    "for s in range(n_SNe)[:len(colors)]:\n",
    "    for t in range(n_types):\n",
    "        p += 1\n",
    "        plt.subplot(n_SNe, n_types, p)\n",
    "        plt.pcolormesh(z_mids, mu_mids, lninterimposteriors[s][t].T, cmap='viridis')#, vmin = 0., vmax = 3.)\n",
    "        plt.colorbar()\n",
    "        plt.xlabel(r'$z$')\n",
    "        plt.ylabel(r'$\\mu$')\n",
    "        plt.axis([z_bins[0], z_bins[-1], mu_bins[0], mu_bins[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interim posteriors must always come with interim hyperparameters $\\underline{\\xi}$ specifying $p(t, z, \\mu)$ used to produce them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "interim_ln_prior = sim_info['interim ln prior']\n",
    "assert np.isclose(np.sum(np.exp(interim_ln_prior) * z_difs[np.newaxis, :, np.newaxis] * mu_difs[np.newaxis, np.newaxis, :]), 1.)\n",
    "\n",
    "fig = plt.figure(figsize=(n_types*len(colors), len(colors)))\n",
    "for t in range(n_types):\n",
    "    plt.subplot(1, n_types, t+1)\n",
    "    plt.pcolormesh(z_mids, mu_mids, interim_ln_prior[t].T, cmap='viridis')#, vmin = 0., vmax = 3.)\n",
    "    plt.title('SN '+types[t]+' log interim prior distribution')\n",
    "    plt.xlabel(r'$z$')\n",
    "    plt.ylabel(r'$\\mu$')\n",
    "    plt.axis([z_bins[0], z_bins[-1], mu_bins[0], mu_bins[-1]])\n",
    "    plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the log-hyperprior probability distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in any Bayesian inference, we must choose a hyperprior distribution over the hyperparameters $\\vec{\\theta}$ and $\\underline{\\phi}$ that we wish to estimate.  At this stage we will only attempt to infer $\\vec{\\theta}=(H_{0}, \\Omega_{m0})$.  When we evaluate the log-hyperprior probability $\\ln[p(\\vec{\\theta}, \\underline{\\phi})]$, we will then be evaluating the log-probability of the hyperprior distribution at the given values of the hyperparameters $\\vec{\\theta}$ and $\\underline{\\phi}$.  Thus, the log-hyperprior probability is a scalar as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lnprior(hyperparameters):\n",
    "    cosmo_hyperparameters = hyperparameters#['theta']\n",
    "#     dist_hyperparameters = hyperparameters['phi']\n",
    "    cosmo_prior_prob = prior_cosmo_dist.logpdf(cosmo_hyperparameters)\n",
    "#     dist_prior_prob = prior_n_of_z.logpdf(dist_hyperparameters.flatten())\n",
    "    return cosmo_prior_prob # + dist_prior_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We will make the assumption that $H_{0}$ and $\\Omega_{m0}$ are independent, i.e. $p(\\vec{\\theta}) = p(H_{0})\\ p(\\Omega_{m0})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class indie_dist(object):\n",
    "    def __init__(self, in_dists):\n",
    "        self.dists = in_dists\n",
    "        self.n_dists = len(self.dists)\n",
    "    def rvs(self, n_samps = 1):\n",
    "        samps = []\n",
    "        for d in range(self.n_dists):\n",
    "            samps.append(self.dists[d].rvs(n_samps))\n",
    "        return np.array(samps).T\n",
    "    def logpdf(self, locs):\n",
    "        n_items = len(locs)\n",
    "        lnprob = 0\n",
    "        for d in range(self.n_dists):\n",
    "            lnprob += self.dists[d].logpdf(locs[d])\n",
    "        return lnprob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make the choice to set hyperprior distributions to be Gaussian for $H_{0}$ and truncated normal for $\\Omega_{m0}$ (so it must be a fraction) with means equal to the WMAP values for these parameters and standard deviations equal to the $3\\sigma$ errors on those measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_sigmas = 2.\n",
    "\n",
    "# WMAP, with 10 * errors so we can see what's going on in crappy plots\n",
    "wmap_H0 = 70.0\n",
    "delta_H0 = 2.2 * n_sigmas\n",
    "[H0_mean, H0_std] = [wmap_H0, delta_H0]\n",
    "min_H0, max_H0 = 50., 90.\n",
    "H0_range = max_H0 - min_H0\n",
    "H0_low, H0_high = (min_H0 - H0_mean) / H0_std, (max_H0 - H0_mean) / H0_std\n",
    "H0_dist = sps.norm(H0_mean, H0_std)#sps.truncnorm(H0_low, H0_high, loc = H0_mean, scale = H0_std)\n",
    "\n",
    "wmap_Om0 = 1. - 0.721\n",
    "delta_Om0 = 0.025 * n_sigmas\n",
    "[Om0_mean, Om0_std] = [wmap_Om0, delta_Om0]\n",
    "min_Om0, max_Om0 = 0., 1.\n",
    "Om0_low, Om0_high = (min_Om0 - Om0_mean) / Om0_std, (max_Om0 - Om0_mean) / Om0_std\n",
    "Om0_dist = sps.norm(Om0_mean, Om0_std)#sps.truncnorm(Om0_low, Om0_high, loc = Om0_mean, scale = Om0_std)\n",
    "\n",
    "prior_cosmo_hyperparams = np.array([H0_dist.rvs(), Om0_dist.rvs()])\n",
    "cosmo_param_names = [r'$H_{0}$', r'$\\Omega_{m0}$']\n",
    "n_cosmo_hyperparams = len(prior_cosmo_hyperparams)\n",
    "prior_cosmo_dist = indie_dist([H0_dist, Om0_dist])\n",
    "prior_cosmo = cosmology.FlatLambdaCDM(H0=prior_cosmo_hyperparams[0], Om0=prior_cosmo_hyperparams[1])\n",
    "\n",
    "def param_check(params):\n",
    "    if params[0] > min_H0 and params[0] < max_H0 and params[1] > min_Om0 and params[1] < max_Om0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# # prior on phi is gaussian about flat in log space\n",
    "# prior_mean = np.ones((n_types, n_zs)) / n_types\n",
    "# prior_mean /= np.sum(prior_mean * z_difs[np.newaxis, :])\n",
    "# assert np.isclose(np.sum(prior_mean * z_difs[np.newaxis, :]), 1.)\n",
    "# prior_mean = safe_log(prior_mean.flatten())\n",
    "# prior_sigmas = np.array([0.5, 1., 0.25])\n",
    "# prior_covariance = la.block_diag(prior_sigmas[0] * np.eye(n_zs), prior_sigmas[1] * np.eye(n_zs), prior_sigmas[2] * np.eye(n_zs))\n",
    "# # prior_n_of_z_hyperparams = [prior_mean, prior_sigmas]\n",
    "# prior_n_of_z_dist = sps.multivariate_normal(mean = prior_mean, cov = prior_covariance)\n",
    "\n",
    "H0_grid = 50. + 50. * np.arange(0., 1., 0.01)\n",
    "Om0_grid = np.arange(0., 1., 0.01)\n",
    "prior_space = np.zeros((100, 100))\n",
    "for i in range(100):\n",
    "    for j in range(100):\n",
    "        prior_space[i][j] = lnprior(np.array([H0_grid[i], Om0_grid[j]]))\n",
    "plt.pcolormesh(H0_grid, Om0_grid, prior_space.T, cmap='viridis')#, vmin = 0., vmax = 3.)\n",
    "plt.colorbar()\n",
    "plt.xlabel(r'$H_{0}$')\n",
    "plt.ylabel(r'$\\Omega_{m0}$')\n",
    "plt.axis([H0_grid[0], H0_grid[-1], Om0_grid[0], Om0_grid[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the log-hyperlikelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log-hyperlikelihood $\\ln[p(t_{n}, z_{n}, \\mu_{n} | \\vec{\\theta}, \\underline{\\phi})] = \\ln[p(t_{n}, z_{n} | \\underline{\\phi})]+\\ln[p(\\mu_{n} | z_{n}, \\vec{\\theta})]$ is the sum of two terms separable in the hyperparameters.  In our parametrization, the first is derived from a constant lookup table that can be neglected for now and the second is a $\\delta$ function located at the `cosmo.distmod()` function evaluated at the given redshift where `cosmo` is defined using the cosmological parameters in $\\vec{\\theta}$.  We will at this stage need to introduce a function mapping redshifts into distance moduli under a given cosmology.  We note that we will be using some particular values of the log-hyperlikelihood repeatedly so will define them as constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lninterimhyperlikelihood = interim_ln_prior\n",
    "flat_hyperlikelihood = np.ones(np.shape(interim_ln_prior)) * epsilon\n",
    "# flat_hyperlikelihood /= np.sum(flat_hyperlikelihood * z_difs[np.newaxis, :, np.newaxis] * mu_difs[np.newaxis, :, np.newaxis])\n",
    "# assert np.isclose(np.sum(flat_hyperlikelihood * z_difs[np.newaxis, :, np.newaxis] * mu_difs[np.newaxis, :, np.newaxis]), 1.)\n",
    "flat_lnhyperlikelihood = safe_log(flat_hyperlikelihood)#log_epsilon * np.ones(np.shape(interim_ln_prior))\n",
    "\n",
    "def lnhyperlikelihood(hyperparameters):\n",
    "    cosmo_hyperparameters = hyperparameters#['theta']\n",
    "#     dist_hyperparameters = hyperparameters['phi']\n",
    "    if param_check(hyperparameters):\n",
    "        sample_cosmo = cosmology.FlatLambdaCDM(H0=cosmo_hyperparameters[0], Om0=cosmo_hyperparameters[1])\n",
    "        delta = mu_binner(sample_cosmo.distmod(z_mids).value)\n",
    "        delta /= np.sum(delta * z_difs[np.newaxis, :, np.newaxis] * mu_difs[np.newaxis, np.newaxis, :] * np.ones((n_types))[:, np.newaxis, np.newaxis] / n_types)\n",
    "        assert np.isclose(np.sum(delta * z_difs[np.newaxis, :, np.newaxis] * mu_difs[np.newaxis, :, np.newaxis] * np.ones((n_types))[:, np.newaxis, np.newaxis] / n_types), 1.)\n",
    "        lnprob = safe_log(delta) # + dist_hyperparameters[:, :, np.newaxis]\n",
    "    else:\n",
    "        lnprob = flat_lnhyperlikelihood\n",
    "    return lnprob\n",
    "\n",
    "def mu_binner(mus):\n",
    "    matrix = []\n",
    "    for mu in mus:\n",
    "        vector = np.zeros(n_mus)\n",
    "        ind = bisect.bisect(mu_bins[:-1], mu) - 1\n",
    "        vector[ind] += 1.\n",
    "        matrix.append(vector)\n",
    "    return np.array([matrix] * n_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the log-posterior probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full log-posterior probability takes the following form:\n",
    "\\begin{equation}\n",
    "\\ln[p(\\vec{\\theta}, \\underline{\\phi} | \\{\\underline{\\ell}_{n}, \\vec{m}_{n}\\}_{N})] \\propto \\ln[p(\\vec{\\theta}, \\underline{\\phi})]+\\sum_{n}^{N}\\ \\ln[\\iiint\\ \\exp[\\ln[p(t_{n}, z_{n}, \\mu_{n} | \\underline{\\ell}_{n}, \\vec{m}_{n}, \\underline{\\xi})]+\\ln[p(t_{n}, z_{n}, \\mu_{n} | \\vec{\\theta}, \\underline{\\phi})]-\\ln[p(t_{n}, z_{n}, \\mu_{n} | \\underline{\\xi})]]\\ d\\mu_{n}\\ dz_{n}\\ dt_{n}]\n",
    "\\end{equation}\n",
    "In words, that's the sum of the log-prior probability and the sum of the logs of the integrals over the sum of the log-interim posteriors, the log-hyperlikelihood, and the negative log-interim hyperlikelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "const_term = reg_vals(lninterimposteriors - lninterimhyperlikelihood[np.newaxis, :])\n",
    "assert(np.all(np.exp(const_term) * z_difs[np.newaxis, :, np.newaxis] * mu_difs[np.newaxis, np.newaxis, :] / n_types) <= 0.)\n",
    "\n",
    "def lnposterior(hyperparameters):\n",
    "    new_term = lnhyperlikelihood(hyperparameters)[np.newaxis, :]\n",
    "    in_exp = reg_vals(const_term + new_term)\n",
    "#     assert np.all(in_exp) <= 0.\n",
    "    in_log = np.sum(np.sum(np.sum(np.exp(in_exp) * mu_difs[np.newaxis, np.newaxis, :], axis=3) * z_difs[np.newaxis, :], axis=2) / n_types, axis=1)\n",
    "    in_sum = safe_log(in_log)\n",
    "    summed = np.sum(in_sum, axis=0)\n",
    "#     assert summed <= 0.\n",
    "    return summed\n",
    "\n",
    "def lnhyperposterior(hyperparameters):\n",
    "    return max(lnprior(hyperparameters) + lnposterior(hyperparameters), log_epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCMC Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mcrandstep = 10.**-1.\n",
    "nthreads = 4\n",
    "nwalkers = 100\n",
    "nsteps = 1000\n",
    "ninit = 100\n",
    "output_chain = True\n",
    "emcee_chain_output = \"emcee_chain_testing.dat\"\n",
    "gr_limit = 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(sampler, ivals, n_samps):\n",
    "    sampler.reset()\n",
    "    stuff = sampler.run_mcmc(ivals, n_samps)\n",
    "    mcmc_outputs = {}\n",
    "    mcmc_outputs['chain'] = sampler.chain\n",
    "    mcmc_outputs['lnprobability'] = sampler.lnprobability\n",
    "    mcmc_outputs['acceptance_fraction'] = sampler.acceptance_fraction\n",
    "#     mcmc_outputs['acor'] = sampler.acor\n",
    "    return mcmc_outputs\n",
    "\n",
    "def single_parameter_gr_stat(chain):\n",
    "    ssq = np.var(chain, axis=1, ddof=1)\n",
    "    W = np.mean(ssq, axis=0)\n",
    "    xb = np.mean(chain, axis=1)\n",
    "    xbb = np.mean(xb, axis=0)\n",
    "    m = chain.shape[0]\n",
    "    n = chain.shape[1]\n",
    "    B = n / (m - 1.) * np.sum((xbb - xb)**2., axis=0)\n",
    "    var_x = (n - 1.) / n * W + 1. / n * B\n",
    "    R_hat = np.sqrt(var_x / W)\n",
    "    return R_hat\n",
    "\n",
    "def multi_parameter_gr_stat(sample):\n",
    "    dims = np.shape(sample)\n",
    "    (n_walkers, n_iterations, n_params) = dims\n",
    "    n_burn_ins = n_iterations / 2\n",
    "    chain_ensemble = np.swapaxes(sample, 0, 1)\n",
    "    chain_ensemble = chain_ensemble[n_burn_ins:, :]\n",
    "    Rs = np.zeros((n_params))\n",
    "    for i in range(n_params):\n",
    "        chains = chain_ensemble[:, :, i].T\n",
    "        Rs[i] = single_parameter_gr_stat(chains)\n",
    "    return Rs\n",
    "\n",
    "def gr_test(sample, threshold=gr_limit):\n",
    "    gr = multi_parameter_gr_stat(sample)\n",
    "    print('Gelman-Rubin test statistic = '+str(gr))\n",
    "    return np.max(gr) > threshold\n",
    "    \n",
    "def run_MCMC(ivals):\n",
    "    sampler = emcee.EnsembleSampler(nwalkers, n_cosmo_hyperparams, lnhyperposterior, threads=nthreads)\n",
    "    burn_ins = 0\n",
    "    burning_in = True\n",
    "    vals = ivals\n",
    "    full_chain = np.array([[ivals[w]] for w in range(nwalkers)])\n",
    "    while burning_in:\n",
    "        print('began sampling '+str(burn_ins))\n",
    "        burn_in_mcmc_outputs = sample(sampler, vals, ninit)\n",
    "        print('finished sampling '+str(burn_ins))\n",
    "        with open('mcmc'+str(burn_ins)+'.hkl', 'w') as file_location:\n",
    "            hickle.dump(burn_in_mcmc_outputs, file_location)\n",
    "        progress_plotter(burn_ins)\n",
    "        full_chain = np.concatenate((full_chain, burn_in_mcmc_outputs['chain']), axis=1)\n",
    "        burning_in = gr_test(full_chain)\n",
    "        vals = np.array([item[-1] for item in burn_in_mcmc_outputs['chain']])\n",
    "        burn_ins += 1\n",
    "    mcmc_outputs = sample(sampler, vals, nsteps)\n",
    "    full_chain = np.concatenate((full_chain, mcmc_outputs['chain']), axis=1)\n",
    "    with open('full_chain.hkl', 'w') as file_location:\n",
    "        hickle.dump(full_chain, file_location)\n",
    "    return full_chain\n",
    "        \n",
    "def progress_plotter(burn_ins):\n",
    "    with open('mcmc'+str(burn_ins)+'.hkl', 'r') as file_location:\n",
    "        to_plot = hickle.load(file_location)\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "    sps_prob = fig.add_subplot(1, 2, 1)\n",
    "    for w in range(nwalkers):\n",
    "        sps_prob.plot(to_plot['lnprobability'][w], alpha=0.1)\n",
    "    sps_prob.set_title('lnprobability evolution for samples '+str(burn_ins*ninit)+'-'+str((burn_ins+1)*ninit))\n",
    "    sps_chain = fig.add_subplot(1, 2, 2)\n",
    "    for w in range(nwalkers):\n",
    "        sps_chain.scatter(to_plot['chain'][w].T[0], to_plot['chain'][w].T[1], alpha=0.01)\n",
    "    sps_chain.scatter(67.9, 1. - 0.693, color='r')\n",
    "    sps_chain.set_xlabel(cosmo_param_names[0])\n",
    "    sps_chain.set_ylabel(cosmo_param_names[1])\n",
    "    fig.savefig('progress_'+str(burn_ins)+'.png')\n",
    "    fig.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init_positions = []\n",
    "while len(init_positions) < nwalkers:\n",
    "    pair = prior_cosmo_dist.rvs()[0]\n",
    "    if param_check(pair):\n",
    "        init_positions.append(pair)\n",
    "init_positions = np.array(init_positions)\n",
    "init_probs = np.array([lnposterior(ival) for ival in init_positions])\n",
    "plt.hist(init_probs)\n",
    "\n",
    "# init_positions = prior_cosmo_dist.rvs(nwalkers)\n",
    "        \n",
    "# plt.scatter(init_positions.T[0], init_positions.T[1])\n",
    "\n",
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "\n",
    "results = run_MCMC(init_positions)\n",
    "\n",
    "pr.disable()\n",
    "s = StringIO.StringIO()\n",
    "sortby = 'cumtime'\n",
    "ps = pstats.Stats(pr, stream=s).sort_stats(sortby)\n",
    "ps.print_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for w in range(nwalkers):\n",
    "#     to_plot = results['chain'][w].T\n",
    "#     plt.scatter(to_plot[0], to_plot[1], alpha=0.01)#1./np.mean(results['probs'][w]))\n",
    "# plt.scatter(67.9, 1. - 0.693, color='r')\n",
    "# plt.xlabel(cosmo_param_names[0])\n",
    "# plt.ylabel(cosmo_param_names[1])\n",
    "# plt.savefig('bananas.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../truth.hkl', 'r+') as true_file:\n",
    "    true_info = hickle.load(true_file)   \n",
    "true_phi = true_info['phi']\n",
    "true_theta = true_info['theta']\n",
    "\n",
    "with open('full_chain.hkl', 'r') as file_location:\n",
    "    mcmc_outputs = hickle.load(file_location)\n",
    "# figure = corner.corner(mcmc_outputs)\n",
    "# figure.savefig('corner.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(n_cosmo_hyperparams * len(colors), len(colors)))\n",
    "priorvals = [H0_mean, Om0_mean]\n",
    "for p in range(n_cosmo_hyperparams):\n",
    "    plt.subplot(1, n_cosmo_hyperparams, p + 1)\n",
    "    for w in range(nwalkers):\n",
    "        to_plot = mcmc_outputs[w].T\n",
    "        plt.plot(to_plot[p], alpha=0.1)#1./np.mean(results['probs'][w]))#, to_plot[1])\n",
    "    plt.plot([true_theta[p]] * nsteps, c='k', linewidth=2)\n",
    "    plt.xlabel('sample number')\n",
    "    plt.ylabel(cosmo_param_names[p])\n",
    "fig.savefig('evolution.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for w in range(nwalkers):\n",
    "    plt.scatter(mcmc_outputs[w].T[0], mcmc_outputs[w].T[1], alpha=0.01)\n",
    "plt.scatter(67.9, 1. - 0.693, color='r')\n",
    "plt.xlabel(cosmo_param_names[0])\n",
    "plt.ylabel(cosmo_param_names[1])\n",
    "plt.savefig('final.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for_corner = mcmc_outputs.reshape(nwalkers*nsteps, 2)\n",
    "# figure = corner.corner(for_corner)\n",
    "# figure.savefig('corner.png')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
