{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Mock Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scippr` requires inputs in the form of catalogs $\\{\\ln[p(t_{n}, z_{n}, \\mu_{n} | \\underline{\\ell}_{n}, \\vec{m}_{n}, \\underline{D}^{*}, \\vec{P}, \\vec{V}, \\vec{C}, \\vec{M})]\\}_{N}$ of interim log-posteriors expressed as `3D` arrays constituting probabilities over $t_{n}$, $z_{n}$, and $\\mu_{n}$, enabling rapid computation of the log-posterior $\\ln[p(\\underline{\\phi}, \\vec{\\theta} | \\{\\underline{\\ell}_{n}, \\vec{m}_{n}\\}_{N}, \\vec{P}, \\vec{V}, \\vec{C}, \\vec{M})$ over the hyperparameters $\\underline{\\phi}$ and $\\vec{\\theta}$ of scientific interest.  This notebook outlines a procedure for generating such a catalog.\n",
    "\n",
    "Perhaps the defining feature of this pipeline is that it does not involve simulating supernova lightcurves or host galaxy photometry and instead simulating the interim posteriors directly.  There are several good reasons for this choice:\n",
    "\n",
    "* The motivation for `scippr` is to develop a method for doing inference with accurate probability distributions over relevant supernova parameters, not to develop methods for obtaining those probability distributions.\n",
    "* We avoid tying our inference method to a particular way of deriving interim posteriors from observed data.\n",
    "* We avoid making assumptions about the details of the observed data, such as the photometric filters, intrinsic lightcurves, and observing conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import daft\n",
    "import astropy.cosmology as cosmology\n",
    "\n",
    "import numpy as np\n",
    "import bisect\n",
    "import scipy.stats as sps\n",
    "import scipy.optimize as spo\n",
    "import hickle\n",
    "import sys\n",
    "epsilon = sys.float_info.epsilon\n",
    "log_epsilon = np.log(epsilon)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib import rc\n",
    "rc(\"font\", family=\"serif\", size=12)\n",
    "rc(\"text\", usetex=True)\n",
    "\n",
    "colors = 'rbgcymk'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scippr` is based on a probabilistic graphical model, illustrated below.  The model has two types of observables, shown in shaded circles, supernova lightcurves $\\underline{\\ell}_{n}$ and host galaxy photometry $\\vec{m}_{n}$.  The parameters, which are by definition not directly observable, are shown in empty circles.  The latent variables of supernova type $t_{n}$, redshift $z_{n}$, and distance modulus $\\mu_{n}$ are parameters over which we will marginalize, without ever directly inferring them, and while all three of them influence $\\underline{\\ell}_{n}$, only $z_{n}$ affects $\\vec{m}_{n}$ in this model.  In other words, _we currently assume no relationship between supernova type and host galaxy photometry, an assumption we may revisit in the future_.  The selection functions parametrized by $\\vec{P}$, $\\vec{V}$, $\\vec{C}$, and $\\vec{M}$ are known constants of the survey symbolized by dots that influence the possible lightcurves and host galaxy photometry that are included in the sample.  The box indicates that the latent variables and the observables are generated independently $N$ times for each supernova in the sample.  The hyperparameters we would like to estimate are the redshift-dependent supernova type proportions $\\underline{\\phi}$ that determine $t_{n}$ and $z_{n}$ and the cosmological parameters $\\vec{\\theta}$ that relate $z_{n}$ to $\\mu_{n}$, which are shared by all $N$ supernovae in the observed sample.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the PGM\n",
    "pgm = daft.PGM([6, 4.5], origin=[0, 0])\n",
    "\n",
    "#desired hyperparameters\n",
    "pgm.add_node(daft.Node(\"cosmology\", r\"$\\vec{\\theta}$\", 2., 4.))\n",
    "pgm.add_node(daft.Node(\"dist\", r\"$\\underline{\\phi}$\", 3.5, 4.))\n",
    "#pgm.add_node(daft.Node(\"rates\", r\"$\\vec{R}$\", 3., 5.5, fixed=True))\n",
    "\n",
    "#latent variables/parameters\n",
    "pgm.add_node(daft.Node(\"distance\", r\"$\\mu_{n}$\", 2., 2.5))\n",
    "pgm.add_node(daft.Node(\"redshift\", r\"$z_{n}$\", 3., 3.))\n",
    "pgm.add_node(daft.Node(\"type\", r\"$t_{n}$\", 4., 2.5))\n",
    "\n",
    "#data\n",
    "pgm.add_node(daft.Node(\"lightcurve\", r\"$\\underline{\\ell}_{n}$\", 2.5, 1., observed=True))\n",
    "pgm.add_node(daft.Node(\"photometry\", r\"$\\vec{m}_{n}$\", 3.5, 1., observed=True))\n",
    "\n",
    "#known constant parameters\n",
    "pgm.add_node(daft.Node(\"lightcurve selection\", r\"$\\vec{P}, \\vec{V}, \\vec{C}$\", 1., 1.75, fixed=True))\n",
    "pgm.add_node(daft.Node(\"photometry selection\", r\"$\\vec{M}$\", 5., 1.75, fixed=True))\n",
    "\n",
    "# Add in the edges.\n",
    "pgm.add_edge(\"dist\", \"type\")\n",
    "pgm.add_edge(\"cosmology\", \"distance\")\n",
    "pgm.add_edge(\"dist\", \"redshift\")\n",
    "pgm.add_edge(\"redshift\", \"distance\")\n",
    "#pgm.add_edge(\"distance\", \"photometry\")\n",
    "pgm.add_edge(\"distance\", \"lightcurve\")\n",
    "pgm.add_edge(\"redshift\", \"photometry\")\n",
    "pgm.add_edge(\"redshift\", \"lightcurve\")\n",
    "pgm.add_edge(\"type\", \"lightcurve\")\n",
    "pgm.add_edge(\"photometry selection\", \"photometry\")\n",
    "pgm.add_edge(\"lightcurve selection\", \"lightcurve\")\n",
    "\n",
    "# plates\n",
    "pgm.add_plate(daft.Plate([1.5, 0.5, 3., 3.], label=r\"$n = 1, \\cdots, N$\"))\n",
    "\n",
    "# Render and save.\n",
    "pgm.render()\n",
    "pgm.figure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simulate a mock catalog for `scippr`, there are three main steps.\n",
    "\n",
    "1. Choose true values for the hyperparameters, which we would like to recover from our inference, and the parameters, over which we intend to marginalize.\n",
    "2. Create likelihoods based on a model for how they are derived from observations under the selection functions.\n",
    "3. Make interim posteriors by assuming interim priors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing true hyperparameters and parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The redshift-dependent type proportions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[the true redshift-dependent type rate distribution, with plot of three functions] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = ['Ia', 'Ibc', 'II']\n",
    "n_types = len(types)\n",
    "# making up the type fractions, will replace this with data soon!\n",
    "frac_types = np.array([0.4, 0.1, 0.5])\n",
    "assert np.isclose(np.sum(frac_types), 1.)\n",
    "\n",
    "min_z = 0.5\n",
    "max_z = 2.\n",
    "\n",
    "n_of_z_consts = {}\n",
    "n_of_z_consts['Ia'] = (1.5, 0.5)\n",
    "n_of_z_consts['Ibc'] = (1., 0.5)\n",
    "n_of_z_consts['II'] = (0.5, 0.5)\n",
    "\n",
    "true_n_of_z = []\n",
    "for t in types:\n",
    "    (mean, std) = n_of_z_consts[t]\n",
    "    low, high = (min_z - mean) / std, (max_z - mean) / std\n",
    "    true_n_of_z.append(sps.truncnorm(low, high, loc = mean, scale = std))\n",
    "\n",
    "plot_res = 20\n",
    "z_range = max_z - min_z\n",
    "z_grid = np.linspace(min_z, max_z, num=plot_res + 1, endpoint=True)\n",
    "z_plot = (z_grid[1:] + z_grid[:-1]) / 2.\n",
    "z_dif_plot = z_grid[1:] - z_grid[:-1]\n",
    "\n",
    "plot_true_n_of_z = np.zeros((n_types, plot_res))\n",
    "for t in range(n_types):\n",
    "    plot_true_n_of_z[t] = true_n_of_z[t].pdf(z_plot)\n",
    "plot_true_n_of_z = frac_types[:, np.newaxis] * np.array(plot_true_n_of_z)# / z_range\n",
    "plot_true_n_of_z /= np.sum(plot_true_n_of_z * z_dif_plot)\n",
    "assert np.isclose(np.sum(plot_true_n_of_z * z_dif_plot), 1.)\n",
    "\n",
    "for t in range(n_types):\n",
    "    plt.plot(z_plot, plot_true_n_of_z[t], color=colors[t], label=types[t])\n",
    "plt.xlabel(r'$z$')\n",
    "plt.ylabel(r'relative rate')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[samples of t, z from the true redshift-dependent type rate distribution, with histograms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_discrete(fracs, n_of_z, N):\n",
    "    out_info = []\n",
    "    cdf = np.cumsum(fracs)\n",
    "    for n in range(N):\n",
    "        each = {}\n",
    "        r = np.random.random()\n",
    "        k = bisect.bisect(cdf, r)\n",
    "        each['t'] = types[k]\n",
    "        each['z'] = n_of_z[k].rvs()\n",
    "        out_info.append(each)\n",
    "    return out_info\n",
    "\n",
    "n_sne = 100\n",
    "\n",
    "true_params = sample_discrete(frac_types, true_n_of_z, n_sne)\n",
    "\n",
    "to_plot = [[d['z'] for d in true_params if d['t'] == types[t]] for t in range(n_types)]\n",
    "for t in range(n_types):\n",
    "    plt.plot(z_plot, plot_true_n_of_z[t] * n_types, color=colors[t], label='true '+types[t])\n",
    "    plt.hist(to_plot[t], color=colors[t], alpha=1./3., label='sampled '+types[t], normed=True)\n",
    "plt.xlabel(r'$z$')\n",
    "plt.ylabel(r'relative rate')\n",
    "plt.legend(fontsize='xx-small')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The true cosmological parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the true hyperparameter vector $\\vec{\\theta}'$ as having two components, $H_{0}'$ and $\\Omega_{m,0}'$.  In a future revision, we may include additional cosmological parameters in this hyperparameter vector.  We choose the true values for the cosmological parameters to be those published by /Planck/.  [include citation]  \n",
    "\n",
    "Since every supernova in our sample already has a true redshift $z_{n}'$, we can easily establish a true distance modulus $\\mu_{n}'$ via the luminosity distance equation.  [insert equation here]  We plot a traditional Hubble diagram of the supernovae in our sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Planck\n",
    "true_H0 = 67.9\n",
    "true_Om0 = 1. - 0.693\n",
    "true_hyperparams = np.array([true_H0, true_Om0])\n",
    "n_hyperparams = len(true_hyperparams)\n",
    "true_cosmo = cosmology.FlatLambdaCDM(H0=true_H0, Om0=true_Om0)\n",
    "\n",
    "for n in range(n_sne):\n",
    "    true_params[n]['mu'] = true_cosmo.distmod(true_params[n]['z']).value\n",
    "    \n",
    "to_plot_x = [[d['z'] for d in true_params if d['t'] == types[t]] for t in range(n_types)]\n",
    "to_plot_y = [[d['mu'] for d in true_params if d['t'] == types[t]] for t in range(n_types)]\n",
    "for t in range(n_types):\n",
    "    plt.scatter(to_plot_x[t], to_plot_y[t], color=colors[t], label=types[t])\n",
    "plt.plot(z_plot, [true_cosmo.distmod(z).value for z in z_plot], color='k')\n",
    "plt.xlabel(r'$z$')\n",
    "plt.ylabel(r'$\\mu$')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title(r'$H_{0}='+str(true_H0)+r', \\Omega_{m,0}='+str(true_Om0)+r'$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating Probabilities\n",
    "\n",
    "`scippr` is intended to be used on interim posterior probabilities derived from a probabilistic lightcurve fitting procedure.  These will be provided as log-probabilities evaluated on a `3D` grid in type, redshift, and distance modulus space.  We choose to work with log-probabilities because they preserve numerical precision better and enable slow products of arrays to be transformed into fast sums of arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def safe_log(arr, threshold=epsilon):\n",
    "#     shape = np.shape(arr)\n",
    "#     flat = arr.flatten()\n",
    "#     logged = np.log(np.array([max(a, threshold) for a in flat])).reshape(shape)\n",
    "    arr[arr < threshold] = threshold\n",
    "    return np.log(arr)\n",
    "\n",
    "def reg_vals(arr, threshold=log_epsilon):\n",
    "    arr[arr < threshold] = threshold\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this binning is arbitrary!\n",
    "n_zs = 20\n",
    "z_bins = np.linspace(min_z, max_z, num=n_zs + 1, endpoint=True)\n",
    "z_difs = z_bins[1:] - z_bins[:-1]\n",
    "z_dif = np.mean(z_difs)\n",
    "z_mids = (z_bins[1:] + z_bins[:-1]) / 2.\n",
    "\n",
    "# want this to be agnostic about true cosmology\n",
    "n_mus = n_zs\n",
    "min_mu, max_mu = min([s['mu'] for s in true_params]) - 0.5, max([s['mu'] for s in true_params]) + 0.5\n",
    "mu_bins = np.linspace(min_mu, max_mu, num=n_mus + 1, endpoint=True)\n",
    "mu_difs = mu_bins[1:] - mu_bins[:-1]\n",
    "mu_dif = np.mean(mu_difs)\n",
    "mu_range = np.max(mu_bins) - np.min(mu_bins)\n",
    "mu_mids = (mu_bins[1:] + mu_bins[:-1]) / 2.\n",
    "\n",
    "z_mu_grid = np.array([[(z, mu) for mu in mu_mids] for z in z_mids])\n",
    "cake_shape = np.shape(z_mu_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating likelihoods\n",
    "\n",
    "The goal here is to simulate realistic outputs from such a procedure without having to develop and run one.   Each log-likelihood $\\ln[p(\\underline{\\ell}_{n}, \\vec{m}_{n} | t_{n}, z_{n}, \\mu_{n}, \\vec{P}, \\vec{V}, \\vec{C}, \\vec{M})]$ may be broken down into simpler components that may be summed to create log-likelihoods.\n",
    "\n",
    "\\begin{align*}\n",
    "\\ln[p(\\underline{\\ell}_{n}, \\vec{m}_{n} | t_{n}, z_{n}, \\mu_{n}, \\vec{M}, \\vec{P}, \\vec{V}, \\vec{C})] &= \\ln[p(\\underline{\\ell}_{n} | t_{n}, z_{n}, \\mu_{n}, \\vec{P}, \\vec{V}, \\vec{C})] + \\ln[p(\\vec{m}_{n} | z_{n}, \\vec{M})]\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling an observed photo-$z$ PDF\n",
    "\n",
    "The photo-$z$ likelihood for a supernova included in the sample can be constructed according to the following:\n",
    "\n",
    "\\begin{equation}\n",
    "p(\\vec{m}_{n} | z_{n}, \\vec{M}) \\propto \\frac{p(\\vec{m}_{n} | z_{n})}{\\int\\ d\\vec{m}_{n}\\ p(\\vec{m}_{n} | \\vec{M})\\ p(\\vec{m}_{n} | z_{n})}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The photo-$z$ \"PDF\"*\n",
    "\n",
    "It is simplest to start with the log-likelihood $\\ln[p(\\vec{m}_{n} | z_{n})]$ of host galaxy photometry $\\vec{m}_{n}$ as a function of redshift $z_{n}$.  We assume the simplest model in which photo-$z$ PDFs are Gaussians $\\mathcal{N}(z_{n}'', \\sigma_{n}^{2})$ where $z_{n}''\\sim\\mathcal{N}(z_{n}', \\sigma_{n}^{2})$.  We will also state that the variance is a constant $\\sigma_{n}\\equiv\\sigma$ for all $n$.  [cite where 0.03 came from]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# very simple p(z) model, simple gaussians\n",
    "pz_sigma = 0.03\n",
    "\n",
    "pzs, ln_pzs = [], []\n",
    "for s in range(n_sne):\n",
    "    dist = sps.norm(loc = true_params[s]['z'], scale = 0.03)\n",
    "    pz_mean = dist.rvs()\n",
    "    new_dist = sps.norm(loc = pz_mean, scale = pz_sigma)\n",
    "    pz = new_dist.pdf(z_mids)\n",
    "    #ln_pz = new_dist.logpdf(z_mids)\n",
    "    pzs.append(pz)\n",
    "    #ln_pzs.append(ln_pz)\n",
    "pzs = np.array(pzs)\n",
    "ln_pzs = safe_log(pzs)#np.array(ln_pzs)\n",
    "\n",
    "for s in range(n_sne)[:len(colors)]:\n",
    "    plt.plot(z_mids, pzs[s], color=colors[s])\n",
    "    plt.vlines(true_params[s]['z'], 0., 15., color=colors[s])\n",
    "plt.xlabel(r'$z$')\n",
    "plt.ylabel(r'$p(z)$')\n",
    "plt.title(r'host galaxy $p(z)$ distributions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The host galaxy photometry selection function*\n",
    "\n",
    "The selection function can be represented by $\\ln[p(\\vec{m}_{n} | \\vec{M})]$.  As a placeholder until we identify a physical model, we assume a very simple selection function corresponding to a cut at $\\vec{M}[argmax_{F}[\\vec{m}_{n}]]$ in the highest magnitude (dimmest) host galaxy magnitude $\\max_{F}[\\vec{m}_{n}]$ in the space of filters $F$.  Thus we will take $\\ln[p(\\vec{m}_{n} | \\vec{M})] = \\ln[p(\\max_{F}[\\vec{m}_{n}] \\leq \\vec{M}[argmax_{F}[\\vec{m}_{n}]] | z'_{n})$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# choose a vector of sensible per-filter host galaxy magnitude cutoffs M\n",
    "# devise a 2D function f_M(max_F[m], z) = p(max_F[m] | z)\n",
    "# integrate f_M(max_F[m], z) over max_F[m] up to M[argmax_F[m]] to get 1D function f_M(z)\n",
    "# evaluate ln[f_M(z)] at z' as ln[p(m_n | M)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The redshift likelihood function*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add the two log-likelihood components together here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling fitting an observed lightcurve\n",
    "\n",
    "Based on how existing lightcurve fitters work, a lightcurve is generally assigned a class before its redshift and distance modulus are estimated because the fitting function will differ based on the assigned class.  Thus, we may assume that the lightcurve likelihood is separable as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "p(\\underline{\\ell}_{n} | t_{n}, z_{n}, \\mu_{n}, \\vec{P}, \\vec{V}, \\vec{C}) &\\propto \\frac{p(\\underline{\\ell}_{n} | t_{n})\\ p(\\underline{\\ell}_{n}, t_{n} | z_{n}, \\mu_{n})}{\\int\\ d\\underline{\\ell}_{n}\\ p(\\underline{\\ell}_{n} | \\vec{P}, \\vec{V}, \\vec{C})\\ p(\\underline{\\ell}_{n} | t_{n})\\ p(\\underline{\\ell}_{n}, t_{n} | z_{n}, \\mu_{n})}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The supernova lightcurve selection function*\n",
    "\n",
    "As a placeholder until we identify a better physical model, we assume a selection function represented by $\\ln[p(\\underline{\\ell}_{n} | \\vec{P}, \\vec{V}, \\vec{C})]$ with three contributions over the spaces of magnitudes in the filters populating the space $F$ and times relative to the peak of the lightcurve populating the space $S$.\n",
    "\n",
    "1. A cut at on peak lightcurve magnitude in the filter where it is dimmest corresponding to the magnitude $\\max_{F}[\\max_{S}[\\underline{\\ell}_{n}]]$ in the filter $argmax_{F}[\\max_{S}[\\underline{\\ell}_{n}]]$ with the highest peak magnitude (i.e. the dimmest) must be lower (i.e. brighter) than some cutoff value in that filter $\\vec{P}[argmax_{F}[\\max_{S}[\\underline{\\ell}_{n}]]]$.  \n",
    "2. A cut on minimum variability wherein the smallest difference $\\max_{F}[\\max_{S}[\\underline{\\ell}_{n}] - \\min_{S}[\\underline{\\ell}_{n}]]$ exceeds a cutoff value $\\vec{V}[argmax_{F}[\\max_{S}[\\underline{\\ell}_{n}] - \\min_{S}[\\underline{\\ell}_{n}]]]$.\n",
    "3. A pair of cuts in the minimum and maximum timescales of variability such that $C_{min} \\leq \\max_{F}[S[argmax_{S}[\\underline{\\ell}_{n}]] - S[argmin_{S}[\\underline{\\ell}_{n}]]] \\leq C_{max}$ and $\\vec{C} = (C_{min}, C_{max})$.\n",
    "\n",
    "This model corresponds to the following selection probability:\n",
    "\n",
    "\\begin{align*}\n",
    "\\ln[p(\\underline{\\ell}_{n} | \\vec{P}, \\vec{V}, \\vec{C})] =& \\ln[p(\\max_{F}[\\max_{S}[\\underline{\\ell}_{n}]] \\leq \\vec{P}[argmax_{F}[\\max_{S}[\\underline{\\ell}_{n}]]] | t'_{n}, z'_{n}, \\mu'_{n})] \\\\\n",
    "& + \\ln[p(\\max_{F}[\\max_{S}[\\underline{\\ell}_{n}] - \\min_{S}[\\underline{\\ell}_{n}]] \\geq \\vec{V}[argmax_{F}[\\max_{S}[\\underline{\\ell}_{n}] - \\min_{S}[\\underline{\\ell}_{n}]]] | t'_{n}, z'_{n}, \\mu'_{n}))] \\\\\n",
    "& + \\ln[p(C_{min} \\leq \\max_{F}[S[argmax_{S}[\\underline{\\ell}_{n}]] - S[argmin_{S}[\\underline{\\ell}_{n}]]] \\leq C_{max} | t'_{n}, z'_{n}, \\mu'_{n})]\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# not sure about mu-dependence for these. . .\n",
    "\n",
    "# choose a vector of sensible per-filter lightcurve peak magnitude cutoffs P\n",
    "# devise a 4D function f_P(max_F[max_S[l]], t, z, mu) = p(max_F[max_S[l]] | t, z, mu)\n",
    "# integrate f_P(max_F[max_S[l]], t, z, mu) over max_F[max_S[l]] below P[argmax_F[max_S[l]]] to get 3D function f_P(t, z, mu)\n",
    "# evaluate ln[f_P(t, z, mu)] at t', z', mu' as ln[p(l | P)]\n",
    "\n",
    "# choose a vector of sensible per-filter maximum lightcurve difference cutoffs V\n",
    "# devise a 4D function f_V(max_F[max_S[l]-min_S[l]], t, z, mu) = p(max_F[max_S[l]-min_S[l]] | t, z, mu)\n",
    "# integrate f_V(max_F[max_S[l]-min_S[l]], t, z, mu) over max_F[max_S[l]-min_S[l]] above V[argmax_F[max_S[l]-min_S[l]]] to get 3D function f_V(t, z, mu)\n",
    "# evaluate ln[f_V(t, z, mu)] at t', z', mu' as ln[p(l | V)]\n",
    "\n",
    "# choose a pair of sensible lightcurve duration cutoffs C\n",
    "# devise a 4D function f_C(max_F[S[argmax_S[l]]-S[argmin_S[l]]], t, z, mu) = p(max_F[S[argmax_S[l]]-S[argmin_S[l]]] | t, z, mu)\n",
    "# integrate f_C(max_F[S[argmax_S[l]]-S[argmin_S[l]]], t, z, mu) over max_F[S[argmax_S[l]]-S[argmin_S[l]]] between C_min and C_max to get 3D function f_C(t, z, mu)\n",
    "# evaluate ln[f_C(t, z, mu)] at t', z', mu' as ln[p(l | C)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Lightcurve classification*\n",
    "\n",
    "The confusion matrix quantifies the probabilities that an item is truly of a certain class given the fact that it has been classified as a different class.  (A more in-depth description of the confusion matrix can be found [here](https://github.com/rbiswas4/SNeLightcurveQualityMetric/blob/master/classification_metric.tex).)  We will use the $p(t_{n}' | \\hat{t}_{n})$ elements of the confusion matrix as a proxy for $p(\\underline{\\ell}_{n} | t_{n})$.  The confusion matrix is specific to each classification method, so we will have to choose one to simulate a realistic mock dataset.  For now, we proceed assuming a fairly trivial confusion matrix giving a 50% chance of correct classification for each type and equal probabilities for all misclassifications.  Obviously this will be revised in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# will need to take this from data of some kind, arbitrary for now\n",
    "conf_matrix = 0.25 + 0.25 * np.eye(3)\n",
    "assert np.isclose(np.sum(conf_matrix, axis=1).all(), frac_types.all())\n",
    "ln_conf_matrix = safe_log(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*$\\chi^{2}$ lightcurve parameter fitting*\n",
    "\n",
    "In order to produce $p(\\underline{\\ell}_{n}, t_{n} | z_{n}, \\mu_{n})$, we will again introduce the idea of using the true type $t_{n}'$ as a proxy for the lightcurve $\\underline{\\ell}_{n}$ and a classified type $\\hat{t}_{n}$ for the variable type $t_{n}$ that appears in the probability expressions.  If we do this, the quantity we want is really $p(t_{n}', \\hat{t}_{n} | z_{n}, \\mu_{n})$.  We can obtain this knowing how lightcurve fitters, in general, estimate redshifts $z_{n}$ and distance moduli $\\mu_{n}$ under all possible combinations of $t_{n}'$ and $\\hat{t}_{n}$.  We will construct functions that aim to simulate the signatures of misclassification in the Hubble diagram.  [find a figure to link to]  Currently, we use placeholder functions that will be replaced later on as information about the consequences of fitting with the wrong function becomes available.  The table below summarizes the fitting function for each true type given a classification of type $Ia$.  All other classified types are assumed to give a distribution that is uniform in $\\mu_{n}$ and Gaussian in $z_{n}$ according to the same prescription used for the photo-$z$ PDFs.\n",
    "\n",
    "| True Type | Functional Form of Ia likelihood |\n",
    "| :-------: | :------------------------------: |\n",
    "| Ia | $\\vec{\\mathcal{N}}((z_{n}'', \\mu_{n}''), \\underline{\\Sigma}_{n})$ where $\\underline{\\Sigma}_{n}=(\\sigma^{2}_{z, Ia}, \\sigma^{2}_{\\mu, Ia})\\times\\underline{I}$ and $(z_{n}'', \\mu_{n}'')\\sim\\vec{\\mathcal{N}}((z_{n}', \\mu_{n}'), \\underline{\\Sigma}_{n})$ |\n",
    "| Ibc | $\\vec{\\mathcal{N}}((z_{n}'', \\mu_{n}''), \\underline{\\Sigma}_{n})$ where $\\underline{\\Sigma}_{n}=(\\sigma^{2}_{z, Ibc}, \\sigma^{2}_{\\mu, Ibc})\\times\\underline{I}$ and $(z_{n}'', \\mu_{n}'')\\sim\\vec{\\mathcal{N}}((z_{n}', \\mu_{n}' - C_{Ibc}), \\underline{\\Sigma}_{n})$ for survey-wide constant $C_{Ibc}$ |\n",
    "| II | $\\vec{\\mathcal{N}}((z_{n}'', \\mu_{n}''), \\underline{\\Sigma}_{n})$ where $\\underline{\\Sigma}_{n}=(\\sigma^{2}_{z, II}, \\sigma^{2}_{\\mu, II})\\times\\underline{I}$ and $(z_{n}'', \\mu_{n}'')\\sim\\vec{\\mathcal{N}}((z_{n}', C_{II}), \\underline{\\Sigma}_{n})$ for survey-wide constant $C_{II}$ |\n",
    "\n",
    "In a future revision, the values of $\\sigma^{2}_{z, \\tau}$ and $\\sigma^{2}_{\\mu, \\tau}$ for each type $\\tau$ will be replaced by random variables themselves representing the intrinsic variation among lightcurves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# must set nuisance parameters inherent in process of producing interim posteriors from lightcurves\n",
    "Ia_Ia_var = np.array([0.01, 0.02]) ** 2\n",
    "Ibc_Ia_delta = 1.\n",
    "Ibc_Ia_var = np.array([0.01, 0.01]) ** 2\n",
    "II_Ia_delta = np.mean(mu_mids)\n",
    "II_Ia_var = np.array([0.01, 0.05]) ** 2\n",
    "z_sigma = 0.03\n",
    "\n",
    "# definitely needs more work on what (z, mu) distributions are expected when lightcurves are fit with wrong templates\n",
    "# just made it flat for now\n",
    "\n",
    "def fit_Ia(z, mu):\n",
    "    cake = np.zeros((n_types, n_zs, n_mus))\n",
    "    cake_Ia = sps.multivariate_normal(mean = np.array([z, mu]), cov = Ia_Ia_var * np.eye(2))\n",
    "    [z_samp, mu_samp] = cake_Ia.rvs()\n",
    "    cake_Ia = sps.multivariate_normal(mean = np.array([z_samp, mu_samp]), cov = Ia_Ia_var * np.eye(2))\n",
    "    cake[0] = cake_Ia.pdf(z_mu_grid.reshape(-1, cake_shape[-1])).reshape(cake_shape[:-1])\n",
    "    dist = sps.norm(loc = z, scale = z_sigma)\n",
    "    z_means = dist.rvs(2)\n",
    "    cake[1] = sps.norm(loc = z_means[0], scale = z_sigma).pdf(z_mids)[:, np.newaxis] / n_mus#np.ones(cake_shape[:-1]) / np.prod(cake_shape[:-1])\n",
    "    cake[2] = sps.norm(loc = z_means[1], scale = z_sigma).pdf(z_mids)[:, np.newaxis] / n_mus#np.ones(cake_shape[:-1]) / np.prod(cake_shape[:-1])\n",
    "    ln_cake = safe_log(cake)\n",
    "    return ln_cake\n",
    "    \n",
    "def fit_Ibc(z, mu):\n",
    "    cake = np.zeros((n_types, n_zs, n_mus))\n",
    "    cake_Ia = sps.multivariate_normal(mean = np.array([z, mu - Ibc_Ia_delta]), cov = Ibc_Ia_var * np.eye(2))\n",
    "    [z_samp, mu_samp] = cake_Ia.rvs()\n",
    "    cake_Ia = sps.multivariate_normal(mean = np.array([z_samp, mu_samp]), cov = Ibc_Ia_var * np.eye(2))\n",
    "    cake[0] = cake_Ia.pdf(z_mu_grid.reshape(-1, cake_shape[-1])).reshape(cake_shape[:-1])\n",
    "    dist = sps.norm(loc = z, scale = z_sigma)\n",
    "    z_means = dist.rvs(2)\n",
    "    cake[1] = sps.norm(loc = z_means[0], scale = z_sigma).pdf(z_mids)[:, np.newaxis] / n_mus#np.ones(cake_shape[:-1]) / np.prod(cake_shape[:-1])\n",
    "    cake[2] = sps.norm(loc = z_means[1], scale = z_sigma).pdf(z_mids)[:, np.newaxis] / n_mus#np.ones(cake_shape[:-1]) / np.prod(cake_shape[:-1])\n",
    "    ln_cake = safe_log(cake)\n",
    "    return ln_cake\n",
    "    \n",
    "def fit_II(z, mu):\n",
    "    cake = np.zeros((n_types, n_zs, n_mus))\n",
    "    cake_Ia = sps.multivariate_normal(mean = np.array([z, II_Ia_delta]), cov = II_Ia_var * np.eye(2))\n",
    "    [z_samp, mu_samp] = cake_Ia.rvs()\n",
    "    cake_Ia = sps.multivariate_normal(mean = np.array([z_samp, mu_samp]), cov = II_Ia_var * np.eye(2))\n",
    "    cake[0] = cake_Ia.pdf(z_mu_grid.reshape(-1, cake_shape[-1])).reshape(cake_shape[:-1])\n",
    "    dist = sps.norm(loc = z, scale = z_sigma)\n",
    "    z_means = dist.rvs(2)\n",
    "    cake[1] = sps.norm(loc = z_means[0], scale = z_sigma).pdf(z_mids)[:, np.newaxis] / n_mus#np.ones(cake_shape[:-1]) / np.prod(cake_shape[:-1])\n",
    "    cake[2] = sps.norm(loc = z_means[1], scale = z_sigma).pdf(z_mids)[:, np.newaxis] / n_mus#np.ones(cake_shape[:-1]) / np.prod(cake_shape[:-1])\n",
    "    ln_cake = safe_log(cake)\n",
    "    return ln_cake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The lightcurve likelihood*\n",
    "\n",
    "Now that the type-specific fitting functions have been established, we may combine all relevant terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_any(true_vals):\n",
    "    if true_vals['t'] == 'Ia':\n",
    "        ln_cake = fit_Ia(true_vals['z'], true_vals['mu']) + ln_conf_matrix[:, 0, np.newaxis, np.newaxis]\n",
    "    if true_vals['t'] == 'Ibc':\n",
    "        ln_cake = fit_Ibc(true_vals['z'], true_vals['mu']) + ln_conf_matrix[:, 1, np.newaxis, np.newaxis]\n",
    "    if true_vals['t'] == 'II':\n",
    "        ln_cake = fit_II(true_vals['z'], true_vals['mu']) + ln_conf_matrix[:, 2, np.newaxis, np.newaxis]\n",
    "    return ln_cake\n",
    "\n",
    "def fit_all(catalog):\n",
    "    dessert = []\n",
    "    for true_vals in catalog:\n",
    "        dessert.append(fit_any(true_vals))\n",
    "    return np.array(dessert)\n",
    "\n",
    "sheet_cake = fit_all(true_params)\n",
    "\n",
    "# happily, these look like what we see in contaminated hubble diagrams!\n",
    "fig = plt.figure(figsize=(n_types*len(colors), n_sne*len(colors)))\n",
    "p = 0\n",
    "for s in range(n_sne)[:len(colors)]:\n",
    "    for t in range(n_types):\n",
    "        p += 1\n",
    "        plt.subplot(n_sne, n_types, p)\n",
    "        plt.pcolormesh(z_mids, mu_mids, sheet_cake[s][t].T, cmap='viridis')\n",
    "        plt.colorbar()\n",
    "        plt.scatter(true_params[s]['z'], true_params[s]['mu'], color='r')\n",
    "        plt.title('true '+true_params[s]['t']+', class '+types[t])\n",
    "        plt.xlabel(r'$z$')\n",
    "        plt.ylabel(r'$\\mu$')\n",
    "        plt.axis([z_bins[0], z_bins[-1], mu_bins[0], mu_bins[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The full likelihood\n",
    "\n",
    "[combine both the host galaxy photometry and supernova lightcurve likelihood components]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these don't have to be normalized\n",
    "ln_likelihoods = reg_vals(sheet_cake + ln_pzs[:, np.newaxis, :, np.newaxis])\n",
    "\n",
    "# look at how much narrower they are! this should be visible in all types, but the color scaling is bad for Ibc & II\n",
    "fig = plt.figure(figsize=(n_types*len(colors), n_sne*len(colors)))\n",
    "p = 0\n",
    "for s in range(n_sne)[:len(colors)]:\n",
    "    for t in range(n_types):\n",
    "        p += 1\n",
    "        plt.subplot(n_sne, n_types, p)\n",
    "        plt.pcolormesh(z_mids, mu_mids, ln_likelihoods[s][t].T, cmap='viridis')#, vmin = 0., vmax = 3.)\n",
    "        plt.colorbar()\n",
    "        plt.scatter(true_params[s]['z'], true_params[s]['mu'], color='r')\n",
    "        plt.title('true '+true_params[s]['t']+', class '+types[t])\n",
    "        plt.xlabel(r'$z$')\n",
    "        plt.ylabel(r'$\\mu$')\n",
    "        plt.axis([z_bins[0], z_bins[-1], mu_bins[0], mu_bins[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making interim posteriors\n",
    "\n",
    "In reality, when we perform classification, lightcurve fitting, and photo-$z$ PDF estimation, we are not determining likelihoods $p(\\underline{\\ell}_{n}, \\vec{m}_{n} | t_{n}, z_{n}, \\mu_{n}, \\vec{P}, \\vec{V}, \\vec{C}, \\vec{M})$ but instead are finding interim posteriors $p(t_{n}, z_{n}, \\mu_{n} | \\underline{\\ell}_{n}, \\vec{m}_{n}, \\underline{\\phi}^{*}, \\vec{\\theta}^{*}, \\vec{P}, \\vec{V}, \\vec{C}, \\vec{M})$, due to the assumptions about the distributions of our latent variables.  In our case, choices of interim hyperpriors $\\underline{\\phi}^{*}$ and $\\vec{\\theta}^{*}$ will translate directly into a prior belief about the `3D` distribution $p(t_{n}, z_{n}, \\mu_{n} | \\underline{\\phi}^{*}, \\vec{\\theta}^{*}, \\vec{P}, \\vec{V}, \\vec{C}, \\vec{M})$ that is independent of any observations (and thus independent of $n$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing interim priors\n",
    "\n",
    "To transform our likelihoods into interim posteriors that accurately represent what we expect a real data analysis pipeline to produce, we must choose interim priors.  The interim prior represents the $p(t, z, \\mu)$ that is used in the estimation of log-posterior probabilities -- our assumptions about $p(t_{n}, z_{n}, \\mu_{n} | \\underline{\\ell}_{n}, \\vec{m}_{n})$ are parametrized by the interim prior parameters comprising $\\underline{xi}$, so the closest we can get to the desired posteriors is the interim posteriors $p(t_{n}, z_{n}, \\mu_{n} | \\underline{\\ell}_{n}, \\vec{m}_{n}, \\underline{\\xi})$.  We will set a uniform prior on the redshift-dependent supernova proportions and use the WMAP values of the cosmological parameters with grossly inflated error bars to restrict the parameter space of $z$ and $\\mu$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flat prior on redshift-dependent SNe proportions\n",
    "interim_n_of_z = np.ones((n_types, n_zs))\n",
    "interim_n_of_z /= np.sum(interim_n_of_z * z_difs[np.newaxis, :])\n",
    "assert np.isclose(np.sum(interim_n_of_z * z_difs[np.newaxis, :]), 1.)\n",
    "interim_ln_n_of_z = safe_log(interim_n_of_z)\n",
    "\n",
    "# WMAP, with 10 * errors so we can see what's going on in crappy plots\n",
    "interim_H0 = 70.0\n",
    "delta_H0 = 2.2 * 10.\n",
    "interim_Om0 = 1. - 0.721\n",
    "delta_Om0 = 0.025 * 10.\n",
    "interim_cosmo_hyperparams = np.array([interim_H0, interim_Om0])\n",
    "interim_cosmo_hyperparam_sigmas = np.array([delta_H0, delta_Om0])\n",
    "interim_cosmo_hyperparam_vars =  interim_cosmo_hyperparam_sigmas * np.eye(n_hyperparams)\n",
    "interim_dist = sps.multivariate_normal(mean = interim_cosmo_hyperparams, cov = interim_cosmo_hyperparam_vars)\n",
    "interim_cosmo = cosmology.FlatLambdaCDM(H0=interim_H0, Om0=interim_Om0)\n",
    "\n",
    "# may have to change this if nontrivial covariances between hyperparameters\n",
    "interim_hyperparams = {}\n",
    "interim_hyperparams['theta'] = np.array([interim_cosmo_hyperparams, interim_cosmo_hyperparam_sigmas])\n",
    "interim_hyperparams['phi'] = interim_ln_n_of_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverter(z, mu):\n",
    "    #note: this inverter is slow! perhaps we could speed it up with interpolation over predefined grids?\n",
    "    def cosmo_helper(hyperparams):\n",
    "        return np.array([abs(cosmology.FlatLambdaCDM(H0=hyperparams[0], Om0=hyperparams[1]).distmod(z).value - mu)])\n",
    "    solved_cosmo = spo.minimize(cosmo_helper, interim_hyperparams['theta'][0], method=\"Nelder-Mead\", options={\"maxfev\": 1e5, \"maxiter\":1e5})\n",
    "    ln_prob = interim_dist.logpdf(solved_cosmo.x)\n",
    "    return ln_prob#max(prob, sys.float_info.epsilon)\n",
    "\n",
    "# note the approximation of cdf[z_min, z_max] = pdf[z_mid]\n",
    "interim_sheet = np.zeros((n_zs, n_mus))\n",
    "for z in range(n_zs):\n",
    "    for mu in range(n_mus):\n",
    "        ln_prob = inverter(z_mids[z], mu_mids[mu])\n",
    "        interim_sheet[z][mu] = ln_prob\n",
    "interim_ln_prior = interim_ln_n_of_z[:, np.newaxis] + interim_sheet[np.newaxis, :]\n",
    "interim_prior = np.exp(interim_ln_prior)\n",
    "interim_prior /= np.sum(interim_prior * z_difs[np.newaxis, :, np.newaxis] * mu_difs[np.newaxis, np.newaxis, :])\n",
    "interim_ln_prior = safe_log(interim_prior)\n",
    "assert np.isclose(np.sum(interim_prior * z_difs[np.newaxis, :, np.newaxis] * mu_difs[np.newaxis, np.newaxis, :]), 1.)\n",
    "\n",
    "# the interim prior and truth are way too close to each other. . . but that's realistic\n",
    "plt.pcolormesh(z_mids, mu_mids, interim_ln_prior[0].T, cmap='viridis')#, vmin = 0., vmax = 3.)\n",
    "plt.plot(z_mids, [true_cosmo.distmod(z).value for z in z_mids], color='k', label='true Hubble relation')\n",
    "plt.title('interim prior distribution')\n",
    "plt.xlabel(r'$z$')\n",
    "plt.ylabel(r'$\\mu$')\n",
    "plt.legend(loc='lower right', fontsize='small')\n",
    "plt.axis([z_bins[0], z_bins[-1], mu_bins[0], mu_bins[-1]])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping it up\n",
    "\n",
    "[add all log-likelihoods and log-interim prior]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note likelihoods were not normalized; this must be normalized\n",
    "interim_ln_posteriors = reg_vals(ln_likelihoods + interim_ln_prior[np.newaxis, :])\n",
    "interim_posteriors = np.exp(interim_ln_posteriors)\n",
    "interim_posteriors /= np.sum(interim_posteriors * z_difs[np.newaxis, :, np.newaxis] * mu_difs[np.newaxis, np.newaxis, :])\n",
    "interim_ln_posteriors = safe_log(interim_posteriors)\n",
    "assert np.isclose(np.sum(interim_posteriors * z_difs[np.newaxis, :, np.newaxis] * mu_difs[np.newaxis, np.newaxis, :]), 1.)\n",
    "\n",
    "# these are going to get a lot narrower\n",
    "fig = plt.figure(figsize=(n_types*len(colors), n_sne*len(colors)))\n",
    "p = 0\n",
    "for s in range(n_sne)[:len(colors)]:\n",
    "    for t in range(n_types):\n",
    "        p += 1\n",
    "        plt.subplot(n_sne, n_types, p)\n",
    "        plt.pcolormesh(z_mids, mu_mids, interim_ln_posteriors[s][t].T, cmap='viridis')#, vmin = 0., vmax = 3.)\n",
    "        plt.colorbar()\n",
    "        plt.scatter(true_params[s]['z'], true_params[s]['mu'], color='r')\n",
    "        plt.title('true '+true_params[s]['t']+', class '+types[t])\n",
    "        plt.xlabel(r'$z$')\n",
    "        plt.ylabel(r'$\\mu$')\n",
    "        plt.axis([z_bins[0], z_bins[-1], mu_bins[0], mu_bins[-1]])\n",
    "plt.savefig('interim_posteriors.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[write data to file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write true hyperparameters just to check\n",
    "\n",
    "truth = {}\n",
    "binned_n_of_z = np.zeros((n_types, n_zs))\n",
    "for t in range(n_types):\n",
    "    cdfs = true_n_of_z[t].cdf(z_bins)\n",
    "    binned_n_of_z[t] = (cdfs[1:] - cdfs[:-1])\n",
    "binned_n_of_z = frac_types[:, np.newaxis] * np.array(binned_n_of_z)# / z_range\n",
    "binned_n_of_z /= np.sum(binned_n_of_z * z_difs[np.newaxis, :])\n",
    "assert np.isclose(np.sum(binned_n_of_z * z_difs[np.newaxis, :]), 1.)\n",
    "truth['phi'] = binned_n_of_z\n",
    "truth['theta'] = true_hyperparams\n",
    "truth['data'] = true_params\n",
    "\n",
    "with open('../truth.hkl', 'w') as true_file:\n",
    "    hickle.dump(truth, true_file)\n",
    "\n",
    "# write axes (types, z_bins, mu_bins)\n",
    "# write interim prior (interim_ln_prior)\n",
    "# write interim posteriors (ln_interim_posteriors)\n",
    "\n",
    "output = {'types': types, 'z_bins': z_bins, 'mu_bins': mu_bins}\n",
    "output['interim ln prior'] = interim_ln_prior\n",
    "output['interim ln posteriors'] = interim_ln_posteriors\n",
    "\n",
    "with open('../data.hkl', 'w') as out_file:\n",
    "    hickle.dump(output, out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch space (Please ignore after this point!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I was sort of proud of writing this function that taught me some array manipulation stuff, so I'm keeping it here for now.\n",
    "def sample_discrete(dist, N, discs, difs, bins):\n",
    "    out_info = []\n",
    "    norm_dist = dist * difs[np.newaxis, :]\n",
    "    assert np.isclose(np.sum(norm_dist), 1.)\n",
    "    dist_shape = np.shape(norm_dist)\n",
    "    flat_dist = norm_dist.flatten()\n",
    "    cdf = np.cumsum(flat_dist)\n",
    "    for n in range(N):\n",
    "        each = {}\n",
    "        r = np.random.random()\n",
    "        k = bisect.bisect(cdf, r)\n",
    "        (t_ind, z_ind) = np.unravel_index(k, dist_shape)\n",
    "        each['t'] = discs[t_ind]\n",
    "        each['z'] = np.random.uniform(low=bins[z_ind], high=bins[z_ind + 1])\n",
    "        out_info.append(each)\n",
    "    return out_info"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
