{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mock Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scippr` requires inputs in the form of catalogs $\\{p(t_{n}, z_{n}, \\mu_{n} | \\underline{\\ell}_{n}, \\vec{m}_{n}, \\underline{\\phi}^{*}, \\vec{\\theta}^{*})\\}_{N}$ of interim posteriors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import daft\n",
    "import astropy.cosmology as cosmology\n",
    "\n",
    "import numpy as np\n",
    "import bisect\n",
    "import sys\n",
    "import scipy.stats as sps\n",
    "import scipy.optimize as spo\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib import rc\n",
    "rc(\"font\", family=\"serif\", size=12)\n",
    "rc(\"text\", usetex=True)\n",
    "\n",
    "colors = 'rbgcymk'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scippr` is based on a probabilistic graphical model, illustrated below.  The model has two types of observables, supernova lightcurves $\\underline{\\ell}_{n}$ and host galaxy photometry $\\vec{m}_{n}$.  The latent variables of supernova type $t_{n}$, redshift $z_{n}$, and distance modulus $\\mu_{n}$ are parameters over which we will marginalize, without ever directly inferring them, and while all three of them influence $\\underline{\\ell}_{n}$, only $z_{n}$ affects $\\vec{m}_{n}$.  The hyperparameters we would like to estimate are the redshift-dependent supernova type proportions $\\underline{\\phi}$ that determine $t_{n}$ and $z_{n}$ and the cosmological parameters $\\vec{\\theta}$ that relate $z_{n}$ to $\\mu_{n}$.  Thus far, the model makes the following assumptions that will be addressed in a future revision:\n",
    "\n",
    "* There is no correlation between the host galaxy photometry and either the supernova type or the lightcurve.\n",
    "* The supernova selection function does not affect the inference of the cosmological parameters, i.e. the observed supernovae are perfectly representative of the set of all supernovae in the universe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#initialize the PGM\n",
    "pgm = daft.PGM([5, 6], origin=[0, 0])\n",
    "\n",
    "#desired hyperparameters\n",
    "pgm.add_node(daft.Node(\"cosmology\", r\"$\\vec{\\theta}$\", 1., 5.5))\n",
    "pgm.add_node(daft.Node(\"dist\", r\"$\\underline{\\phi}$\", 2.5, 5.5))\n",
    "#pgm.add_node(daft.Node(\"rates\", r\"$\\vec{R}$\", 3., 5.5, fixed=True))\n",
    "\n",
    "#latent variables/parameters\n",
    "pgm.add_node(daft.Node(\"distance\", r\"$\\mu_{n}$\", 1., 4.))\n",
    "pgm.add_node(daft.Node(\"redshift\", r\"$z_{n}$\", 2., 4.5))\n",
    "pgm.add_node(daft.Node(\"type\", r\"$t_{n}$\", 3., 4.5))\n",
    "\n",
    "#data\n",
    "pgm.add_node(daft.Node(\"lightcurve\", r\"$\\underline{\\ell}_{n}$\", 1.5, 3., observed=True))\n",
    "pgm.add_node(daft.Node(\"photometry\", r\"$\\vec{m}_{n}$\", 3., 3., observed=True))\n",
    "\n",
    "# Add in the edges.\n",
    "pgm.add_edge(\"dist\", \"type\")\n",
    "pgm.add_edge(\"cosmology\", \"distance\")\n",
    "pgm.add_edge(\"dist\", \"redshift\")\n",
    "pgm.add_edge(\"redshift\", \"distance\")\n",
    "#pgm.add_edge(\"distance\", \"photometry\")\n",
    "pgm.add_edge(\"distance\", \"lightcurve\")\n",
    "pgm.add_edge(\"redshift\", \"photometry\")\n",
    "pgm.add_edge(\"redshift\", \"lightcurve\")\n",
    "pgm.add_edge(\"type\", \"lightcurve\")\n",
    "\n",
    "# plates\n",
    "pgm.add_plate(daft.Plate([0.5, 2., 3., 3.], label=r\"$n = 1, \\cdots, N$\"))\n",
    "\n",
    "# Render and save.\n",
    "pgm.render()\n",
    "pgm.figure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simulate mock data for `scippr`, there are three main steps.\n",
    "\n",
    "1. Choose true values for the hyperparameters, which we would like to recover from our inference, and the parameters, over which we intend to marginalize.\n",
    "2. Create likelihoods based on a model for how they are derived from observations.\n",
    "3. Make interim posteriors by assuming interim priors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing true hyperparameters and parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[the true redshift-dependent type rate distribution, with plot of three functions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "types = ['Ia', 'Ibc', 'II']\n",
    "n_types = len(types)\n",
    "frac_types = np.array([0.2, 0.3, 0.5])\n",
    "assert np.isclose(np.sum(frac_types), 1.)\n",
    "\n",
    "n_zs = 20\n",
    "min_z = 0.5\n",
    "max_z = 2.\n",
    "z_bins = np.linspace(min_z, max_z, num=n_zs + 1, endpoint=True)\n",
    "z_difs = z_bins[1:] - z_bins[:-1]\n",
    "z_dif = np.mean(z_difs)\n",
    "z_range = max_z - min_z\n",
    "z_mids = (z_bins[1:] + z_bins[:-1]) / 2.\n",
    "\n",
    "n_of_z = np.zeros((n_types, n_zs))\n",
    "n_of_z[0] += sps.norm(loc = 1.5, scale = 0.5).pdf(z_mids)\n",
    "n_of_z[1] += sps.norm(loc = 1., scale = 0.5).pdf(z_mids)\n",
    "n_of_z[2] += sps.norm(loc = 0.5, scale = 0.5).pdf(z_mids)\n",
    "n_of_z /= np.sum(n_of_z * z_difs[np.newaxis, :], axis=1)[:, np.newaxis]\n",
    "\n",
    "true_n_of_z = frac_types[:, np.newaxis] * np.array(n_of_z)# / z_range\n",
    "true_n_of_z /= np.sum(true_n_of_z * z_difs[np.newaxis, :])\n",
    "assert np.isclose(np.sum(true_n_of_z * z_difs[np.newaxis, :]), 1.)\n",
    "\n",
    "for t in range(n_types):\n",
    "    plt.plot(z_mids, true_n_of_z[t], color=colors[t], label=types[t])\n",
    "plt.xlabel(r'$z$')\n",
    "plt.ylabel(r'relative rate')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[samples of t, z from the true redshift-dependent type rate distribution, with histograms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sample_discrete(dist, N):\n",
    "    out_info = []\n",
    "    norm_dist = dist * z_difs[np.newaxis, :]\n",
    "    assert np.isclose(np.sum(norm_dist), 1.)\n",
    "    dist_shape = np.shape(norm_dist)\n",
    "    flat_dist = norm_dist.flatten()\n",
    "    cdf = np.cumsum(flat_dist)\n",
    "    for n in range(N):\n",
    "        each = {}\n",
    "        r = np.random.random()\n",
    "        k = bisect.bisect(cdf, r)\n",
    "        (t_ind, z_ind) = np.unravel_index(k, dist_shape)\n",
    "        each['t'] = types[t_ind]\n",
    "        each['z'] = np.random.uniform(low=z_bins[z_ind], high=z_bins[z_ind + 1])\n",
    "        out_info.append(each)\n",
    "    return out_info\n",
    "\n",
    "n_sne = 50\n",
    "\n",
    "true_params = sample_discrete(true_n_of_z, n_sne)\n",
    "\n",
    "to_plot = [[d['z'] for d in true_params if d['t'] == types[t]] for t in range(n_types)]\n",
    "for t in range(n_types):\n",
    "    plt.plot(z_mids, true_n_of_z[t]*3., color=colors[t], label=types[t])\n",
    "    plt.hist(to_plot[t], color=colors[t], alpha=1./3., label=types[t], normed=True)\n",
    "plt.xlabel(r'$z$')\n",
    "plt.ylabel(r'relative rate')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[the true cosmology, true mu for each t, z, plot hubble diagram]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Planck\n",
    "true_H0 = 67.9\n",
    "true_Om0 = 1. - 0.693\n",
    "true_hyperparams = np.array([true_H0, true_Om0])\n",
    "n_hyperparams = len(true_hyperparams)\n",
    "true_cosmo = cosmology.FlatLambdaCDM(H0=true_H0, Om0=true_Om0)\n",
    "\n",
    "for n in range(n_sne):\n",
    "    true_params[n]['mu'] = true_cosmo.distmod(true_params[n]['z']).value\n",
    "    \n",
    "to_plot_x = [[d['z'] for d in true_params if d['t'] == types[t]] for t in range(n_types)]\n",
    "to_plot_y = [[d['mu'] for d in true_params if d['t'] == types[t]] for t in range(n_types)]\n",
    "for t in range(n_types):\n",
    "    plt.scatter(to_plot_x[t], to_plot_y[t], color=colors[t], label=types[t])\n",
    "plt.plot(z_mids, [true_cosmo.distmod(z).value for z in z_mids], color='k')\n",
    "plt.xlabel(r'$z$')\n",
    "plt.ylabel(r'$\\mu$')\n",
    "plt.legend()\n",
    "plt.title(r'$H_{0}='+str(true_H0)+r', \\Omega_{m,0}='+str(true_Om0)+r'$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[the confusion matrix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conf_matrix = 0.25 + 0.25 * np.eye(3)#np.ones((n_types, n_types)) / n_types **2\n",
    "assert np.isclose(np.sum(conf_matrix, axis=1).all(), frac_types.all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[set up mu parametrization]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_mus = n_zs\n",
    "min_mu, max_mu = min([s['mu'] for s in true_params]) - 0.5, max([s['mu'] for s in true_params]) + 0.5\n",
    "mu_bins = np.linspace(min_mu, max_mu, num=n_mus + 1, endpoint=True)#true_cosmo.distmod(z_bins).value\n",
    "mu_difs = mu_bins[1:] - mu_bins[:-1]\n",
    "mu_dif = np.mean(mu_difs)\n",
    "mu_range = np.max(mu_bins) - np.min(mu_bins)\n",
    "mu_mids = (mu_bins[1:] + mu_bins[:-1]) / 2.\n",
    "\n",
    "z_mu_grid = np.array([[(z, mu) for mu in mu_mids] for z in z_mids])\n",
    "cake_shape = np.shape(z_mu_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[the functions taking true type, true z, and true mu returning p(type, z, mu | hat(t))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Ia_Ia_var = np.array([0.01, 0.05])\n",
    "Ibc_Ia_delta = 1.\n",
    "Ibc_Ia_var = np.array([0.01, 0.01])\n",
    "II_Ia_delta = np.mean(mu_mids)\n",
    "II_Ia_var = np.array([0.05, 0.1])\n",
    "\n",
    "def fit_Ia(z, mu):\n",
    "    cake = np.zeros((n_types, n_zs, n_mus))\n",
    "    cake_Ia = sps.multivariate_normal(mean = np.array([z, mu]), cov = Ia_Ia_var * np.eye(2))\n",
    "    [z_samp, mu_samp] = cake_Ia.rvs()\n",
    "    cake_Ia = sps.multivariate_normal(mean = np.array([z_samp, mu_samp]), cov = Ia_Ia_var * np.eye(2))\n",
    "    cake[0] = cake_Ia.pdf(z_mu_grid.reshape(-1, cake_shape[-1])).reshape(cake_shape[:-1])\n",
    "    cake[1] = np.ones(cake_shape[:-1]) / np.prod(cake_shape[:-1])\n",
    "    cake[2] = np.ones(cake_shape[:-1]) / np.prod(cake_shape[:-1])\n",
    "    cake *= conf_matrix[:, 0, np.newaxis, np.newaxis]\n",
    "    return cake\n",
    "    \n",
    "def fit_Ibc(z, mu):\n",
    "    cake = np.zeros((n_types, n_zs, n_mus))\n",
    "    cake_Ia = sps.multivariate_normal(mean = np.array([z, mu - Ibc_Ia_delta]), cov = Ibc_Ia_var * np.eye(2))\n",
    "    [z_samp, mu_samp] = cake_Ia.rvs()\n",
    "    cake_Ia = sps.multivariate_normal(mean = np.array([z_samp, mu_samp]), cov = Ibc_Ia_var * np.eye(2))\n",
    "    cake[0] = cake_Ia.pdf(z_mu_grid.reshape(-1, cake_shape[-1])).reshape(cake_shape[:-1])\n",
    "    cake[1] = np.ones(cake_shape[:-1]) / np.prod(cake_shape[:-1])\n",
    "    cake[2] = np.ones(cake_shape[:-1]) / np.prod(cake_shape[:-1])\n",
    "    cake *= conf_matrix[:, 1, np.newaxis, np.newaxis]\n",
    "    return cake\n",
    "    \n",
    "def fit_II(z, mu):\n",
    "    cake = np.zeros((n_types, n_zs, n_mus))\n",
    "    cake_Ia = sps.multivariate_normal(mean = np.array([z, II_Ia_delta]), cov = II_Ia_var * np.eye(2))\n",
    "    [z_samp, mu_samp] = cake_Ia.rvs()\n",
    "    cake_Ia = sps.multivariate_normal(mean = np.array([z_samp, mu_samp]), cov = II_Ia_var * np.eye(2))\n",
    "    cake[0] = cake_Ia.pdf(z_mu_grid.reshape(-1, cake_shape[-1])).reshape(cake_shape[:-1])\n",
    "    cake[1] = np.ones(cake_shape[:-1]) / np.prod(cake_shape[:-1])\n",
    "    cake[2] = np.ones(cake_shape[:-1]) / np.prod(cake_shape[:-1])\n",
    "    cake *= conf_matrix[:, 2, np.newaxis, np.newaxis]\n",
    "    return cake\n",
    "    \n",
    "def fit_any(true_vals):\n",
    "    if true_vals['t'] == 'Ia':\n",
    "        cake = fit_Ia(true_vals['z'], true_vals['mu'])\n",
    "    if true_vals['t'] == 'Ibc':\n",
    "        cake = fit_Ibc(true_vals['z'], true_vals['mu'])\n",
    "    if true_vals['t'] == 'II':\n",
    "        cake = fit_II(true_vals['z'], true_vals['mu'])\n",
    "    return cake\n",
    "\n",
    "def fit_all(catalog):\n",
    "    dessert = []\n",
    "    for true_vals in catalog:\n",
    "        dessert.append(fit_any(true_vals))\n",
    "    return np.array(dessert)\n",
    "\n",
    "sheet_cake = fit_all(true_params)\n",
    "\n",
    "fig = plt.figure(figsize=(n_types*len(colors), n_sne*len(colors)))\n",
    "p = 0\n",
    "for s in range(n_sne)[:len(colors)]:\n",
    "    for t in range(n_types):\n",
    "        p += 1\n",
    "        plt.subplot(n_sne, n_types, p)\n",
    "        plt.pcolormesh(z_mids, mu_mids, sheet_cake[s][t].T, cmap='viridis', vmin = 0., vmax = 3.)\n",
    "        plt.colorbar()\n",
    "        plt.scatter(true_params[s]['z'], true_params[s]['mu'], color='k')\n",
    "#         ax.set_xticklabels(z_mids)\n",
    "#         ax.set_yticklabels(mu_mids)\n",
    "        plt.title('true '+true_params[s]['t']+', class '+types[t])\n",
    "        plt.xlabel(r'$z$')\n",
    "        plt.ylabel(r'$\\mu$')\n",
    "        plt.axis([z_bins[0], z_bins[-1], mu_bins[0], mu_bins[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[make p(z)s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pzs = []\n",
    "for s in range(n_sne):\n",
    "    dist = sps.norm(loc = true_params[s]['z'], scale = 0.03)\n",
    "    pz_mean = dist.rvs()\n",
    "    pz = sps.norm(loc = pz_mean, scale = 0.03).pdf(z_mids)\n",
    "    pzs.append(pz)\n",
    "pzs = np.array(pzs)\n",
    "\n",
    "for s in range(n_sne)[:len(colors)]:\n",
    "    plt.plot(z_mids, pzs[s], color=colors[s])\n",
    "    plt.vlines(true_params[s]['z'], 0., 15., color=colors[s])\n",
    "plt.xlabel(r'$z$')\n",
    "plt.ylabel(r'$p(z)$')\n",
    "plt.title(r'host galaxy $p(z)$ distributions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[multiply likelihood components]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "likelihoods = sheet_cake * pzs[:, np.newaxis, :, np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making interim posteriors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[interim priors to make interim posteriors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "interim_n_of_z = np.ones((n_types, n_zs))\n",
    "interim_n_of_z /= np.sum(interim_n_of_z * z_difs[np.newaxis, :])\n",
    "assert np.isclose(np.sum(interim_n_of_z * z_difs[np.newaxis, :]), 1.)\n",
    "\n",
    "#WMAP\n",
    "interim_H0 = 70.0\n",
    "delta_H0 = 2.2 * 10.\n",
    "interim_Om0 = 1. - 0.721\n",
    "delta_Om0 = 0.025 * 10.\n",
    "interim_hyperparams = np.array([interim_H0, interim_Om0])\n",
    "interim_hyperparam_vars = np.array([delta_H0, delta_Om0]) * np.eye(n_hyperparams)\n",
    "interim_dist = sps.multivariate_normal(mean = interim_hyperparams, cov = interim_hyperparam_vars)\n",
    "interim_cosmo = cosmology.FlatLambdaCDM(H0=interim_hyperparams[0], Om0=interim_hyperparams[1])\n",
    "\n",
    "def inverter(z, mu):\n",
    "    def cosmo_helper(hyperparams):\n",
    "        return np.array([abs(cosmology.FlatLambdaCDM(H0=hyperparams[0], Om0=hyperparams[1]).distmod(z).value - mu)])\n",
    "    solved_cosmo = spo.minimize(cosmo_helper, interim_hyperparams, method=\"Nelder-Mead\", options={\"maxfev\": 1e5, \"maxiter\":1e5})\n",
    "    prob = interim_dist.pdf(solved_cosmo.x)\n",
    "#     print(z, mu, solved_cosmo.x, max(prob, sys.float_info.epsilon))\n",
    "    return max(prob, sys.float_info.epsilon)\n",
    "\n",
    "interim_sheet = np.zeros((n_zs, n_mus))\n",
    "for z in range(n_zs):\n",
    "    for mu in range(n_mus):\n",
    "        prob = inverter(z_mids[z], mu_mids[mu])\n",
    "        interim_sheet[z][mu] = prob\n",
    "interim_prior = interim_n_of_z[:, np.newaxis] * interim_sheet[np.newaxis, :]\n",
    "interim_prior /= np.sum(interim_prior * z_difs[np.newaxis, :, np.newaxis] * mu_difs[np.newaxis, np.newaxis, :])\n",
    "assert np.isclose(np.sum(interim_prior * z_difs[np.newaxis, :, np.newaxis] * mu_difs[np.newaxis, np.newaxis, :]), 1.)\n",
    "\n",
    "plt.pcolormesh(z_mids, mu_mids, interim_sheet.T, cmap='viridis')\n",
    "plt.plot(z_mids, [true_cosmo.distmod(z).value for z in z_mids], color='k')\n",
    "plt.plot(z_mids, [interim_cosmo.distmod(z).value for z in z_mids], color='r')\n",
    "plt.title('interim prior distribution')\n",
    "plt.xlabel(r'$z$')\n",
    "plt.ylabel(r'$\\mu$')\n",
    "plt.axis([z_bins[0], z_bins[-1], mu_bins[0], mu_bins[-1]])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[multiply likelihoods and interim prior]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "interim_posteriors = likelihoods * interim_prior[np.newaxis, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing output for `scippr` inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[write data to file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write true hyperparameters\n",
    "# write axes\n",
    "# write interim prior\n",
    "# write interim posteriors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
