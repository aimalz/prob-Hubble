{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mock Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scippr` requires inputs in the form of catalogs $\\{p(t_{n}, z_{n}, \\mu_{n} | \\underline{\\ell}_{n}, \\vec{m}_{n}, \\underline{\\phi}^{*}, \\vec{\\theta}^{*})\\}_{N}$ of interim posteriors expressed as `3D` arrays constituting probabilities over $t_{n}$, $z_{n}$, and $\\mu_{n}$, enabling rapid computation of the posterior $p(\\underline{\\phi}, \\vec{\\theta} | \\{\\underline{\\ell}_{n}, \\vec{m}_{n}\\}_{N})$ over the hyperparameters $\\underline{\\phi}$ and $\\vec{\\theta}$ of scientific interest.  This notebook outlines a procedure for generating such a catalog.\n",
    "\n",
    "Perhaps the defining feature of this pipeline is that it does not involve simulating supernova lightcurves or host galaxy photometry and instead simulating the interim posteriors directly.  There are several good reasons for this choice:\n",
    "\n",
    "* The motivation for `scippr` is to develop a method for doing inference with accurate probability distributions over relevant supernova parameters, not to develop methods for obtaining those probability distributions.\n",
    "* We avoid tying our inference method to a particular way of deriving interim posteriors from observed data.\n",
    "* We avoid making assumptions about the details of the observed data, such as the photometric filters, intrinsic lightcurves, and observing conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import daft\n",
    "import astropy.cosmology as cosmology\n",
    "\n",
    "import numpy as np\n",
    "import bisect\n",
    "import sys\n",
    "import scipy.stats as sps\n",
    "import scipy.optimize as spo\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib import rc\n",
    "rc(\"font\", family=\"serif\", size=12)\n",
    "rc(\"text\", usetex=True)\n",
    "\n",
    "colors = 'rbgcymk'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scippr` is based on a probabilistic graphical model, illustrated below.  The model has two types of observables, shown in shaded circles, supernova lightcurves $\\underline{\\ell}_{n}$ and host galaxy photometry $\\vec{m}_{n}$.  The parameters, which are by definition not directly observable, are shown in empty circles.  The latent variables of supernova type $t_{n}$, redshift $z_{n}$, and distance modulus $\\mu_{n}$ are parameters over which we will marginalize, without ever directly inferring them, and while all three of them influence $\\underline{\\ell}_{n}$, only $z_{n}$ affects $\\vec{m}_{n}$ in this model.  The box indicates that these latent variables and the observables are generated independently $N$ times for each supernova in the universe.  The hyperparameters we would like to estimate are the redshift-dependent supernova type proportions $\\underline{\\phi}$ that determine $t_{n}$ and $z_{n}$ and the cosmological parameters $\\vec{\\theta}$ that relate $z_{n}$ to $\\mu_{n}$, which are shared by all $N$ supernovae.  Thus far, the model makes the following assumptions that may be addressed in a future revision:\n",
    "\n",
    "* There is no correlation between the host galaxy photometry and either the supernova type or the lightcurve.\n",
    "* The supernova selection function does not affect the inference of the cosmological parameters, i.e. the observed supernovae are representative of the set of all supernovae in the universe, or, in other words, the lightcurve and host galaxy photometry of every supernova in the universe has been observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#initialize the PGM\n",
    "pgm = daft.PGM([5, 4.5], origin=[0, 0])\n",
    "\n",
    "#desired hyperparameters\n",
    "pgm.add_node(daft.Node(\"cosmology\", r\"$\\vec{\\theta}$\", 1., 4.))\n",
    "pgm.add_node(daft.Node(\"dist\", r\"$\\underline{\\phi}$\", 2.5, 4.))\n",
    "#pgm.add_node(daft.Node(\"rates\", r\"$\\vec{R}$\", 3., 5.5, fixed=True))\n",
    "\n",
    "#latent variables/parameters\n",
    "pgm.add_node(daft.Node(\"distance\", r\"$\\mu_{n}$\", 1., 2.5))\n",
    "pgm.add_node(daft.Node(\"redshift\", r\"$z_{n}$\", 2., 3.))\n",
    "pgm.add_node(daft.Node(\"type\", r\"$t_{n}$\", 3., 2.5))\n",
    "\n",
    "#data\n",
    "pgm.add_node(daft.Node(\"lightcurve\", r\"$\\underline{\\ell}_{n}$\", 1.5, 1., observed=True))\n",
    "pgm.add_node(daft.Node(\"photometry\", r\"$\\vec{m}_{n}$\", 3., 1., observed=True))\n",
    "\n",
    "# Add in the edges.\n",
    "pgm.add_edge(\"dist\", \"type\")\n",
    "pgm.add_edge(\"cosmology\", \"distance\")\n",
    "pgm.add_edge(\"dist\", \"redshift\")\n",
    "pgm.add_edge(\"redshift\", \"distance\")\n",
    "#pgm.add_edge(\"distance\", \"photometry\")\n",
    "pgm.add_edge(\"distance\", \"lightcurve\")\n",
    "pgm.add_edge(\"redshift\", \"photometry\")\n",
    "pgm.add_edge(\"redshift\", \"lightcurve\")\n",
    "pgm.add_edge(\"type\", \"lightcurve\")\n",
    "\n",
    "# plates\n",
    "pgm.add_plate(daft.Plate([0.5, 0.5, 3., 3.], label=r\"$n = 1, \\cdots, N$\"))\n",
    "\n",
    "# Render and save.\n",
    "pgm.render()\n",
    "pgm.figure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simulate a mock catalog for `scippr`, there are three main steps.\n",
    "\n",
    "1. Choose true values for the hyperparameters, which we would like to recover from our inference, and the parameters, over which we intend to marginalize.\n",
    "2. Create likelihoods based on a model for how they are derived from observations.\n",
    "3. Make interim posteriors by assuming interim priors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing true hyperparameters and parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The redshift-dependent type proportions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[the true redshift-dependent type rate distribution, with plot of three functions] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "types = ['Ia', 'Ibc', 'II']\n",
    "n_types = len(types)\n",
    "# making up the type fractions, will replace this with data soon!\n",
    "frac_types = np.array([0.2, 0.3, 0.5])\n",
    "assert np.isclose(np.sum(frac_types), 1.)\n",
    "\n",
    "# this binning is arbitrary!\n",
    "n_zs = 20\n",
    "min_z = 0.5\n",
    "max_z = 2.\n",
    "z_bins = np.linspace(min_z, max_z, num=n_zs + 1, endpoint=True)\n",
    "z_difs = z_bins[1:] - z_bins[:-1]\n",
    "z_dif = np.mean(z_difs)\n",
    "z_range = max_z - min_z\n",
    "z_mids = (z_bins[1:] + z_bins[:-1]) / 2.\n",
    "\n",
    "# it actually doesn't make sense to bin this up at this stage -- the true n_t(z) can be continuous\n",
    "n_of_z = np.zeros((n_types, n_zs))\n",
    "n_of_z[0] += sps.norm(loc = 1.5, scale = 0.5).pdf(z_mids)\n",
    "n_of_z[1] += sps.norm(loc = 1., scale = 0.5).pdf(z_mids)\n",
    "n_of_z[2] += sps.norm(loc = 0.5, scale = 0.5).pdf(z_mids)\n",
    "n_of_z /= np.sum(n_of_z * z_difs[np.newaxis, :], axis=1)[:, np.newaxis]\n",
    "\n",
    "true_n_of_z = frac_types[:, np.newaxis] * np.array(n_of_z)# / z_range\n",
    "true_n_of_z /= np.sum(true_n_of_z * z_difs[np.newaxis, :])\n",
    "assert np.isclose(np.sum(true_n_of_z * z_difs[np.newaxis, :]), 1.)\n",
    "\n",
    "for t in range(n_types):\n",
    "    plt.plot(z_mids, true_n_of_z[t], color=colors[t], label=types[t])\n",
    "plt.xlabel(r'$z$')\n",
    "plt.ylabel(r'relative rate')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[samples of t, z from the true redshift-dependent type rate distribution, with histograms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# I'm sampling from a piecewise constant function, but it should actually be continuous at this stage\n",
    "def sample_discrete(dist, N):\n",
    "    out_info = []\n",
    "    norm_dist = dist * z_difs[np.newaxis, :]\n",
    "    assert np.isclose(np.sum(norm_dist), 1.)\n",
    "    dist_shape = np.shape(norm_dist)\n",
    "    flat_dist = norm_dist.flatten()\n",
    "    cdf = np.cumsum(flat_dist)\n",
    "    for n in range(N):\n",
    "        each = {}\n",
    "        r = np.random.random()\n",
    "        k = bisect.bisect(cdf, r)\n",
    "        (t_ind, z_ind) = np.unravel_index(k, dist_shape)\n",
    "        each['t'] = types[t_ind]\n",
    "        each['z'] = np.random.uniform(low=z_bins[z_ind], high=z_bins[z_ind + 1])\n",
    "        out_info.append(each)\n",
    "    return out_info\n",
    "\n",
    "n_sne = 50\n",
    "\n",
    "true_params = sample_discrete(true_n_of_z, n_sne)\n",
    "\n",
    "to_plot = [[d['z'] for d in true_params if d['t'] == types[t]] for t in range(n_types)]\n",
    "for t in range(n_types):\n",
    "    plt.plot(z_mids, true_n_of_z[t]*3., color=colors[t], label=types[t])\n",
    "    plt.hist(to_plot[t], color=colors[t], alpha=1./3., label=types[t], normed=True)\n",
    "plt.xlabel(r'$z$')\n",
    "plt.ylabel(r'relative rate')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The true cosmological parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the true hyperparameter vector $\\vec{\\theta}'$ as having two components, $H_{0}'$ and $\\Omega_{m,0}'$.  In a future revision, we may include additional cosmological parameters in this hyperparameter vector.  We choose the true values for the cosmological parameters to be those published by /Planck/.  [include citation]  \n",
    "\n",
    "Since every supernova in our sample already has a true redshift $z_{n}'$, we can easily establish a true distance modulus $\\mu_{n}'$ via the luminosity distance equation.  [insert equation here]  We plot a traditional Hubble diagram of the supernovae in our sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Planck\n",
    "true_H0 = 67.9\n",
    "true_Om0 = 1. - 0.693\n",
    "true_hyperparams = np.array([true_H0, true_Om0])\n",
    "n_hyperparams = len(true_hyperparams)\n",
    "true_cosmo = cosmology.FlatLambdaCDM(H0=true_H0, Om0=true_Om0)\n",
    "\n",
    "for n in range(n_sne):\n",
    "    true_params[n]['mu'] = true_cosmo.distmod(true_params[n]['z']).value\n",
    "    \n",
    "to_plot_x = [[d['z'] for d in true_params if d['t'] == types[t]] for t in range(n_types)]\n",
    "to_plot_y = [[d['mu'] for d in true_params if d['t'] == types[t]] for t in range(n_types)]\n",
    "for t in range(n_types):\n",
    "    plt.scatter(to_plot_x[t], to_plot_y[t], color=colors[t], label=types[t])\n",
    "plt.plot(z_mids, [true_cosmo.distmod(z).value for z in z_mids], color='k')\n",
    "plt.xlabel(r'$z$')\n",
    "plt.ylabel(r'$\\mu$')\n",
    "plt.legend()\n",
    "plt.title(r'$H_{0}='+str(true_H0)+r', \\Omega_{m,0}='+str(true_Om0)+r'$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating Probabilities\n",
    "\n",
    "`scippr` is intended to be used on interim posterior probabilities derived from a probabilistic lightcurve fitting procedure.  These will be provided as log-probabilities evaluated on a `3D` grid in type, redshift, and distance modulus space.  We choose to work with log-probabilities because they preserve numerical precision better and enable slow products of arrays to be transformed into fast sums of arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def safe_log(arr, threshold=sys.float_info.epsilon):\n",
    "    shape = np.shape(arr)\n",
    "    flat = arr.flatten()\n",
    "    logged = np.log(np.array([max(a, threshold) for a in flat])).reshape(shape)\n",
    "    return logged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating likelihoods\n",
    "\n",
    "The goal here is to simulate realistic outputs from such a procedure without having to develop and run one.   Each log-likelihood $\\ln[p(\\underline{\\ell}_{n}, \\vec{m}_{n} | t_{n}, z_{n}, \\mu_{n})]$ may be broken down into simpler components that may be summed to create log-likelihoods.\n",
    "\n",
    "\\begin{align*}\n",
    "\\ln[p(\\underline{\\ell}_{n}, \\vec{m}_{n} | t_{n}, z_{n}, \\mu_{n})] &= \\ln[p(\\underline{\\ell}_{n} | t_{n}, z_{n}, \\mu_{n})] + \\ln[p(\\vec{m}_{n} | z_{n})]\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling a photo-$z$ PDF\n",
    "\n",
    "It is simplest to start with the likelihood of host galaxy photometry $\\vec{m}_{n}$ as a function of redshift $z_{n}$.  We assume the simplest model in which photo-$z$ PDFs are Gaussians $\\mathcal{N}(z_{n}'', \\sigma_{n}^{2})$ where $z_{n}''\\sim\\mathcal{N}(z_{n}', \\sigma_{n}^{2})$.  We will also state that the variance is a constant $\\sigma_{n}\\equiv\\sigma$ for all $n$.  [cite where 0.03 came from]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# very simple p(z) model, simple gaussians\n",
    "pz_sigma = 0.03\n",
    "\n",
    "pzs = []\n",
    "for s in range(n_sne):\n",
    "    dist = sps.norm(loc = true_params[s]['z'], scale = 0.03)\n",
    "    pz_mean = dist.rvs()\n",
    "    pz = sps.norm(loc = pz_mean, scale = pz_sigma).pdf(z_mids)\n",
    "    pzs.append(pz)\n",
    "pzs = np.array(pzs)\n",
    "ln_pzs = safe_log(pzs)\n",
    "\n",
    "for s in range(n_sne)[:len(colors)]:\n",
    "    plt.plot(z_mids, pzs[s], color=colors[s])\n",
    "    plt.vlines(true_params[s]['z'], 0., 15., color=colors[s])\n",
    "plt.xlabel(r'$z$')\n",
    "plt.ylabel(r'$p(z)$')\n",
    "plt.title(r'host galaxy $p(z)$ distributions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling a lightcurve fitter\n",
    "\n",
    "Based on how existing lightcurve fitters work, a lightcurve is generally assigned a class before its redshift and distance modulus are estimated because the fitting function will differ based on the assigned class.  Thus, we may assume that the lightcurve likelihood is separable as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "\\ln[p(\\underline{\\ell}_{n} | t_{n}, z_{n}, \\mu_{n})] &= \\ln[p(\\underline{\\ell}_{n} | t_{n})] + \\ln[p(\\underline{\\ell}_{n}, t_{n} | z_{n}, \\mu_{n})]\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Lightcurve classification*\n",
    "\n",
    "The confusion matrix quantifies the probabilities that an item is truly of a certain class given the fact that it has been classified as a different class.  (A more in-depth description of the confusion matrix can be found [here](https://github.com/rbiswas4/SNeLightcurveQualityMetric/blob/master/classification_metric.tex).)  We will use the $p(t_{n}' | \\hat{t}_{n})$ elements of the confusion matrix as a proxy for $p(\\underline{\\ell}_{n} | t_{n})$.  The confusion matrix is specific to each classification method, so we will have to choose one to simulate a realistic mock dataset.  For now, we proceed assuming a fairly trivial confusion matrix giving a 50% chance of correct classification for each type and equal probabilities for all misclassifications.  Obviously this will be revised in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# will need to take this from data of some kind, arbitrary for now\n",
    "conf_matrix = 0.25 + 0.25 * np.eye(3)\n",
    "assert np.isclose(np.sum(conf_matrix, axis=1).all(), frac_types.all())\n",
    "ln_conf_matrix = safe_log(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[set up mu parametrization]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# want this to be agnostic about true cosmology\n",
    "n_mus = n_zs\n",
    "min_mu, max_mu = min([s['mu'] for s in true_params]) - 0.5, max([s['mu'] for s in true_params]) + 0.5\n",
    "mu_bins = np.linspace(min_mu, max_mu, num=n_mus + 1, endpoint=True)\n",
    "mu_difs = mu_bins[1:] - mu_bins[:-1]\n",
    "mu_dif = np.mean(mu_difs)\n",
    "mu_range = np.max(mu_bins) - np.min(mu_bins)\n",
    "mu_mids = (mu_bins[1:] + mu_bins[:-1]) / 2.\n",
    "\n",
    "z_mu_grid = np.array([[(z, mu) for mu in mu_mids] for z in z_mids])\n",
    "cake_shape = np.shape(z_mu_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*$\\chi^{2}$ lightcurve fitting*\n",
    "\n",
    "In order to produce $p(\\underline{\\ell}_{n}, t_{n} | z_{n}, \\mu_{n})$, we will again introduce the idea of using the true type $t_{n}'$ as a proxy for the lightcurve $\\underline{\\ell}_{n}$ and a classified type $\\hat{t}_{n}$ for the variable type $t_{n}$ that appears in the probability expressions.  If we do this, the quantity we want is really $p(t_{n}', \\hat{t}_{n} | z_{n}, \\mu_{n})$.  We can obtain this knowing how lightcurve fitters, in general, estimate redshifts $z_{n}$ and distance moduli $\\mu_{n}$ under all possible combinations of $t_{n}'$ and $\\hat{t}_{n}$.  We will construct functions that aim to simulate the signatures of misclassification in the Hubble diagram.  [find a figure to link to]  Currently, we use placeholder functions that will be replaced later on as information about the consequences of fitting with the wrong function becomes available.  The table below summarizes the fitting function for each true type given a classification of type $Ia$.  All other classified types are assumed to give a distribution that is uniform in $\\mu_{n}$ and Gaussian in $z_{n}$ according to the same prescription used for the photo-$z$ PDFs.\n",
    "\n",
    "| True Type | Functional Form of Ia likelihood |\n",
    "| :-------: | :------------------------------: |\n",
    "| Ia | $\\vec{\\mathcal{N}}((z_{n}'', \\mu_{n}''), \\underline{\\Sigma}_{n})$ where $\\underline{\\Sigma}_{n}=(\\sigma^{2}_{z, Ia}, \\sigma^{2}_{\\mu, Ia})\\times\\underline{I}$ and $(z_{n}'', \\mu_{n}'')\\sim\\vec{\\mathcal{N}}((z_{n}', \\mu_{n}'), \\underline{\\Sigma}_{n})$ |\n",
    "| Ibc | $\\vec{\\mathcal{N}}((z_{n}'', \\mu_{n}''), \\underline{\\Sigma}_{n})$ where $\\underline{\\Sigma}_{n}=(\\sigma^{2}_{z, Ibc}, \\sigma^{2}_{\\mu, Ibc})\\times\\underline{I}$ and $(z_{n}'', \\mu_{n}'')\\sim\\vec{\\mathcal{N}}((z_{n}', \\mu_{n}' - C_{Ibc}), \\underline{\\Sigma}_{n})$ for survey-wide constant $C_{Ibc}$ |\n",
    "| II | $\\vec{\\mathcal{N}}((z_{n}'', \\mu_{n}''), \\underline{\\Sigma}_{n})$ where $\\underline{\\Sigma}_{n}=(\\sigma^{2}_{z, II}, \\sigma^{2}_{\\mu, II})\\times\\underline{I}$ and $(z_{n}'', \\mu_{n}'')\\sim\\vec{\\mathcal{N}}((z_{n}', C_{II}), \\underline{\\Sigma}_{n})$ for survey-wide constant $C_{II}$ |\n",
    "\n",
    "In a future revision, the values of $\\sigma^{2}_{z, \\tau}$ and $\\sigma^{2}_{\\mu, \\tau}$ for each type $\\tau$ will be replaced by random variables themselves representing the intrinsic variation among lightcurves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# must set nuisance parameters inherent in process of producing interim posteriors from lightcurves\n",
    "Ia_Ia_var = np.array([0.01, 0.02])\n",
    "Ibc_Ia_delta = 1.\n",
    "Ibc_Ia_var = np.array([0.01, 0.01])\n",
    "II_Ia_delta = np.mean(mu_mids)\n",
    "II_Ia_var = np.array([0.01, 0.05])\n",
    "z_sigma = 0.03\n",
    "\n",
    "# definitely needs more work on what (z, mu) distributions are expected when lightcurves are fit with wrong templates\n",
    "# just made it flat for now\n",
    "\n",
    "def fit_Ia(z, mu):\n",
    "    cake = np.zeros((n_types, n_zs, n_mus))\n",
    "    cake_Ia = sps.multivariate_normal(mean = np.array([z, mu]), cov = Ia_Ia_var * np.eye(2))\n",
    "    [z_samp, mu_samp] = cake_Ia.rvs()\n",
    "    cake_Ia = sps.multivariate_normal(mean = np.array([z_samp, mu_samp]), cov = Ia_Ia_var * np.eye(2))\n",
    "    cake[0] = cake_Ia.pdf(z_mu_grid.reshape(-1, cake_shape[-1])).reshape(cake_shape[:-1])\n",
    "    dist = sps.norm(loc = z, scale = z_sigma)\n",
    "    z_means = dist.rvs(2)\n",
    "    cake[1] = sps.norm(loc = z_means[0], scale = z_sigma).pdf(z_mids)[:, np.newaxis] / n_mus#np.ones(cake_shape[:-1]) / np.prod(cake_shape[:-1])\n",
    "    cake[2] = sps.norm(loc = z_means[1], scale = z_sigma).pdf(z_mids)[:, np.newaxis] / n_mus#np.ones(cake_shape[:-1]) / np.prod(cake_shape[:-1])\n",
    "    ln_cake = safe_log(cake) + ln_conf_matrix[:, 0, np.newaxis, np.newaxis]\n",
    "    return ln_cake\n",
    "    \n",
    "def fit_Ibc(z, mu):\n",
    "    cake = np.zeros((n_types, n_zs, n_mus))\n",
    "    cake_Ia = sps.multivariate_normal(mean = np.array([z, mu - Ibc_Ia_delta]), cov = Ibc_Ia_var * np.eye(2))\n",
    "    [z_samp, mu_samp] = cake_Ia.rvs()\n",
    "    cake_Ia = sps.multivariate_normal(mean = np.array([z_samp, mu_samp]), cov = Ibc_Ia_var * np.eye(2))\n",
    "    cake[0] = cake_Ia.pdf(z_mu_grid.reshape(-1, cake_shape[-1])).reshape(cake_shape[:-1])\n",
    "    dist = sps.norm(loc = z, scale = z_sigma)\n",
    "    z_means = dist.rvs(2)\n",
    "    cake[1] = sps.norm(loc = z_means[0], scale = z_sigma).pdf(z_mids)[:, np.newaxis] / n_mus#np.ones(cake_shape[:-1]) / np.prod(cake_shape[:-1])\n",
    "    cake[2] = sps.norm(loc = z_means[1], scale = z_sigma).pdf(z_mids)[:, np.newaxis] / n_mus#np.ones(cake_shape[:-1]) / np.prod(cake_shape[:-1])\n",
    "    ln_cake = safe_log(cake) + ln_conf_matrix[:, 1, np.newaxis, np.newaxis]\n",
    "    return ln_cake\n",
    "    \n",
    "def fit_II(z, mu):\n",
    "    cake = np.zeros((n_types, n_zs, n_mus))\n",
    "    cake_Ia = sps.multivariate_normal(mean = np.array([z, II_Ia_delta]), cov = II_Ia_var * np.eye(2))\n",
    "    [z_samp, mu_samp] = cake_Ia.rvs()\n",
    "    cake_Ia = sps.multivariate_normal(mean = np.array([z_samp, mu_samp]), cov = II_Ia_var * np.eye(2))\n",
    "    cake[0] = cake_Ia.pdf(z_mu_grid.reshape(-1, cake_shape[-1])).reshape(cake_shape[:-1])\n",
    "    dist = sps.norm(loc = z, scale = z_sigma)\n",
    "    z_means = dist.rvs(2)\n",
    "    cake[1] = sps.norm(loc = z_means[0], scale = z_sigma).pdf(z_mids)[:, np.newaxis] / n_mus#np.ones(cake_shape[:-1]) / np.prod(cake_shape[:-1])\n",
    "    cake[2] = sps.norm(loc = z_means[1], scale = z_sigma).pdf(z_mids)[:, np.newaxis] / n_mus#np.ones(cake_shape[:-1]) / np.prod(cake_shape[:-1])\n",
    "    ln_cake = safe_log(cake) + ln_conf_matrix[:, 2, np.newaxis, np.newaxis]\n",
    "    return ln_cake\n",
    "    \n",
    "def fit_any(true_vals):\n",
    "    if true_vals['t'] == 'Ia':\n",
    "        ln_cake = fit_Ia(true_vals['z'], true_vals['mu'])\n",
    "    if true_vals['t'] == 'Ibc':\n",
    "        ln_cake = fit_Ibc(true_vals['z'], true_vals['mu'])\n",
    "    if true_vals['t'] == 'II':\n",
    "        ln_cake = fit_II(true_vals['z'], true_vals['mu'])\n",
    "    return ln_cake\n",
    "\n",
    "def fit_all(catalog):\n",
    "    dessert = []\n",
    "    for true_vals in catalog:\n",
    "        dessert.append(fit_any(true_vals))\n",
    "    return np.array(dessert)\n",
    "\n",
    "sheet_cake = fit_all(true_params)\n",
    "\n",
    "# happily, these look like what we see in contaminated hubble diagrams!\n",
    "fig = plt.figure(figsize=(n_types*len(colors), n_sne*len(colors)))\n",
    "p = 0\n",
    "for s in range(n_sne)[:len(colors)]:\n",
    "    for t in range(n_types):\n",
    "        p += 1\n",
    "        plt.subplot(n_sne, n_types, p)\n",
    "        plt.pcolormesh(z_mids, mu_mids, sheet_cake[s][t].T, cmap='viridis')\n",
    "        plt.colorbar()\n",
    "        plt.scatter(true_params[s]['z'], true_params[s]['mu'], color='k')\n",
    "        plt.title('true '+true_params[s]['t']+', class '+types[t])\n",
    "        plt.xlabel(r'$z$')\n",
    "        plt.ylabel(r'$\\mu$')\n",
    "        plt.axis([z_bins[0], z_bins[-1], mu_bins[0], mu_bins[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[multiply likelihood components]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# these don't have to be normalized\n",
    "ln_likelihoods = sheet_cake + ln_pzs[:, np.newaxis, :, np.newaxis]\n",
    "\n",
    "# look at how much narrower they are! this should be visible in all types, but the color scaling is bad for Ibc & II\n",
    "fig = plt.figure(figsize=(n_types*len(colors), n_sne*len(colors)))\n",
    "p = 0\n",
    "for s in range(n_sne)[:len(colors)]:\n",
    "    for t in range(n_types):\n",
    "        p += 1\n",
    "        plt.subplot(n_sne, n_types, p)\n",
    "        plt.pcolormesh(z_mids, mu_mids, ln_likelihoods[s][t].T, cmap='viridis')#, vmin = 0., vmax = 3.)\n",
    "        plt.colorbar()\n",
    "        plt.scatter(true_params[s]['z'], true_params[s]['mu'], color='k')\n",
    "        plt.title('true '+true_params[s]['t']+', class '+types[t])\n",
    "        plt.xlabel(r'$z$')\n",
    "        plt.ylabel(r'$\\mu$')\n",
    "        plt.axis([z_bins[0], z_bins[-1], mu_bins[0], mu_bins[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making interim posteriors\n",
    "\n",
    "In reality, when we perform classification, lightcurve fitting, and photo-$z$ PDF estimation, we are not determining likelihoods $p(\\underline{\\ell}_{n}, \\vec{m}_{n} | t_{n}, z_{n}, \\mu_{n})$ but instead are finding interim posteriors, due to the assumptions about the distributions of our latent variables.  In our case, choices of interim hyperpriors $\\underline{\\phi}^{*}$ and $\\vec{\\theta}^{*}$ will translate directly into a prior belief about the `3D` distribution $p(t_{n}, z_{n}, \\mu_{n} | \\underline{\\phi}^{*}, \\vec{\\theta}^{*})$ that is independent of any observations (and thus independent of $n$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing interim priors\n",
    "\n",
    "To transform our likelihoods into interim posteriors that accurately represent what we expect a real data analysis pipeline to produce, we must choose interim values of the hyperparameters.  We will set a uniform prior on the redshift-dependent supernova proportions and use the WMAP values of the cosmological parameters with grossly inflated error bars to restrict the parameter space of $z$ and $\\mu$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# flat prior on redshift-dependent SNe proportions\n",
    "interim_n_of_z = np.ones((n_types, n_zs))\n",
    "interim_n_of_z /= np.sum(interim_n_of_z * z_difs[np.newaxis, :])\n",
    "assert np.isclose(np.sum(interim_n_of_z * z_difs[np.newaxis, :]), 1.)\n",
    "interim_ln_n_of_z = safe_log(interim_n_of_z)\n",
    "\n",
    "# WMAP, which 10 * errors so we can see what's going on in crappy plots\n",
    "interim_H0 = 70.0\n",
    "delta_H0 = 2.2 * 10.\n",
    "interim_Om0 = 1. - 0.721\n",
    "delta_Om0 = 0.025 * 10.\n",
    "interim_hyperparams = np.array([interim_H0, interim_Om0])\n",
    "interim_hyperparam_vars = np.array([delta_H0, delta_Om0]) * np.eye(n_hyperparams)\n",
    "interim_dist = sps.multivariate_normal(mean = interim_hyperparams, cov = interim_hyperparam_vars)\n",
    "interim_cosmo = cosmology.FlatLambdaCDM(H0=interim_hyperparams[0], Om0=interim_hyperparams[1])\n",
    "\n",
    "def inverter(z, mu):\n",
    "    def cosmo_helper(hyperparams):\n",
    "        return np.array([abs(cosmology.FlatLambdaCDM(H0=hyperparams[0], Om0=hyperparams[1]).distmod(z).value - mu)])\n",
    "    solved_cosmo = spo.minimize(cosmo_helper, interim_hyperparams, method=\"Nelder-Mead\", options={\"maxfev\": 1e5, \"maxiter\":1e5})\n",
    "    ln_prob = interim_dist.logpdf(solved_cosmo.x)\n",
    "    return ln_prob#max(prob, sys.float_info.epsilon)\n",
    "\n",
    "interim_sheet = np.zeros((n_zs, n_mus))\n",
    "for z in range(n_zs):\n",
    "    for mu in range(n_mus):\n",
    "        ln_prob = inverter(z_mids[z], mu_mids[mu])\n",
    "        interim_sheet[z][mu] = ln_prob\n",
    "interim_ln_prior = interim_ln_n_of_z[:, np.newaxis] + interim_sheet[np.newaxis, :]\n",
    "interim_prior = np.exp(interim_ln_prior)\n",
    "interim_prior /= np.sum(interim_prior * z_difs[np.newaxis, :, np.newaxis] * mu_difs[np.newaxis, np.newaxis, :])\n",
    "interim_ln_prior = safe_log(interim_prior)\n",
    "assert np.isclose(np.sum(interim_prior * z_difs[np.newaxis, :, np.newaxis] * mu_difs[np.newaxis, np.newaxis, :]), 1.)\n",
    "\n",
    "# the interim prior and truth are way too close to each other. . . but that's realistic\n",
    "plt.pcolormesh(z_mids, mu_mids, interim_ln_prior[0].T, cmap='viridis')#, vmin = 0., vmax = 3.)\n",
    "plt.plot(z_mids, [true_cosmo.distmod(z).value for z in z_mids], color='k')\n",
    "plt.plot(z_mids, [interim_cosmo.distmod(z).value for z in z_mids], color='r')\n",
    "plt.title('interim prior distribution')\n",
    "plt.xlabel(r'$z$')\n",
    "plt.ylabel(r'$\\mu$')\n",
    "plt.axis([z_bins[0], z_bins[-1], mu_bins[0], mu_bins[-1]])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping it up\n",
    "\n",
    "[multiply likelihoods and interim prior]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# note likelihoods were not normalized; this must be normalized\n",
    "interim_ln_posteriors = ln_likelihoods + interim_ln_prior[np.newaxis, :]\n",
    "interim_posteriors = np.exp(interim_ln_posteriors)\n",
    "interim_posteriors /= np.sum(interim_posteriors * z_difs[np.newaxis, :, np.newaxis] * mu_difs[np.newaxis, np.newaxis, :])\n",
    "interim_ln_posteriors = safe_log(interim_posteriors)\n",
    "assert np.isclose(np.sum(interim_posteriors * z_difs[np.newaxis, :, np.newaxis] * mu_difs[np.newaxis, np.newaxis, :]), 1.)\n",
    "\n",
    "# these are going to get a lot narrower\n",
    "fig = plt.figure(figsize=(n_types*len(colors), n_sne*len(colors)))\n",
    "p = 0\n",
    "for s in range(n_sne)[:len(colors)]:\n",
    "    for t in range(n_types):\n",
    "        p += 1\n",
    "        plt.subplot(n_sne, n_types, p)\n",
    "        plt.pcolormesh(z_mids, mu_mids, interim_ln_posteriors[s][t].T, cmap='viridis')#, vmin = 0., vmax = 3.)\n",
    "        plt.colorbar()\n",
    "        plt.scatter(true_params[s]['z'], true_params[s]['mu'], color='k')\n",
    "        plt.title('true '+true_params[s]['t']+', class '+types[t])\n",
    "        plt.xlabel(r'$z$')\n",
    "        plt.ylabel(r'$\\mu$')\n",
    "        plt.axis([z_bins[0], z_bins[-1], mu_bins[0], mu_bins[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[write data to file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write true hyperparameters just to check\n",
    "\n",
    "# write axes\n",
    "# write interim prior\n",
    "# write interim posteriors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
