{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Mock Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scippr` requires inputs in the form of catalogs $\\{\\ln[p(t_{n}, z_{n}, \\mu_{n} | \\underline{\\ell}_{n}, \\vec{f}_{n}, \\underline{\\phi}^{*}, \\vec{\\alpha}, \\vec{\\beta})]\\}_{N}$ of interim log-posteriors expressed as `3D` arrays constituting probabilities over $t_{n}$, $z_{n}$, and $\\mu_{n}$, enabling rapid computation of the log-posterior $\\ln[p(\\underline{\\phi}, \\vec{\\Omega} | \\{\\underline{\\ell}_{n}, \\vec{f}_{n}\\}_{N}, \\vec{\\alpha}, \\vec{\\beta})$ over the hyperparameters $\\underline{\\phi}$ and $\\vec{\\Omega}$ of scientific interest.  This notebook outlines a procedure for generating such a catalog.\n",
    "\n",
    "Perhaps the defining feature of this pipeline is that it does not involve simulating supernova lightcurves or host galaxy photometry and instead simulating the interim posteriors directly.  There are several good reasons for this choice:\n",
    "\n",
    "* The motivation for `scippr` is to develop a method for doing inference with accurate probability distributions over relevant supernova parameters, not to develop methods for obtaining those probability distributions.\n",
    "* We avoid tying our inference method to a particular way of deriving interim posteriors from observed data.\n",
    "* We avoid making assumptions about the details of the observed data, such as the photometric filters, intrinsic lightcurves, and observing conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import daft\n",
    "import astropy.cosmology as cosmology\n",
    "\n",
    "import numpy as np\n",
    "import bisect\n",
    "import scipy.stats as sps\n",
    "import scipy.interpolate as spi\n",
    "import scipy.optimize as spo\n",
    "import hickle\n",
    "import sys\n",
    "log_epsilon = sys.float_info.min_exp\n",
    "epsilon = sys.float_info.min\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib import rc\n",
    "rc(\"font\", family=\"serif\", size=12)\n",
    "rc(\"text\", usetex=True)\n",
    "\n",
    "colors = 'rbgcymk'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scippr` is based on a probabilistic graphical model, illustrated below.  The model has two types of observables, shown in shaded circles, supernova lightcurves $\\underline{\\ell}_{n}$ and host galaxy photometry $\\vec{f}_{n}$.  The parameters, which are by definition not directly observable, are shown in empty circles.  The latent variables of supernova type $t_{n}$, redshift $z_{n}$, and distance modulus $\\mu_{n}$ are parameters over which we will marginalize, without ever directly inferring them, and while all three of them influence $\\underline{\\ell}_{n}$, only $z_{n}$ affects $\\vec{f}_{n}$ in this model.  In other words, _we currently assume no relationship between supernova type and host galaxy photometry, an assumption we may revisit in the future_.  The selection function parameters $\\vec{\\alpha}$ and $\\vec{\\beta}$ are known constants of the survey symbolized by dots that influence the possible lightcurves and host galaxy photometry that are included in the sample.  The box indicates that the latent variables and the observables are generated independently $N$ times for each supernova in the sample.  The hyperparameters we would like to estimate are the redshift-dependent supernova type proportions $\\underline{\\phi}$ that determine $t_{n}$ and $z_{n}$ and the cosmological parameters $\\vec{\\Omega}$ that relate $z_{n}$ to $\\mu_{n}$, which are shared by all $N$ supernovae in the observed sample.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#initialize the PGM\n",
    "pgm = daft.PGM([6, 4.5], origin=[0, 0])\n",
    "\n",
    "#desired hyperparameters\n",
    "pgm.add_node(daft.Node(\"cosmology\", r\"$\\vec{\\Omega}$\", 2., 4.))\n",
    "pgm.add_node(daft.Node(\"dist\", r\"$\\underline{\\phi}$\", 3.5, 4.))\n",
    "#pgm.add_node(daft.Node(\"rates\", r\"$\\vec{R}$\", 3., 5.5, fixed=True))\n",
    "\n",
    "#latent variables/parameters\n",
    "pgm.add_node(daft.Node(\"distance\", r\"$\\mu_{n}$\", 2., 2.5))\n",
    "pgm.add_node(daft.Node(\"redshift\", r\"$z_{n}$\", 3., 3.))\n",
    "pgm.add_node(daft.Node(\"type\", r\"$t_{n}$\", 4., 2.5))\n",
    "\n",
    "#data\n",
    "pgm.add_node(daft.Node(\"lightcurve\", r\"$\\underline{\\ell}_{n}$\", 2.5, 1., observed=True))\n",
    "pgm.add_node(daft.Node(\"photometry\", r\"$\\vec{f}_{n}$\", 3.5, 1., observed=True))\n",
    "\n",
    "#known constant parameters\n",
    "pgm.add_node(daft.Node(\"lightcurve selection\", r\"$\\vec{\\alpha}$\", 1., 1.75, fixed=True))\n",
    "pgm.add_node(daft.Node(\"photometry selection\", r\"$\\vec{\\beta}$\", 5., 1.75, fixed=True))\n",
    "\n",
    "# Add in the edges.\n",
    "pgm.add_edge(\"dist\", \"type\")\n",
    "pgm.add_edge(\"cosmology\", \"distance\")\n",
    "pgm.add_edge(\"dist\", \"redshift\")\n",
    "pgm.add_edge(\"redshift\", \"distance\")\n",
    "#pgm.add_edge(\"distance\", \"photometry\")\n",
    "pgm.add_edge(\"distance\", \"lightcurve\")\n",
    "pgm.add_edge(\"redshift\", \"photometry\")\n",
    "pgm.add_edge(\"redshift\", \"lightcurve\")\n",
    "pgm.add_edge(\"type\", \"lightcurve\")\n",
    "pgm.add_edge(\"photometry selection\", \"photometry\")\n",
    "pgm.add_edge(\"lightcurve selection\", \"lightcurve\")\n",
    "\n",
    "# plates\n",
    "pgm.add_plate(daft.Plate([1.5, 0.5, 3., 3.], label=r\"$n = 1, \\cdots, N$\"))\n",
    "\n",
    "# Render and save.\n",
    "pgm.render()\n",
    "pgm.figure.show()\n",
    "pgm.figure.savefig('plots/sim_pgm.png', dpi=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simulate a mock catalog for `scippr`, there are three main steps.\n",
    "\n",
    "1. Choose true values for the hyperparameters, which we would like to recover from our inference, and the parameters, over which we intend to marginalize.\n",
    "2. Create likelihoods based on a model for how they are derived from observations under the selection functions.\n",
    "3. Make interim posteriors by assuming interim priors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing true hyperparameters and parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The redshift-dependent supernova type proportions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that the true redshift-dependent type proportions are provided as parameters $\\underline{\\phi}'$ of continuous functions for a finite number of types across a restricted redshift range.  \n",
    "\n",
    "*[The following cell is a placeholder for a realistic $\\underline{\\phi}'$.  It will soon be replaced with reading in a file containing this information.]*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "types = ['Ia', 'Ibc', 'II']\n",
    "n_types = len(types)\n",
    "# making up the type fractions, will replace this with data soon!\n",
    "frac_types = np.array([0.4, 0.1, 0.5])\n",
    "assert np.isclose(np.sum(frac_types), 1.)\n",
    "\n",
    "min_z = 0.15\n",
    "max_z = 0.65\n",
    "\n",
    "n_of_z_consts = {}\n",
    "n_of_z_consts['Ia'] = (1.5, 0.5)\n",
    "n_of_z_consts['Ibc'] = (1., 0.5)\n",
    "n_of_z_consts['II'] = (0.5, 0.5)\n",
    "\n",
    "true_n_of_z = []\n",
    "for t in types:\n",
    "    (mean, std) = n_of_z_consts[t]\n",
    "    low, high = (min_z - mean) / std, (max_z - mean) / std\n",
    "    true_n_of_z.append(sps.truncnorm(low, high, loc = mean, scale = std))\n",
    "\n",
    "plot_res = 20\n",
    "z_range = max_z - min_z\n",
    "z_grid = np.linspace(min_z, max_z, num=plot_res + 1, endpoint=True)\n",
    "z_plot = (z_grid[1:] + z_grid[:-1]) / 2.\n",
    "z_dif_plot = z_grid[1:] - z_grid[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we plot the true redshift-dependent type proportions after ensuring they are properly normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_true_n_of_z = np.zeros((n_types, plot_res))\n",
    "for t in range(n_types):\n",
    "    plot_true_n_of_z[t] = true_n_of_z[t].pdf(z_plot)\n",
    "plot_true_n_of_z = frac_types[:, np.newaxis] * np.array(plot_true_n_of_z)# / z_range\n",
    "plot_true_n_of_z /= np.sum(plot_true_n_of_z * z_dif_plot)\n",
    "assert np.isclose(np.sum(plot_true_n_of_z * z_dif_plot), 1.)\n",
    "\n",
    "for t in range(n_types):\n",
    "    plt.plot(z_plot, plot_true_n_of_z[t], color=colors[t], label=types[t])\n",
    "plt.xlabel(r'$z$')\n",
    "plt.ylabel(r'relative rate')\n",
    "plt.legend()\n",
    "plt.savefig('plots/true_rates.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sample pairs of type and redshift from these distributions by choosing a type based on the overall type proportions, then sampling its underlying redshift distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_discrete(fracs, n_of_z, N):\n",
    "    out_info = []\n",
    "    cdf = np.cumsum(fracs)\n",
    "    for n in range(N):\n",
    "        each = {}\n",
    "        r = np.random.random()\n",
    "        k = bisect.bisect(cdf, r)\n",
    "        each['t'] = types[k]\n",
    "        each['z'] = n_of_z[k].rvs()\n",
    "        out_info.append(each)\n",
    "    return out_info\n",
    "\n",
    "n_sne = 1000\n",
    "\n",
    "true_params = sample_discrete(frac_types, true_n_of_z, n_sne)\n",
    "\n",
    "true_zs = [true_param['z'] for true_param in true_params]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we plot a histogram of the true redshift values for the three types of supernovae, along with the redshift-dependent type functions from which they were drawn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "to_plot = [[d['z'] for d in true_params if d['t'] == types[t]] for t in range(n_types)]\n",
    "hist_bins = np.linspace(min_z, max_z, plot_res + 1)\n",
    "bin_difs = hist_bins[1:] - hist_bins[:-1]\n",
    "for t in range(n_types):\n",
    "    plt.plot(z_plot, plot_true_n_of_z[t] * n_sne * bin_difs, color=colors[t], label='true '+types[t])\n",
    "    plt.hist(to_plot[t], bins=hist_bins, color=colors[t], alpha=1./3., label='sampled '+types[t], normed=False)\n",
    "plt.xlabel(r'$z$')\n",
    "plt.ylabel(r'relative rate')\n",
    "plt.legend(fontsize='xx-small')\n",
    "plt.savefig('plots/obs_rates.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The true cosmological parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the true hyperparameter vector $\\vec{\\Omega}'$ as having two components, $H_{0}'$ and $\\Omega_{m,0}'$.  In a future revision, we may include additional cosmological parameters in this hyperparameter vector.  We choose the true values for the cosmological parameters to be those published by /Planck/.  [include citation]  \n",
    "\n",
    "Since every supernova in our sample already has a true redshift $z_{n}'$, we can easily establish a true distance modulus $\\mu_{n}'$ via the luminosity distance equation.  [insert equation here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Planck 2015 results XIV. Dark energy and modified gravity - Figure 3\n",
    "true_H0 = 67.9\n",
    "true_Ode0 = 0.693\n",
    "true_Om0 = 1. - true_Ode0\n",
    "true_w0 = -0.09\n",
    "true_wa = -0.20\n",
    "true_hyperparams = np.array([true_w0, true_wa])\n",
    "n_hyperparams = len(true_hyperparams)\n",
    "#true_cosmo = cosmology.FlatLambdaCDM(H0=true_H0, Om0=true_Om0)\n",
    "true_cosmo = cosmology.w0waCDM(true_H0, true_Om0, true_Ode0, w0=true_w0, wa=true_wa)\n",
    "\n",
    "for n in range(n_sne):\n",
    "    true_params[n]['mu'] = true_cosmo.distmod(true_params[n]['z']).value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot a traditional Hubble diagram of the supernovae in our sample.\n",
    "\n",
    "*We must note that this is not a Hubble diagram like any other ever observed!  The distance moduli are of course not accessible for supernovae of types other than Ia.  However, non-Ia supernovae do still have a distance modulus, and do still follow the cosmological Hubble flow, so in simulated data, this is a perfectly reasonable quantity to visualize.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "to_plot_x = [[d['z'] for d in true_params if d['t'] == types[t]] for t in range(n_types)]\n",
    "to_plot_y = [[d['mu'] for d in true_params if d['t'] == types[t]] for t in range(n_types)]\n",
    "for t in range(n_types):\n",
    "    plt.scatter(to_plot_x[t], to_plot_y[t], color=colors[t], marker='.', label=types[t], alpha=0.1)\n",
    "plt.plot(z_plot, [true_cosmo.distmod(z).value for z in z_plot], color='k', alpha=0.2)\n",
    "plt.xlabel(r'$z$')\n",
    "plt.ylabel(r'$\\mu$')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title(r'$H_{0}='+str(true_H0)+r', \\Omega_{m,0}='+str(true_Om0)+r'$')\n",
    "plt.savefig('plots/true_hubble.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating Probabilities\n",
    "\n",
    "`scippr` is intended to be used on interim posterior probabilities derived from a probabilistic lightcurve fitting procedure.  These will be provided as log-probabilities evaluated on a `3D` grid in type, redshift, and distance modulus space.  We choose to work with arrays of log-probabilities because they preserve numerical precision and enable slower products of arrays to be transformed into fast sums of arrays.  *This choice of parametrization for the input probabilities will be hard to change later on!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we will be working in log-space, it is important to define functions that ensure that the elements of the log-probability arrays do not result in `NaN` values and do not throw errors in the functions used to take logs.  We do this by defining a very small positive number, many orders of magnitude less than the limit of floating point precision, as the minimum probability allowed in our universe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def safe_log(arr, threshold=epsilon):\n",
    "#     shape = np.shape(arr)\n",
    "#     flat = arr.flatten()\n",
    "#     logged = np.log(np.array([max(a, threshold) for a in flat])).reshape(shape)\n",
    "    arr[arr < threshold] = threshold\n",
    "    return np.log(arr)\n",
    "\n",
    "def reg_vals(arr, threshold=log_epsilon):\n",
    "    arr[arr < threshold] = threshold\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We establish a binning in the space of $z$ and $\\mu$ for the arrays of log-probabilities.  This is arbitrary and can easily be changed.  [The cell below will be replaced by reading in physically motivated bin limits from a file.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this binning is arbitrary!\n",
    "n_zs = 6\n",
    "z_bins = np.linspace(min_z, max_z, num=n_zs, endpoint=True)\n",
    "z_difs = z_bins[1:] - z_bins[:-1]\n",
    "z_dif = np.mean(z_difs)\n",
    "z_mids = (z_bins[1:] + z_bins[:-1]) / 2.\n",
    "\n",
    "# want this to be agnostic about true cosmology\n",
    "n_mus = 6\n",
    "min_mu, max_mu = min([s['mu'] for s in true_params]) - 0.5, max([s['mu'] for s in true_params]) + 0.5\n",
    "mu_bins = np.linspace(min_mu, max_mu, num=n_mus, endpoint=True)\n",
    "mu_difs = mu_bins[1:] - mu_bins[:-1]\n",
    "mu_dif = np.mean(mu_difs)\n",
    "mu_range = np.max(mu_bins) - np.min(mu_bins)\n",
    "mu_mids = (mu_bins[1:] + mu_bins[:-1]) / 2.\n",
    "\n",
    "z_mu_grid = np.array([[(z, mu) for mu in mu_mids] for z in z_mids])\n",
    "cake_shape = np.shape(z_mu_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating likelihoods\n",
    "\n",
    "We now simulate likelihoods for the type, redshift, and distance modulus of our sample of supernovae.  This is something that will be done internally by a probabilistic lightcurve fitting procedure.\n",
    "\n",
    "The goal here is to simulate realistic outputs from such a procedure without having to develop and run one.   Each log-likelihood $\\ln[p(\\underline{\\ell}_{n}, \\vec{f}_{n} | t_{n}, z_{n}, \\mu_{n}, \\vec{\\alpha}, \\vec{\\beta})]$ may be broken down into simpler components that may be summed to create log-likelihoods.\n",
    "\n",
    "\\begin{align*}\n",
    "\\ln[p(\\underline{\\ell}_{n}, \\vec{f}_{n} | t_{n}, z_{n}, \\mu_{n}, \\vec{\\alpha}, \\vec{\\beta})] &= \\ln[p(\\underline{\\ell}_{n} | t_{n}, z_{n}, \\mu_{n}, \\vec{\\alpha})] + \\ln[p(\\vec{f}_{n} | z_{n}, \\vec{\\beta})]\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling an observed photo-$z$ PDF\n",
    "\n",
    "It is simplest to start with the log-likelihood $\\ln[p(\\vec{f}_{n} | z_{n})]$ of host galaxy photometry $\\vec{f}_{n}$ as a function of redshift $z_{n}$.  We assume the simplest model in which photo-$z$ PDFs are Gaussians $\\mathcal{N}(z_{n}'', \\sigma_{n}^{2})$ where $z_{n}''\\sim\\mathcal{N}(z_{n}', \\sigma_{n}^{2})$.  We will also state that the variance is a constant $\\sigma_{n}\\equiv\\sigma$ for all $n$.  [cite where 0.03 came from]\n",
    "\n",
    "*The choice of a Gaussian photo-$z$ PDF model is irrelevant -- any continuous function or linear combination thereof, as well as any discrete distribution, can be converted to the binned parametrization used here.  The cell below could easily be replaced with one that reads in more realistically modeled photo-$z$ likelihoods from a file.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# very simple p(z) model, simple gaussians\n",
    "pz_sigma = 0.03\n",
    "\n",
    "pzs, ln_pzs = [], []\n",
    "for s in range(n_sne):\n",
    "    dist = sps.norm(loc = true_params[s]['z'], scale = 0.03)\n",
    "    pz_mean = dist.rvs()\n",
    "    new_dist = sps.norm(loc = pz_mean, scale = pz_sigma)\n",
    "    pz = new_dist.pdf(z_mids)\n",
    "    #ln_pz = new_dist.logpdf(z_mids)\n",
    "    pzs.append(pz)\n",
    "    #ln_pzs.append(ln_pz)\n",
    "pzs = np.array(pzs)\n",
    "ln_pzs = safe_log(pzs)#np.array(ln_pzs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot a few sample photo-$z$ likelihoods, which we note need not be normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for s in range(n_sne)[:len(colors)]:\n",
    "    plt.plot(z_mids, pzs[s], color=colors[s])\n",
    "    plt.vlines(true_params[s]['z'], 0., 15., color=colors[s])\n",
    "plt.xlabel(r'$z$')\n",
    "plt.ylabel(r'$p(z)$')\n",
    "plt.title(r'host galaxy $p(z)$ distributions')\n",
    "plt.savefig('plots/host_likelihoods.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling fitting an observed lightcurve\n",
    "\n",
    "Based on how existing lightcurve fitters work, a lightcurve is generally assigned a class before its redshift and distance modulus are estimated because the fitting function will differ based on the assigned class.  Thus, we may assume that the lightcurve likelihood is separable as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "\\ln[p(\\underline{\\ell}_{n} | t_{n}, z_{n}, \\mu_{n})] &= \\ln[p(\\underline{\\ell}_{n} | t_{n})] + \\ln[p(\\underline{\\ell}_{n}, t_{n} | z_{n}, \\mu_{n})]\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Lightcurve classification*\n",
    "\n",
    "The confusion matrix quantifies the probabilities that an item is truly of a certain class given the fact that it has been classified as a different class.  (A more in-depth description of the confusion matrix can be found [here](https://github.com/rbiswas4/SNeLightcurveQualityMetric/blob/master/classification_metric.tex).)  We will use the $p(t_{n}' | \\hat{t}_{n})$ elements of the confusion matrix as a proxy for $p(\\underline{\\ell}_{n} | t_{n})$.  The confusion matrix is specific to each classification method, so we will have to choose one to simulate a realistic mock dataset.  For now, we proceed assuming a fairly trivial confusion matrix giving a 50% chance of correct classification for each type and equal probabilities for all misclassifications.  Obviously this will be revised in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# will need to take this from data of some kind, arbitrary for now\n",
    "conf_matrix = 0.25 + 0.25 * np.eye(3)\n",
    "assert np.isclose(np.sum(conf_matrix, axis=1).all(), frac_types.all())\n",
    "ln_conf_matrix = safe_log(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*$\\chi^{2}$ lightcurve parameter fitting*\n",
    "\n",
    "In order to produce $p(\\underline{\\ell}_{n}, t_{n} | z_{n}, \\mu_{n})$, we will again introduce the idea of using the true type $t_{n}'$ as a proxy for the lightcurve $\\underline{\\ell}_{n}$ and a classified type $\\hat{t}_{n}$ for the variable type $t_{n}$ that appears in the probability expressions.  If we do this, the quantity we want is really $p(t_{n}', \\hat{t}_{n} | z_{n}, \\mu_{n})$.  We can obtain this knowing how lightcurve fitters, in general, estimate redshifts $z_{n}$ and distance moduli $\\mu_{n}$ under all possible combinations of $t_{n}'$ and $\\hat{t}_{n}$.  We will construct functions that aim to simulate the signatures of misclassification in the Hubble diagram.  [find a figure to link to]  Currently, we use placeholder functions that will be replaced later on as information about the consequences of fitting with the wrong function becomes available.  The table below summarizes the fitting function for each true type given a classification of type $Ia$.  All other classified types are assumed to give a distribution that is uniform in $\\mu_{n}$ and Gaussian in $z_{n}$ according to the same prescription used for the photo-$z$ PDFs.\n",
    "\n",
    "| True Type | Functional Form of Ia likelihood |\n",
    "| :-------: | :------------------------------: |\n",
    "| Ia | $\\vec{\\mathcal{N}}((z_{n}'', \\mu_{n}''), \\underline{\\Sigma}_{n})$ where $\\underline{\\Sigma}_{n}=(\\sigma^{2}_{z, Ia}, \\sigma^{2}_{\\mu, Ia})\\times\\underline{I}$ and $(z_{n}'', \\mu_{n}'')\\sim\\vec{\\mathcal{N}}((z_{n}', \\mu_{n}'), \\underline{\\Sigma}_{n})$ |\n",
    "| Ibc | $\\vec{\\mathcal{N}}((z_{n}'', \\mu_{n}''), \\underline{\\Sigma}_{n})$ where $\\underline{\\Sigma}_{n}=(\\sigma^{2}_{z, Ibc}, \\sigma^{2}_{\\mu, Ibc})\\times\\underline{I}$ and $(z_{n}'', \\mu_{n}'')\\sim\\vec{\\mathcal{N}}((z_{n}', \\mu_{n}' - C_{Ibc}), \\underline{\\Sigma}_{n})$ for survey-wide constant $C_{Ibc}$ |\n",
    "| II | $\\vec{\\mathcal{N}}((z_{n}'', \\mu_{n}''), \\underline{\\Sigma}_{n})$ where $\\underline{\\Sigma}_{n}=(\\sigma^{2}_{z, II}, \\sigma^{2}_{\\mu, II})\\times\\underline{I}$ and $(z_{n}'', \\mu_{n}'')\\sim\\vec{\\mathcal{N}}((z_{n}', C_{II}), \\underline{\\Sigma}_{n})$ for survey-wide constant $C_{II}$ |\n",
    "\n",
    "In a future revision, the values of $\\sigma^{2}_{z, \\tau}$ and $\\sigma^{2}_{\\mu, \\tau}$ for each type $\\tau$ will be replaced by random variables themselves representing the intrinsic variation among lightcurves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# must set nuisance parameters inherent in process of producing interim posteriors from lightcurves\n",
    "Ia_Ia_var = np.array([0.01, 0.02]) ** 2\n",
    "Ibc_Ia_delta = 1.\n",
    "Ibc_Ia_var = np.array([0.01, 0.01]) ** 2\n",
    "II_Ia_delta = np.mean(mu_mids)\n",
    "II_Ia_var = np.array([0.01, 0.05]) ** 2\n",
    "z_sigma = 0.03\n",
    "\n",
    "# definitely needs more work on what (z, mu) distributions are expected when lightcurves are fit with wrong templates\n",
    "# just made it flat for now\n",
    "\n",
    "def fit_Ia(z, mu):\n",
    "    cake = np.zeros((n_types, n_zs-1, n_mus-1))\n",
    "    cake_Ia = sps.multivariate_normal(mean = np.array([z, mu]), cov = Ia_Ia_var * np.eye(2))\n",
    "    [z_samp, mu_samp] = cake_Ia.rvs()\n",
    "    cake_Ia = sps.multivariate_normal(mean = np.array([z_samp, mu_samp]), cov = Ia_Ia_var * np.eye(2))\n",
    "    cake[0] = cake_Ia.pdf(z_mu_grid.reshape(-1, cake_shape[-1])).reshape(cake_shape[:-1])\n",
    "    dist = sps.norm(loc = z, scale = z_sigma)\n",
    "    z_means = dist.rvs(2)\n",
    "    cake[1] = sps.norm(loc = z_means[0], scale = z_sigma).pdf(z_mids)[:, np.newaxis] / n_mus#np.ones(cake_shape[:-1]) / np.prod(cake_shape[:-1])\n",
    "    cake[2] = sps.norm(loc = z_means[1], scale = z_sigma).pdf(z_mids)[:, np.newaxis] / n_mus#np.ones(cake_shape[:-1]) / np.prod(cake_shape[:-1])\n",
    "    ln_cake = safe_log(cake)\n",
    "    return ln_cake\n",
    "    \n",
    "def fit_Ibc(z, mu):\n",
    "    cake = np.zeros((n_types, n_zs-1, n_mus-1))\n",
    "    cake_Ia = sps.multivariate_normal(mean = np.array([z, mu - Ibc_Ia_delta]), cov = Ibc_Ia_var * np.eye(2))\n",
    "    [z_samp, mu_samp] = cake_Ia.rvs()\n",
    "    cake_Ia = sps.multivariate_normal(mean = np.array([z_samp, mu_samp]), cov = Ibc_Ia_var * np.eye(2))\n",
    "    cake[0] = cake_Ia.pdf(z_mu_grid.reshape(-1, cake_shape[-1])).reshape(cake_shape[:-1])\n",
    "    dist = sps.norm(loc = z, scale = z_sigma)\n",
    "    z_means = dist.rvs(2)\n",
    "    cake[1] = sps.norm(loc = z_means[0], scale = z_sigma).pdf(z_mids)[:, np.newaxis] / n_mus#np.ones(cake_shape[:-1]) / np.prod(cake_shape[:-1])\n",
    "    cake[2] = sps.norm(loc = z_means[1], scale = z_sigma).pdf(z_mids)[:, np.newaxis] / n_mus#np.ones(cake_shape[:-1]) / np.prod(cake_shape[:-1])\n",
    "    ln_cake = safe_log(cake)\n",
    "    return ln_cake\n",
    "    \n",
    "def fit_II(z, mu):\n",
    "    cake = np.zeros((n_types, n_zs-1, n_mus-1))\n",
    "    cake_Ia = sps.multivariate_normal(mean = np.array([z, II_Ia_delta]), cov = II_Ia_var * np.eye(2))\n",
    "    [z_samp, mu_samp] = cake_Ia.rvs()\n",
    "    cake_Ia = sps.multivariate_normal(mean = np.array([z_samp, mu_samp]), cov = II_Ia_var * np.eye(2))\n",
    "    cake[0] = cake_Ia.pdf(z_mu_grid.reshape(-1, cake_shape[-1])).reshape(cake_shape[:-1])\n",
    "    dist = sps.norm(loc = z, scale = z_sigma)\n",
    "    z_means = dist.rvs(2)\n",
    "    cake[1] = sps.norm(loc = z_means[0], scale = z_sigma).pdf(z_mids)[:, np.newaxis] / n_mus#np.ones(cake_shape[:-1]) / np.prod(cake_shape[:-1])\n",
    "    cake[2] = sps.norm(loc = z_means[1], scale = z_sigma).pdf(z_mids)[:, np.newaxis] / n_mus#np.ones(cake_shape[:-1]) / np.prod(cake_shape[:-1])\n",
    "    ln_cake = safe_log(cake)\n",
    "    return ln_cake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The lightcurve likelihood*\n",
    "\n",
    "Now that the type-specific fitting functions have been established, we may combine all relevant terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_any(true_vals):\n",
    "    if true_vals['t'] == 'Ia':\n",
    "        ln_cake = fit_Ia(true_vals['z'], true_vals['mu']) + ln_conf_matrix[:, 0, np.newaxis, np.newaxis]\n",
    "    if true_vals['t'] == 'Ibc':\n",
    "        ln_cake = fit_Ibc(true_vals['z'], true_vals['mu']) + ln_conf_matrix[:, 1, np.newaxis, np.newaxis]\n",
    "    if true_vals['t'] == 'II':\n",
    "        ln_cake = fit_II(true_vals['z'], true_vals['mu']) + ln_conf_matrix[:, 2, np.newaxis, np.newaxis]\n",
    "    return ln_cake\n",
    "\n",
    "def fit_all(catalog):\n",
    "    dessert = []\n",
    "    for true_vals in catalog:\n",
    "        dessert.append(fit_any(true_vals))\n",
    "    return np.array(dessert)\n",
    "\n",
    "sheet_cake = fit_all(true_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot a handful of these likelihoods.  Each row is the log-likelihood for a different supernova in the catalog.  The columns represent the log-likelihood in the space of $z$ and $\\mu$ (Hubble diagram-space) for that supernova if it were classified as type $Ia$, type $Ibc$ and type $II$.  The true $z$ and $\\mu$ for each are also plotted.  The title of each panel gives the true type for each supernova.  We can see the effect on the likelihoods for $z$ and $\\mu$ based on each combination of true and classified type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# happily, these look like what we see in contaminated hubble diagrams!\n",
    "fig = plt.figure(figsize=(n_types*5, len(colors)*5))\n",
    "p = 0\n",
    "for s in range(n_sne)[:len(colors)]:\n",
    "    for t in range(n_types):\n",
    "        p += 1\n",
    "        plt.subplot(len(colors), n_types, p)\n",
    "        plt.pcolormesh(z_bins, mu_bins, sheet_cake[s][t].T, cmap='viridis')\n",
    "        plt.colorbar()\n",
    "        plt.scatter(true_params[s]['z'], true_params[s]['mu'], color='r')\n",
    "        plt.title('true '+true_params[s]['t']+', class '+types[t])\n",
    "        plt.xlabel(r'$z$')\n",
    "        plt.ylabel(r'$\\mu$')\n",
    "        plt.axis([z_bins[0], z_bins[-1], mu_bins[0], mu_bins[-1]])\n",
    "plt.savefig('plots/lc_likelihoods.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The full likelihood\n",
    "\n",
    "[combine both the host galaxy photometry and supernova lightcurve likelihood components]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# these don't have to be normalized\n",
    "ln_likelihoods = reg_vals(sheet_cake + ln_pzs[:, np.newaxis, :, np.newaxis])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the combined likelihoods for when lightcurves and host galaxy photometry are available.  For the same supernovae as in the previous plot, you can see how the constraints in $z$ become much narrower when th redshift likelihoods are included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this should be visible in all types, but the color scaling is bad for Ibc & II\n",
    "fig = plt.figure(figsize=(n_types*5, len(colors)*5))\n",
    "p = 0\n",
    "for s in range(n_sne)[:len(colors)]:\n",
    "    for t in range(n_types):\n",
    "        p += 1\n",
    "        plt.subplot(len(colors), n_types, p)\n",
    "        plt.pcolormesh(z_bins, mu_bins, ln_likelihoods[s][t].T, cmap='viridis')#, vmin = 0., vmax = 3.)\n",
    "        plt.colorbar()\n",
    "        plt.scatter(true_params[s]['z'], true_params[s]['mu'], color='r')\n",
    "        plt.title('true '+true_params[s]['t']+', class '+types[t])\n",
    "        plt.xlabel(r'$z$')\n",
    "        plt.ylabel(r'$\\mu$')\n",
    "        plt.axis([z_bins[0], z_bins[-1], mu_bins[0], mu_bins[-1]])\n",
    "plt.savefig('plots/sn_likelihoods.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making interim posteriors\n",
    "\n",
    "The likelihoods we just constructed are all well and good, and they really do exist in nature.  However, they are not things we observers are in general able to obtain.  To use them in inference, we would need to regularly integrate over the entire space of data, which is rarely something we know how to do.  When we run a code that produces the probability distribution of unobservable parameters, it really calculates a posterior, not a likelihood; because its inputs are data, it must be conditioning its estimate on that information.  \n",
    "\n",
    "In addition to that, any way of estimating our unobservable parameters from data also conditions their posteriors on other information, namely interim priors and selection functions.  In reality, when we perform classification, lightcurve fitting, and photo-$z$ PDF estimation, we are finding interim posteriors $p(t_{n}, z_{n}, \\mu_{n} | \\underline{\\ell}_{n}, \\vec{f}_{n}, \\underline{\\xi}, \\vec{\\alpha}, \\vec{\\beta})$ instead of likelihoods $p(\\underline{\\ell}_{n}, \\vec{f}_{n} | t_{n}, z_{n}, \\mu_{n})$, due to the assumptions about the distributions of our latent variables and the propagation of selection effects on the data.  In our case, choices of the interim hyperpriors $\\underline{\\xi}$ will translate directly into a prior belief about the `3D` distribution $p(t, z, \\mu | \\underline{\\xi}, \\vec{\\alpha}, \\vec{\\beta})$ that is independent of any observations (and thus independent of $n$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing interim priors\n",
    "\n",
    "To transform our likelihoods into interim posteriors that accurately represent what we expect a real data analysis pipeline to produce, we must choose interim priors.  The interim prior represents the $p(t, z, \\mu)$ that is used in the estimation of log-posterior probabilities -- our assumptions about $p(t_{n}, z_{n}, \\mu_{n} | \\underline{\\ell}_{n}, \\vec{f}_{n})$ are parametrized by the interim prior parameters comprising $\\underline{\\xi}$, so the closest we can get to the desired posteriors is the interim posteriors $\\ln[p(t_{n}, z_{n}, \\mu_{n} | \\underline{\\ell}_{n}, \\vec{f}_{n}, \\underline{\\xi})] = \\ln[p(t_{n}, z_{n}, \\mu_{n} | \\underline{\\ell}_{n}, \\vec{f}_{n})] + \\ln[p(t_{n}, z_{n}, \\mu_{n} | \\underline{\\xi})]$.  \n",
    "\n",
    "This section is about making the interim prior $\\ln[p(t, z, \\mu | \\underline{\\xi})]$.  We will set a uniform prior on the redshift-dependent supernova proportions and use the WMAP values of the cosmological parameters with grossly inflated error bars to restrict the parameter space of $z$ and $\\mu$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# separate interim prior from LC fitter and photo-z PDFs: this is for photo-z PDFs, flat for now, replace with SDSS n(z)\n",
    "pz_interim = np.ones(n_zs-1)\n",
    "pz_interim /= np.sum(pz_interim * z_difs)\n",
    "assert np.isclose(np.sum(pz_interim * z_difs), 1.)\n",
    "ln_pz_interim = safe_log(pz_interim)\n",
    "\n",
    "#interim_hyperparams = {}\n",
    "#interim_hyperparams['pz_int_pr'] = ln_pz_interim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(z_mids, pz_interim, color='k')\n",
    "plt.xlabel(r'$z$')\n",
    "plt.ylabel(r'$p(z)$')\n",
    "plt.title(r'interim redshift distribution function')\n",
    "plt.savefig('plots/pz_interim_prior.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SN interim prior in z-dependent type proportion space\n",
    "sn_interim_nz = np.ones((n_types, n_zs-1))#this is flat, replace this with nontrivial interim prior on types and redshifts\n",
    "sn_interim_nz /= np.sum(sn_interim_nz * z_difs[np.newaxis, :])\n",
    "assert np.isclose(np.sum(sn_interim_nz * z_difs[np.newaxis, :]), 1.)\n",
    "ln_sn_interim_nz = safe_log(sn_interim_nz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flat prior on redshift-dependent SNe proportions, replace with SDSS n(z)\n",
    "# interim_n_of_z = np.ones((n_types, n_zs-1))\n",
    "# interim_n_of_z /= np.sum(interim_n_of_z * z_difs[np.newaxis, :])\n",
    "# assert np.isclose(np.sum(interim_n_of_z * z_difs[np.newaxis, :]), 1.)\n",
    "# interim_ln_n_of_z = safe_log(interim_n_of_z)\n",
    "\n",
    "# read these off Planck plots\n",
    "interim_w0 = -1.0\n",
    "delta_w0 = 0.5#0.2# * 10.\n",
    "interim_wa = 0.0\n",
    "delta_wa = 0.5#1.5# * 10.\n",
    "interim_cosmo_hyperparams = np.array([interim_w0, interim_wa])\n",
    "interim_cosmo_hyperparam_sigmas = np.array([delta_w0, delta_wa])\n",
    "interim_cosmo_hyperparam_vars = (interim_cosmo_hyperparam_sigmas) * np.eye(n_hyperparams)\n",
    "interim_dist = sps.multivariate_normal(mean = interim_cosmo_hyperparams, cov = interim_cosmo_hyperparam_vars)\n",
    "interim_cosmo = cosmology.w0waCDM(true_H0, true_Om0, true_Ode0, w0=interim_w0, wa=interim_wa)\n",
    "\n",
    "# may have to change this if nontrivial covariances between hyperparameters=\n",
    "#interim_hyperparams['theta'] = np.array([interim_cosmo_hyperparams, interim_cosmo_hyperparam_sigmas])\n",
    "#interim_hyperparams['phi'] = interim_ln_n_of_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must now define a function that produces the posterior probability of a supernova taking the value of a particular pair $(z, \\mu)$ under a given set of cosmological parameters $\\vec{\\Omega}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inverter(z, mu):\n",
    "    #note: this inverter is slow! perhaps we could speed it up with interpolation over predefined grids?\n",
    "    def cosmo_helper(hyperparams):\n",
    "        return np.array([abs(cosmology.w0waCDM(true_H0, true_Om0, true_Ode0, w0=hyperparams[0], wa=hyperparams[1]).distmod(z).value - mu)])\n",
    "    solved_cosmo = spo.minimize(cosmo_helper, interim_cosmo_hyperparams, method=\"Nelder-Mead\", options={\"maxfev\": 1e5, \"maxiter\":1e5})\n",
    "    ln_prob = interim_dist.logpdf(solved_cosmo.x)\n",
    "    return ln_prob#max(prob, sys.float_info.epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this function, we can construct an interim prior probability, which has the dimensions of a single `3D` probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SN interim prior in Hubble diagram space\n",
    "ln_sn_interim_hubble = np.zeros((n_zs-1, n_mus-1))\n",
    "for z in range(n_zs-1):\n",
    "    for mu in range(n_mus-1):\n",
    "        ln_prob = inverter(z_mids[z], mu_mids[mu])\n",
    "        ln_sn_interim_hubble[z][mu] = ln_prob\n",
    "        \n",
    "ln_sn_interim = reg_vals(ln_sn_interim_nz[:, np.newaxis] + ln_sn_interim_hubble[np.newaxis, :])\n",
    "sn_interim = np.exp(ln_sn_interim)\n",
    "sn_interim /= np.sum(sn_interim * z_difs[np.newaxis, :, np.newaxis] * mu_difs[np.newaxis, np.newaxis, :])\n",
    "ln_sn_interim = safe_log(sn_interim)\n",
    "assert np.isclose(np.sum(sn_interim * z_difs[np.newaxis, :, np.newaxis] * mu_difs[np.newaxis, np.newaxis, :]), 1.)\n",
    "\n",
    "#interim_hyperparams['SN_int_pr'] = ln_sn_interim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Something is very wrong here, rounding error is part of it\n",
    "fig = plt.figure(figsize=(n_types*6, 5))\n",
    "p = 0\n",
    "for t in range(n_types):\n",
    "    p += 1\n",
    "    plt.subplot(1, n_types, p)\n",
    "    plt.pcolormesh(z_bins, mu_bins, ln_sn_interim[t].T, cmap='viridis')\n",
    "    plt.plot(z_bins, [true_cosmo.distmod(z).value for z in z_bins], color='k', label='true Hubble relation')\n",
    "    plt.plot(z_bins, [interim_cosmo.distmod(z).value for z in z_bins], color='r', label='interim Hubble relation')\n",
    "    plt.colorbar()\n",
    "    plt.title('log interim prior')\n",
    "    plt.xlabel(r'$z$')\n",
    "    plt.ylabel(r'$\\mu$')\n",
    "    plt.axis([z_bins[0], z_bins[-1], mu_bins[0], mu_bins[-1]])\n",
    "plt.savefig('plots/lc_interim.png')\n",
    "\n",
    "# plt.pcolormesh(z_bins, mu_bins, ln_sn_interim[0].T, cmap='viridis')#, vmin = 0., vmax = 3.)\n",
    "# plt.plot(z_bins, [true_cosmo.distmod(z).value for z in z_bins], color='k', label='true Hubble relation')\n",
    "# plt.title('interim prior distribution')\n",
    "# plt.xlabel(r'$z$')\n",
    "# plt.ylabel(r'$\\mu$')\n",
    "# plt.legend(loc='lower right', fontsize='small')\n",
    "# plt.axis([z_bins[0], z_bins[-1], mu_bins[0], mu_bins[-1]])\n",
    "# plt.colorbar()\n",
    "# plt.savefig('plots/sn_interim_prior.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "interim_ln_prior = ln_sn_interim + ln_pz_interim[np.newaxis, np.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # note the approximation of cdf[z_min, z_max] = pdf[z_mid]\n",
    "# interim_sheet = np.zeros((n_zs-1, n_mus-1))\n",
    "# for z in range(n_zs-1):\n",
    "#     for mu in range(n_mus-1):\n",
    "#         ln_prob = inverter(z_mids[z], mu_mids[mu])\n",
    "#         interim_sheet[z][mu] = ln_prob\n",
    "# interim_ln_prior = reg_vals(interim_ln_n_of_z[:, np.newaxis] + interim_sheet[np.newaxis, :])\n",
    "# interim_prior = np.exp(interim_ln_prior)\n",
    "# interim_prior /= np.sum(interim_prior * z_difs[np.newaxis, :, np.newaxis] * mu_difs[np.newaxis, np.newaxis, :])\n",
    "# interim_ln_prior = safe_log(interim_prior)\n",
    "# assert np.isclose(np.sum(interim_prior * z_difs[np.newaxis, :, np.newaxis] * mu_difs[np.newaxis, np.newaxis, :]), 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the log of the combined interim prior and show that it is reasonable but not too restrictive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # the interim prior and truth are way too close to each other. . . but that's realistic\n",
    "# plt.pcolormesh(z_bins, mu_bins, interim_ln_prior[0].T, cmap='viridis')#, vmin = 0., vmax = 3.)\n",
    "# plt.plot(z_bins, [true_cosmo.distmod(z).value for z in z_bins], color='k', label='true Hubble relation')\n",
    "# plt.title('interim prior distribution')\n",
    "# plt.xlabel(r'$z$')\n",
    "# plt.ylabel(r'$\\mu$')\n",
    "# plt.legend(loc='lower right', fontsize='small')\n",
    "# plt.axis([z_bins[0], z_bins[-1], mu_bins[0], mu_bins[-1]])\n",
    "# plt.colorbar()\n",
    "# plt.savefig('plots/interim_prior.png')\n",
    "fig = plt.figure(figsize=(n_types*6, 5))\n",
    "p = 0\n",
    "for t in range(n_types):\n",
    "    p += 1\n",
    "    plt.subplot(1, n_types, p)\n",
    "    plt.pcolormesh(z_bins, mu_bins, interim_ln_prior[t].T, cmap='viridis')\n",
    "    plt.plot(z_bins, [true_cosmo.distmod(z).value for z in z_bins], color='k', label='true Hubble relation')\n",
    "    plt.plot(z_bins, [interim_cosmo.distmod(z).value for z in z_bins], color='r', label='interim Hubble relation')\n",
    "    plt.colorbar()\n",
    "    plt.title('log interim prior')\n",
    "    plt.xlabel(r'$z$')\n",
    "    plt.ylabel(r'$\\mu$')\n",
    "    plt.axis([z_bins[0], z_bins[-1], mu_bins[0], mu_bins[-1]])\n",
    "plt.savefig('plots/full_interim_prior.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection Functions\n",
    "\n",
    "We have not yet included selection effects in this treatment.  The graphical model included known hyperparameters $\\vec{\\alpha}$ and $\\vec{\\beta}$ to represent the selection function for supernova lightcurves and host galaxy photometry.  If these hyperparameters are known, then we can safely say we know $p(\\underline{\\ell}_{n} | \\vec{\\alpha})$ and $p(\\vec{f}_{n} | \\vec{\\beta})$ for all supernovae $n$ in our catalog.  However, what we really need is $p(t_{n}, z_{n}, \\mu_{n} | \\vec{\\alpha}, \\vec{\\beta})$.  We obtain these using an approach similar to <a href=\"https://github.com/ixkael/Photoz-tools/blob/master/Photoz%20galaxy%20survey%20mock%20and%20N(z)%20inference.ipynb\">that of Boris Leistedt</a>.  *Actually, I'm not really sure how to relate what we're doing, which we discussed with Boris, to what's in his notebook.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The host galaxy redshift selection function*\n",
    "\n",
    "The selection function can be represented by $\\ln[p(z | \\vec{\\beta})]$.  A selection function in which $\\vec{\\beta}$ is comprised of magnitude limits in all photometric bands is commonly imposed on galaxy surveys, such that $p(\\vec{f} | \\vec{\\beta})$ is known (and often is a step function in as many dimensions as there are filters).  Furthermore, we have a reasonably good idea of what $p(z | \\vec{f})$ is.  The selection function we want is just $\\int\\ p(z | \\vec{f})\\ p(\\vec{f} | \\vec{\\beta})\\ d\\vec{f}$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We emulate this using data from a realistic galaxy simulation.\n",
    "# We want the number of galaxies as a function of redshift, SED type, and luminosity.\n",
    "# (Buzzard, for example, includes this.)\n",
    "# Using a realistic set of magnitude limits, we want to calculate the recovered fraction\n",
    "# of galaxies as a function of redshift, SED type, and luminosity.\n",
    "# Then we just integrate over SED type and luminosity to get $p(z | \\vec{\\beta})$\n",
    "\n",
    "# pz_selfun = np.ones(n_zs-1)\n",
    "# pz_selfun /= np.sum(pz_selfun * z_difs)\n",
    "# assert np.isclose(np.sum(pz_selfun * z_difs), 1.)\n",
    "# ln_pz_selfun = safe_log(pz_selfun)\n",
    "\n",
    "host_sel_fun = np.ones(n_zs-1)\n",
    "host_sel_fun_norm = np.sum(host_sel_fun * z_difs)\n",
    "host_sel_fun /= host_sel_fun_norm\n",
    "assert np.isclose(np.sum(host_sel_fun * z_difs), 1.)\n",
    "ln_host_selection_function = safe_log(host_sel_fun)\n",
    "\n",
    "#interim_hyperparams['pz_sel_fun'] = ln_host_selection_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(z_mids, host_sel_fun, color='k')\n",
    "plt.xlabel(r'$z$')\n",
    "plt.ylabel(r'$p(z)$')\n",
    "plt.title(r'host galaxy redshift selection function')\n",
    "plt.savefig('plots/pz_sel_fun.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The supernova lightcurve selection function*\n",
    "\n",
    "The supernova lightcurve selection function will be more complicated than the host galxy photometry selection function for a variety of reasons.\n",
    "\n",
    "*We are actively considering strategies for implementation in our mock data, with two main approaches.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "_Empirical proposal_\n",
    "\n",
    "We want to go directly to $p(t, z, \\mu | \\vec{\\alpha})$ knowing only $p(\\underline{\\ell} | \\vec{\\alpha})$ and $p(t, z, \\mu | \\underline{\\ell})$.  We note that in the empirical approach, because $\\mu$ is determined by $z$ in the probailistic graphical model, we can say $p(t, z, \\mu | \\vec{\\alpha}) = p(t, z | \\vec{\\alpha}) = \\int\\ p(t, z | \\underline{\\ell})\\ p(\\underline{\\ell} | \\vec{\\alpha})\\ d\\underline{\\ell}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We emulate this using data from a realistic supernova simulation.\n",
    "# We want the number of supernovae as a function of redshift and SN type.\n",
    "# (The cadence studies with SNANA should have this.)\n",
    "# Using a realistic set of selection cuts, we want to calculate the recovered fraction\n",
    "# of SN as a function of SN type and redshift under a given survey strategy\n",
    "# relative to the \"best possible\" survey strategy.\n",
    "with open('data/ratios_wfd.txt', 'r') as wfd_file:\n",
    "#     wfd_file.next()\n",
    "    tuples = (line.split(None) for line in wfd_file)\n",
    "    wfddata = [[pair[k] for k in range(0,len(pair))] for pair in tuples]\n",
    "zs_eval = np.array([float(wfddata[i][0]) for i in range(1, n_zs)])\n",
    "wfd_data = np.array([np.array([int(wfddata[i][2 * j]) for j in range(1, (len(wfddata[i]))/2+1)]) for i in range(1, n_zs)])\n",
    "wfd_data[np.isnan(wfd_data)] = 0.\n",
    "# print(wfd_data)\n",
    "with open('data/ratios_ddf.txt', 'r') as ddf_file:\n",
    "#     ddf_file.next()\n",
    "    tuples = (line.split(None) for line in ddf_file)\n",
    "    ddfdata = [[pair[k] for k in range(0,len(pair))] for pair in tuples]\n",
    "ddf_data = np.array([np.array([float(ddfdata[i][2 * j]) for j in range(1, (len(ddfdata[i]))/2+1)]) for i in range(1, n_zs)])\n",
    "# print(ddf_data)\n",
    "# these are the recovery rates\n",
    "sn_sel_fun = wfd_data / ddf_data\n",
    "sn_sel_fun = sn_sel_fun.T\n",
    "# print(sn_sel_fun)\n",
    "# It's actually a big problem for the selection function to go to 0 or exceed 1.\n",
    "# Note: these need to be normalized by survey volume so are not valid at this time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#need to fix this to interpolate selection function to grid but broken for now\n",
    "# #purity = # real classified as real / all classified as real\n",
    "# #want real classified as real / all true real\n",
    "# #for now using purity as what we want\n",
    "# #should use really low purity for testing to check that it's doing something!\n",
    "# #need this to be # types * # z bins in shape\n",
    "# interpolator = spi.interp1d(zs_eval, wfd_data.T[1])\n",
    "# interpolated = interpolator(z_mids)\n",
    "# sn_sel_fun = np.vstack((interpolated, 1.*interpolated/3., 2.*interpolated/3.))\n",
    "# #should divide this by DDF, for now say it's perfect\n",
    "# sn_sel_fun /= np.ones((n_types, n_zs))\n",
    "for t in range(n_types):\n",
    "    plt.plot(z_mids, sn_sel_fun[t], label=types[t])\n",
    "plt.xlabel(r'$z$')\n",
    "plt.ylabel('recovery fraction')\n",
    "plt.legend(loc='upper right')\n",
    "plt.savefig('plots/lc_sel_func.png')\n",
    "#label this\n",
    "#will need this for inference, should write to file and skeleton should read it in\n",
    "sn_sel_fun_norm = np.sum(sn_sel_fun * z_difs[np.newaxis, :])\n",
    "sn_sel_fun /= sn_sel_fun_norm\n",
    "assert np.isclose(np.sum(sn_sel_fun * z_difs[np.newaxis, :]), 1.)\n",
    "ln_sn_selection_function = safe_log(sn_sel_fun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Model-based proposal_\n",
    "\n",
    "If the selection cuts aren't in the space of lightcurves but are in the space of lightcurve fit parameters $x$, $m$, and $c$, we have a slightly different approach.  In this case, we have $p(t, z, \\mu | \\vec{\\alpha}) = \\iiint\\ p(t, z, \\mu | x, m, c)\\ p(x, m, c | \\vec{\\alpha})\\ dx\\ dm\\ dc$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We emulate this using data from a realistic supernova simulation.\n",
    "# We want the number of supernovae as a function of their fit parameters.\n",
    "# (The cadence studies with SNANA should have this.)\n",
    "# Using a realistic set of selection cuts, we want to calculate the recovered fraction\n",
    "# of SN as a function of SN type, redshift, and distance modulus\n",
    "# under a given survey strategy relative to the \"best possible\" survey strategy.\n",
    "host_sel_fun = np.ones(n_zs-1)\n",
    "host_sel_fun_norm = np.sum(host_sel_fun * z_difs)\n",
    "host_sel_fun /= host_sel_fun_norm\n",
    "assert np.isclose(np.sum(host_sel_fun * z_difs), 1.)\n",
    "ln_host_selection_function = safe_log(host_sel_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wrap up the simulation/emulation of the selection functions by combining the terms as follows: $\\ln[p(t, z, \\mu | \\vec{\\alpha}, \\vec{\\beta})] = \\ln[p(t, z, \\mu | \\vec{\\alpha})] + \\ln[p(z | \\vec{\\beta})]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put together those terms\n",
    "\n",
    "selection_sheet = np.zeros((n_types, n_zs-1, n_mus-1))\n",
    "ln_selection_function = reg_vals(ln_sn_selection_function[:, np.newaxis] + ln_host_selection_function[np.newaxis, :, np.newaxis] + selection_sheet)\n",
    "selection_function = np.exp(ln_selection_function)\n",
    "sel_fun_norm = np.sum(selection_function * z_difs[np.newaxis, :, np.newaxis] * mu_difs[np.newaxis, np.newaxis, :] / n_types)\n",
    "selection_function /= sel_fun_norm\n",
    "assert np.isclose(np.sum(selection_function * z_difs[np.newaxis, :, np.newaxis] * mu_difs[np.newaxis, np.newaxis, :] / n_types), 1.)\n",
    "ln_selction_function = safe_log(selection_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final result: the mock interim posteriors\n",
    "\n",
    "We will create mock interim posteriors according to the following, based on Bayes' Rule.\n",
    "\n",
    "\\begin{align*}\n",
    "\\ln[p(t_{n}, z_{n}, \\mu_{n} | \\underline{\\ell}_{n}, \\vec{f}_{n}, \\underline{\\xi}, \\vec{\\alpha}, \\vec{\\beta})] &\\propto \\ln[p(\\underline{\\ell}_{n}, \\vec{f}_{n} | t_{n}, z_{n}, \\mu_{n})] + \\ln[p(t, z, \\mu | \\vec{\\alpha}, \\vec{\\beta})] + \\ln[p(t, z, \\mu | \\vec{\\xi})]\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# note likelihoods were not normalized; this must be normalized\n",
    "interim_ln_posteriors = reg_vals(ln_likelihoods + interim_ln_prior[np.newaxis, :] + ln_selection_function)\n",
    "interim_posteriors = np.exp(interim_ln_posteriors)\n",
    "interim_posteriors /= np.sum(interim_posteriors * z_difs[np.newaxis, :, np.newaxis] * mu_difs[np.newaxis, np.newaxis, :])\n",
    "interim_ln_posteriors = safe_log(interim_posteriors)\n",
    "print np.shape(interim_posteriors), np.shape(interim_ln_posteriors)\n",
    "print np.shape(interim_posteriors[2][0]), np.shape(interim_ln_posteriors[2][0])\n",
    "# print zip(interim_posteriors[2][0], interim_ln_posteriors[2][0])\n",
    "interim_ln_posteriors = ln_likelihoods + interim_ln_prior[np.newaxis, :]\n",
    "interim_posteriors = np.exp(interim_ln_posteriors)\n",
    "interim_posteriors /= np.sum(interim_posteriors * z_difs[np.newaxis, :, np.newaxis] * mu_difs[np.newaxis, np.newaxis, :])\n",
    "assert np.isclose(np.sum(interim_posteriors * z_difs[np.newaxis, :, np.newaxis] * mu_difs[np.newaxis, np.newaxis, :]), 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot a few examples of the final mock interim posteriors.  These will be the input to `scippr`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# these are going to get a lot narrower\n",
    "fig = plt.figure(figsize=(n_types*5, len(colors)*5))\n",
    "p = 0\n",
    "for s in range(n_sne)[:len(colors)]:\n",
    "    for t in range(n_types):\n",
    "        p += 1\n",
    "        plt.subplot(len(colors), n_types, p)\n",
    "        plt.pcolormesh(z_bins, mu_bins, interim_ln_posteriors[s][t].T, cmap='viridis')#, vmin = 0., vmax = 3.)\n",
    "        plt.colorbar()\n",
    "        plt.scatter(true_params[s]['z'], true_params[s]['mu'], color='r')\n",
    "        plt.title('true '+true_params[s]['t']+', class '+types[t])\n",
    "        plt.xlabel(r'$z$')\n",
    "        plt.ylabel(r'$\\mu$')\n",
    "        plt.axis([z_bins[0], z_bins[-1], mu_bins[0], mu_bins[-1]])\n",
    "plt.savefig('plots/out_interim_posteriors.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[write data to file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# write true hyperparameters just to check\n",
    "\n",
    "truth = {}\n",
    "binned_n_of_z = np.zeros((n_types, n_zs-1))\n",
    "for t in range(n_types):\n",
    "    cdfs = true_n_of_z[t].cdf(z_bins)\n",
    "    binned_n_of_z[t] = (cdfs[1:] - cdfs[:-1])\n",
    "binned_n_of_z = frac_types[:, np.newaxis] * np.array(binned_n_of_z)# / z_range\n",
    "binned_n_of_z /= np.sum(binned_n_of_z * z_difs[np.newaxis, :])\n",
    "assert np.isclose(np.sum(binned_n_of_z * z_difs[np.newaxis, :]), 1.)\n",
    "truth['phi'] = binned_n_of_z\n",
    "truth['theta'] = true_hyperparams\n",
    "truth['data'] = true_params\n",
    "\n",
    "with open('data/truth.hkl', 'w') as true_file:\n",
    "    hickle.dump(truth, true_file)\n",
    "\n",
    "# write axes (types, z_bins, mu_bins)\n",
    "# write interim prior (interim_ln_prior)\n",
    "# write interim posteriors (ln_interim_posteriors)\n",
    "\n",
    "output = {'types': types, 'z_bins': z_bins, 'mu_bins': mu_bins}\n",
    "output['ln selection function'] = ln_selection_function\n",
    "output['interim ln prior'] = interim_ln_prior\n",
    "output['interim ln posteriors'] = interim_ln_posteriors\n",
    "\n",
    "with open('data/data.hkl', 'w') as out_file:\n",
    "    hickle.dump(output, out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch space (Please ignore after this point!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I was sort of proud of writing this function that taught me some array manipulation stuff, so I'm keeping it here for now.\n",
    "def sample_discrete(dist, N, discs, difs, bins):\n",
    "    out_info = []\n",
    "    norm_dist = dist * difs[np.newaxis, :]\n",
    "    assert np.isclose(np.sum(norm_dist), 1.)\n",
    "    dist_shape = np.shape(norm_dist)\n",
    "    flat_dist = norm_dist.flatten()\n",
    "    cdf = np.cumsum(flat_dist)\n",
    "    for n in range(N):\n",
    "        each = {}\n",
    "        r = np.random.random()\n",
    "        k = bisect.bisect(cdf, r)\n",
    "        (t_ind, z_ind) = np.unravel_index(k, dist_shape)\n",
    "        each['t'] = discs[t_ind]\n",
    "        each['z'] = np.random.uniform(low=bins[z_ind], high=bins[z_ind + 1])\n",
    "        out_info.append(each)\n",
    "    return out_info"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
