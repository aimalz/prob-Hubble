{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Mock Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scippr` requires inputs in the form of catalogs $\\{\\ln[p(t_{n}, z_{n}, \\mu_{n} | \\underline{\\ell}_{n}, \\vec{f}_{n}, \\underline{\\Phi}^{*}, \\vec{\\varphi}^{*}, \\underline{\\alpha}, \\vec{\\beta})]\\}_{N}$ of interim log-posteriors expressed as `3D` arrays constituting probabilities over $t_{n}$, $z_{n}$, and $\\mu_{n}$, enabling rapid computation of the log-posterior $\\ln[p(\\underline{\\phi}, \\vec{\\Omega} | \\{\\underline{\\ell}_{n}, \\vec{f}_{n}\\}_{N}, \\underline{\\alpha}, \\vec{\\beta})$ over the hyperparameters $\\underline{\\phi}$ and $\\vec{\\Omega}$ of scientific interest.  This notebook outlines a procedure for generating such a catalog.\n",
    "\n",
    "Perhaps the defining feature of this pipeline is that it does not involve simulating supernova lightcurves or host galaxy photometry and instead simulating the interim posteriors directly.  There are several good reasons for this choice:\n",
    "\n",
    "* The motivation for `scippr` is to develop a method for doing inference with accurate probability distributions over relevant supernova parameters, not to develop methods for obtaining those probability distributions.\n",
    "* We avoid tying our inference method to a particular way of deriving interim posteriors from observed data.\n",
    "* We avoid making assumptions about the details of the observed data, such as the photometric filters, intrinsic lightcurves, and observing conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import daft\n",
    "import astropy.cosmology as cosmology\n",
    "\n",
    "import numpy as np\n",
    "import bisect\n",
    "import scipy.stats as sps\n",
    "import scipy.interpolate as spi\n",
    "import scipy.optimize as spo\n",
    "import hickle\n",
    "\n",
    "import os\n",
    "paths = ['data', 'plots']\n",
    "if not os.path.exists('data'):\n",
    "    print('WARNING: You will need to put some data files in the `data` directory to have nontrivial mock data.')\n",
    "for path in paths:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "import sys\n",
    "log_epsilon = sys.float_info.min_exp\n",
    "epsilon = sys.float_info.min\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib import rc\n",
    "rc(\"font\", family=\"serif\", size=12)\n",
    "rc(\"text\", usetex=True)\n",
    "\n",
    "# colors = 'rbgcymk'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scippr` is based on a probabilistic graphical model, illustrated below.  The model has two types of observables, shown in shaded circles, supernova lightcurves $\\underline{\\ell}_{n}$ and host galaxy photometry $\\vec{f}_{n}$.  The parameters, which are by definition not directly observable, are shown in empty circles.  The latent variables of supernova type $t_{n}$, redshift $z_{n}$, and distance modulus $\\mu_{n}$ are parameters over which we will marginalize, without ever directly inferring them, and while all three of them influence $\\underline{\\ell}_{n}$, only $z_{n}$ affects $\\vec{f}_{n}$ in this model.  In other words, _we currently assume no relationship between supernova type and host galaxy photometry, an assumption we may revisit in the future_.  The selection function parameters $\\underline{\\alpha}$ and $\\vec{\\beta}$ are known constants of the survey symbolized by dots that influence the possible lightcurves and host galaxy photometry that are included in the sample.  The box indicates that the latent variables and the observables are generated independently $N$ times for each supernova in the sample.  The hyperparameters we would like to estimate are the redshift-dependent supernova type proportions $\\underline{\\phi}$ that determine $t_{n}$ and $z_{n}$ and the cosmological parameters $\\vec{\\Omega}$ that relate $z_{n}$ to $\\mu_{n}$, which are shared by all $N$ supernovae in the observed sample.  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#initialize the PGM\n",
    "pgm = daft.PGM([6, 4.5], origin=[0, 0])\n",
    "\n",
    "#desired hyperparameters\n",
    "pgm.add_node(daft.Node(\"cosmology\", r\"$\\vec{\\Omega}$\", 2., 4.))\n",
    "pgm.add_node(daft.Node(\"dist\", r\"$\\underline{\\phi}$\", 3.5, 4.))\n",
    "#pgm.add_node(daft.Node(\"rates\", r\"$\\vec{R}$\", 3., 5.5, fixed=True))\n",
    "\n",
    "#latent variables/parameters\n",
    "pgm.add_node(daft.Node(\"distance\", r\"$\\mu_{n}$\", 2., 2.5))\n",
    "pgm.add_node(daft.Node(\"redshift\", r\"$z_{n}$\", 3., 3.))\n",
    "pgm.add_node(daft.Node(\"type\", r\"$t_{n}$\", 4., 2.5))\n",
    "\n",
    "#data\n",
    "pgm.add_node(daft.Node(\"lightcurve\", r\"$\\underline{\\ell}_{n}$\", 2.5, 1., observed=True))\n",
    "pgm.add_node(daft.Node(\"photometry\", r\"$\\vec{f}_{n}$\", 3.5, 1., observed=True))\n",
    "\n",
    "#known constant parameters\n",
    "pgm.add_node(daft.Node(\"lightcurve selection\", r\"$\\underline{\\alpha}$\", 1., 1.75, fixed=True))\n",
    "pgm.add_node(daft.Node(\"photometry selection\", r\"$\\vec{\\beta}$\", 5., 1.75, fixed=True))\n",
    "\n",
    "# Add in the edges.\n",
    "pgm.add_edge(\"dist\", \"type\")\n",
    "pgm.add_edge(\"cosmology\", \"distance\")\n",
    "pgm.add_edge(\"dist\", \"redshift\")\n",
    "pgm.add_edge(\"redshift\", \"distance\")\n",
    "#pgm.add_edge(\"distance\", \"photometry\")\n",
    "pgm.add_edge(\"distance\", \"lightcurve\")\n",
    "pgm.add_edge(\"redshift\", \"photometry\")\n",
    "pgm.add_edge(\"redshift\", \"lightcurve\")\n",
    "pgm.add_edge(\"type\", \"lightcurve\")\n",
    "pgm.add_edge(\"photometry selection\", \"photometry\")\n",
    "pgm.add_edge(\"lightcurve selection\", \"lightcurve\")\n",
    "\n",
    "# plates\n",
    "pgm.add_plate(daft.Plate([1.5, 0.5, 3., 3.], label=r\"$n = 1, \\cdots, N$\"))\n",
    "\n",
    "# Render and save.\n",
    "pgm.render()\n",
    "pgm.figure.show()\n",
    "pgm.figure.savefig('plots/sim_pgm.png', dpi=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simulate a mock catalog for `scippr`, there are three main steps.\n",
    "\n",
    "1. Choose true values for the hyperparameters, which we would like to recover from our inference, and the parameters, over which we intend to marginalize.\n",
    "2. Create likelihoods based on a model for how they are derived from observations under the selection functions.\n",
    "3. Make interim posteriors by assuming interim priors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing true hyperparameters and parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The redshift-dependent supernova type proportions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that the true redshift-dependent type proportions are provided as parameters $\\underline{\\phi}'$ of continuous functions for a finite number of types across a restricted redshift range.  \n",
    "\n",
    "*[The following cell is a placeholder for a realistic $\\underline{\\phi}'$.  It will soon be replaced with reading in a file containing this information.]*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = ['Ia', 'Ibc', 'II']\n",
    "colors = ['b', 'm', 'g']\n",
    "n_types = len(types)\n",
    "# making up the type fractions, will replace this with data soon!\n",
    "frac_types = np.array([0.4, 0.1, 0.5])\n",
    "assert np.isclose(np.sum(frac_types), 1.)\n",
    "\n",
    "# these arbitrary limits are from the selection function\n",
    "min_z = 0.2\n",
    "max_z = 0.6\n",
    "\n",
    "n_of_z_consts = {}\n",
    "n_of_z_consts['Ia'] = (1.5, 0.5)\n",
    "n_of_z_consts['Ibc'] = (1., 0.5)\n",
    "n_of_z_consts['II'] = (0.5, 0.5)\n",
    "\n",
    "true_n_of_z = []\n",
    "for t in types:\n",
    "    (mean, std) = n_of_z_consts[t]\n",
    "    low, high = (min_z - mean) / std, (max_z - mean) / std\n",
    "    true_n_of_z.append(sps.truncnorm(low, high, loc = mean, scale = std))\n",
    "\n",
    "plot_res = 20\n",
    "z_range = max_z - min_z\n",
    "z_grid = np.linspace(min_z, max_z, num=plot_res + 1, endpoint=True)\n",
    "z_plot = (z_grid[1:] + z_grid[:-1]) / 2.\n",
    "z_dif_plot = z_grid[1:] - z_grid[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we plot the true redshift-dependent type proportions after ensuring they are properly normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_true_n_of_z = np.zeros((n_types, plot_res))\n",
    "for t in range(n_types):\n",
    "    plot_true_n_of_z[t] = true_n_of_z[t].pdf(z_plot)\n",
    "plot_true_n_of_z = frac_types[:, np.newaxis] * np.array(plot_true_n_of_z)# / z_range\n",
    "plot_true_n_of_z /= np.sum(plot_true_n_of_z * z_dif_plot)\n",
    "assert np.isclose(np.sum(plot_true_n_of_z * z_dif_plot), 1.)\n",
    "\n",
    "for t in range(n_types):\n",
    "    plt.plot(z_plot, plot_true_n_of_z[t], color=colors[t], label=types[t])\n",
    "plt.xlabel(r'$z$')\n",
    "plt.ylabel(r'relative rate')\n",
    "plt.legend()\n",
    "plt.savefig('plots/true_rates.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sample pairs of type and redshift from these distributions by choosing a type based on the overall type proportions, then sampling its underlying redshift distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_discrete(fracs, n_of_z, N):\n",
    "    found_types = [0, 0, 0]\n",
    "    poster_indices = []\n",
    "    out_info = []\n",
    "    cdf = np.cumsum(fracs)\n",
    "    for n in range(N):\n",
    "        each = {}\n",
    "        r = np.random.random()\n",
    "        k = bisect.bisect(cdf, r)\n",
    "        each['t'] = types[k]\n",
    "        each['z'] = n_of_z[k].rvs()\n",
    "        out_info.append(each)\n",
    "    return out_info\n",
    "\n",
    "n_sne = 100\n",
    "true_id = range(100)\n",
    "\n",
    "true_params = sample_discrete(frac_types, true_n_of_z, n_sne)\n",
    "\n",
    "true_zs = [true_param['z'] for true_param in true_params]\n",
    "true_types = [true_param['t'] for true_param in true_params]\n",
    "posters = []\n",
    "posters.append(next(i for i,v in enumerate(true_types) if v == 'Ia'))\n",
    "posters.append(next(i for i,v in enumerate(true_types) if v == 'Ibc'))\n",
    "posters.append(next(i for i,v in enumerate(true_types) if v == 'II'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_id = range(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((1.29/1)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we plot a histogram of the true redshift values for the three types of supernovae, along with the redshift-dependent type functions from which they were drawn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot = [[d['z'] for d in true_params if d['t'] == types[t]] for t in range(n_types)]\n",
    "hist_bins = np.linspace(min_z, max_z, plot_res + 1)\n",
    "bin_difs = hist_bins[1:] - hist_bins[:-1]\n",
    "for t in range(n_types):\n",
    "    plt.plot(z_plot, plot_true_n_of_z[t] * n_sne * bin_difs, color=colors[t], label='true '+types[t])\n",
    "    plt.hist(to_plot[t], bins=hist_bins, color=colors[t], alpha=1./3., label='sampled '+types[t], normed=False)\n",
    "plt.xlabel(r'$z$')\n",
    "plt.ylabel(r'relative rate')\n",
    "plt.legend(fontsize='xx-small')\n",
    "plt.savefig('plots/obs_rates.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The true cosmological parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the true hyperparameter vector $\\vec{\\Omega}'$ as having two components, $w_{0}'$ and $w_{a}'$.  In a future revision, we may include additional cosmological parameters in this hyperparameter vector, but for now, they are held constant.  We choose the true values for the cosmological parameters to be those published by /Planck/ (2015, paper XIV, Figure 3).  [include citation]  \n",
    "\n",
    "Since every supernova in our sample already has a true redshift $z_{n}'$, we can easily establish a true distance modulus $\\mu_{n}'$ via the luminosity distance equation:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mu\\ =\\ 5\\log\\left[(1+z)\\frac{1}{10\\ pc}\\int_{0}^{z}\\frac{dz'}{\\sqrt{\\Omega_{M}(1+z)^{3}+\\Omega_{k}(1+z)^{2}+\\Omega_{\\Lambda}}}\\right]\n",
    "\\end{equation*}\n",
    "\n",
    "[TO DO: Modify this for nontrivial $w_{0}$, $w_{a}$!  Luckily `asstropy` does this for us in code.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Planck 2015 results XIV. Dark energy and modified gravity - Figure 3\n",
    "true_H0 = 67.9\n",
    "true_Ode0 = 0.693\n",
    "true_Om0 = 1. - true_Ode0\n",
    "true_w0 = -1.09\n",
    "true_wa = -0.20\n",
    "true_hyperparams = np.array([true_w0, true_wa])\n",
    "n_hyperparams = len(true_hyperparams)\n",
    "#true_cosmo = cosmology.FlatLambdaCDM(H0=true_H0, Om0=true_Om0)\n",
    "true_cosmo = cosmology.w0waCDM(true_H0, true_Om0, true_Ode0, w0=true_w0, wa=true_wa)\n",
    "\n",
    "for n in range(n_sne):\n",
    "    true_params[n]['mu'] = true_cosmo.distmod(true_params[n]['z']).value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot a traditional Hubble diagram of the supernovae in our sample.\n",
    "\n",
    "*We must note that this is not a Hubble diagram like any other ever observed!  The distance moduli are of course not accessible for supernovae of types other than Ia.  However, non-Ia supernovae do still have a distance modulus, and do still follow the cosmological Hubble flow, so in simulated data, this is a perfectly reasonable quantity to visualize.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_x = [[d['z'] for d in true_params if d['t'] == types[t]] for t in range(n_types)]\n",
    "to_plot_y = [[d['mu'] for d in true_params if d['t'] == types[t]] for t in range(n_types)]\n",
    "for t in range(n_types):\n",
    "    plt.scatter(to_plot_x[t], to_plot_y[t], color=colors[t], marker='.', label=types[t], alpha=0.1)\n",
    "plt.plot(z_plot, [true_cosmo.distmod(z).value for z in z_plot], color='k', alpha=0.2)\n",
    "plt.xlabel(r'$z$')\n",
    "plt.ylabel(r'$\\mu$')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title(r'$w_{0}='+str(true_w0)+r', w_{a}='+str(true_wa)+r'$')\n",
    "#plt.savefig('plots/true_hubble.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating Probabilities\n",
    "\n",
    "`scippr` is intended to be used on interim posterior probabilities derived from a probabilistic lightcurve fitting procedure.  These will be provided as log-probabilities evaluated on a `3D` grid in type, redshift, and distance modulus space.  We choose to work with arrays of log-probabilities because they preserve numerical precision and enable slower products of arrays to be transformed into fast sums of arrays.  *This choice of parametrization for the input probabilities will be hard to change later on!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we will be working in log-space, it is important to define functions that ensure that the elements of the log-probability arrays do not result in `NaN` values and do not throw errors in the functions used to take logs.  We do this by defining a very small positive number, many orders of magnitude less than the limit of floating point precision, as the minimum probability allowed in our universe, as welln as its log, a fairly large negative number. We will make use of functions to check that these conditions are satisfied.  This introduces some imprecision but cannot be avoided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_log(arr, threshold=epsilon):\n",
    "    arr[arr < threshold] = threshold\n",
    "    return np.log(arr)\n",
    "\n",
    "def reg_vals(arr, threshold=log_epsilon):\n",
    "    arr[arr < threshold] = threshold\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We establish a binning in the space of $z$ and $\\mu$ for the arrays of log-probabilities.  This is arbitrary and can easily be changed.  This binning is also used for the redshift-dependent type proportions because they are currently implemented in a piecewise constant parametrization, so the inference will be slower with more bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this binning is arbitrary!\n",
    "n_zs = 101\n",
    "z_bins = np.linspace(min_z, max_z, num=n_zs, endpoint=True)\n",
    "z_difs = z_bins[1:] - z_bins[:-1]\n",
    "z_dif = np.mean(z_difs)\n",
    "z_mids = (z_bins[1:] + z_bins[:-1]) / 2.\n",
    "\n",
    "mu_lims = (true_cosmo.distmod(min_z).value, true_cosmo.distmod(max_z).value)\n",
    "\n",
    "# want this to be agnostic about true cosmology\n",
    "n_mus = 101\n",
    "(min_mu, max_mu) = mu_lims#mu_lims[0] - np.random.random(), mu_lims[1] + np.random.random()#min([s['mu'] for s in true_params]) - 0.5, max([s['mu'] for s in true_params]) + 0.5\n",
    "mu_bins = np.linspace(min_mu, max_mu, num=n_mus, endpoint=True)\n",
    "mu_difs = mu_bins[1:] - mu_bins[:-1]\n",
    "mu_dif = np.mean(mu_difs)\n",
    "mu_range = np.max(mu_bins) - np.min(mu_bins)\n",
    "print(mu_bins)\n",
    "mu_mids = (mu_bins[1:] + mu_bins[:-1]) / 2.\n",
    "\n",
    "z_mu_grid = np.array([[(z, mu) for mu in mu_mids] for z in z_mids])\n",
    "cake_shape = np.shape(z_mu_grid)\n",
    "unity = np.ones((n_sne, n_types, n_zs-1, n_mus-1))\n",
    "pmin, pmax = log_epsilon, np.log(1./(min(z_difs) * min(mu_difs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define additional functions to normalize probabilities so they integrate to unity over our $(t, z, \\mu)$ parameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unity_t = np.ones(n_types)\n",
    "def normalize_t(arr, vb=False):\n",
    "    norm_factor = np.sum(arr)\n",
    "    arr /= norm_factor\n",
    "    var = np.sum(arr)\n",
    "    try:\n",
    "        assert np.isclose(var, 1.)\n",
    "        if vb: print(var)\n",
    "    except AssertionError:\n",
    "        print('normalization error '+str(var))\n",
    "    return arr\n",
    "\n",
    "unity_z = np.ones(n_zs-1)\n",
    "def normalize_z(arr, vb=False):\n",
    "    norm_factor = np.sum(arr * z_difs)\n",
    "    arr /= norm_factor\n",
    "    var = np.sum(arr * z_difs)\n",
    "    try:\n",
    "        assert np.isclose(var, 1.)\n",
    "        if vb: print(var)\n",
    "    except AssertionError: \n",
    "        print('normalization error '+str(var))\n",
    "    return arr\n",
    "\n",
    "unity_zt = np.ones((n_types, n_zs-1))\n",
    "def normalize_zt(arr, vb=False):\n",
    "    norm_factor = np.sum(arr * z_difs[np.newaxis, :])\n",
    "    arr /= norm_factor\n",
    "    var = np.sum(arr * z_difs[np.newaxis, :])\n",
    "    try:\n",
    "        assert np.isclose(var, 1.)\n",
    "        if vb: print(var)\n",
    "    except AssertionError: \n",
    "        print('normalization error '+str(var))\n",
    "    return arr\n",
    "\n",
    "unity_hubble = np.ones((n_zs-1, n_mus-1))\n",
    "def normalize_hubble(arr, vb=False):\n",
    "    norm_factor = np.sum(arr * z_difs[:, np.newaxis] * mu_difs[np.newaxis, :])\n",
    "    arr /= norm_factor\n",
    "    var = np.sum(arr * z_difs[:, np.newaxis] * mu_difs[np.newaxis, :])\n",
    "    try: \n",
    "        assert np.isclose(var, 1.)\n",
    "        if vb: print(var)\n",
    "    except AssertionError: \n",
    "        print('normalization error '+str(var))\n",
    "    return arr\n",
    "\n",
    "unity_one = np.ones((n_types, n_zs-1, n_mus-1))\n",
    "def normalize_one(arr, vb=False):\n",
    "    norm_factor = np.sum(arr * z_difs[np.newaxis, :, np.newaxis] * mu_difs[np.newaxis, np.newaxis, :])\n",
    "    arr /= norm_factor\n",
    "    var = np.sum(arr * z_difs[np.newaxis, :, np.newaxis] * mu_difs[np.newaxis, np.newaxis, :])\n",
    "    try: \n",
    "        assert np.isclose(var, 1.)\n",
    "        if vb: print(var)\n",
    "    except AssertionError: \n",
    "        print('normalization error '+str(var))\n",
    "    return arr\n",
    "\n",
    "unity_all = np.ones((n_sne, n_types, n_zs-1, n_mus-1))\n",
    "def normalize_all(arr, vb=False):\n",
    "    n_objs = len(arr)\n",
    "    norm_factor = np.sum(np.sum(np.sum(arr * z_difs[np.newaxis, np.newaxis, :, np.newaxis] * mu_difs[np.newaxis, np.newaxis, np.newaxis, :], axis=3), axis=2), axis=1)\n",
    "    arr /= norm_factor[:, np.newaxis, np.newaxis, np.newaxis]\n",
    "    var = np.sum(np.sum(np.sum(arr * z_difs[np.newaxis, np.newaxis, :, np.newaxis] * mu_difs[np.newaxis, np.newaxis, np.newaxis, :], axis=3), axis=2), axis=1)\n",
    "    try:\n",
    "        assert np.all(np.isclose(var, np.ones(n_objs)))\n",
    "        if vb: print(var)\n",
    "    except AssertionError: \n",
    "        print('normalization error '+str(var))\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will construct the three-dimensional log probabilities produced by a probabilistic lightcurve fitter separately from the one-dimensional log-probabilities produced by a probabilistic redshift fitter and combine them at the end as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\ln[p(t_{i}, z_{i}, \\mu_{i} | \\underline{\\ell}_{i}, \\vec{f}_{i}, \\underline{\\alpha}, \\vec{\\beta}, \\underline{\\Phi}^{*}, \\vec{\\varphi}^{*})]\\ =\\ \\ln[p(t_{i}, z_{i}, \\mu_{i} | \\underline{\\ell}_{i}, \\underline{\\alpha}, \\underline{\\Phi}^{*})]\\ +\\ \\ln[p(t_{i}, z_{i}, \\mu_{i} | \\vec{f}_{i}, \\vec{\\beta}, \\vec{\\varphi}^{*})]\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supernova lightcurve probability distributions\n",
    "\n",
    "\\begin{equation*}\n",
    "\\ln[p(t_{i}, z_{i}, \\mu_{i} | \\underline{\\ell}_{i}, \\underline{\\alpha}, \\underline{\\Phi}^{*}]\\ =\\ \\ln[p(t_{i}, z_{i}, \\mu_{i} | \\underline{\\ell}_{i})]\\ +\\ \\ln[p(t_{i}, z_{i}, \\mu_{i} | \\underline{\\alpha})]\\ +\\ \\ln[p(t_{i}, z_{i}, \\mu_{i} | \\underline{\\Phi}^{*})]\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Supernova lightcurve log-posteriors**\n",
    "\n",
    "$\\ln[p(t, z, \\mu | \\underline{\\ell}_{i})]$\n",
    "\n",
    "Based on how existing lightcurve fitters work, a lightcurve is generally assigned a class before its redshift and distance modulus are estimated because the fitting function will differ based on the assigned class.  Thus, we may assume that the supernova lightcurve posterior is separable as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\ln[p(t, z, \\mu | \\underline{\\ell}_{i})] = \\ln[p(t | \\underline{\\ell}_{i})] + \\ln[p(z, \\mu | t, \\underline{\\ell}_{i})]\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Lightcurve classification*\n",
    "\n",
    "The confusion matrix quantifies the probabilities that an item is truly of a certain class given the fact that it has been classified as a different class.  (A more in-depth description of the confusion matrix can be found [here](https://github.com/rbiswas4/SNeLightcurveQualityMetric/blob/master/classification_metric.tex).)  We will use the $p(t'_{i} | t)$ elements of the confusion matrix to make $p(t | t'_{i})$ (via Bayes' rule), a proxy for $p(t | \\underline{\\ell}_{i})$ as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "p(t | t'_{i})\\ =\\ p(t'_{i} | t)\\ \\frac{p(t)}{p(t'_{i})}\n",
    "\\end{equation*}\n",
    "\n",
    "We recall that the sum across rows of the confusion matrix is $p(t')$ and the sum down columns is $p(t)$.\n",
    "\n",
    "The confusion matrix is specific to each classification method, so we will have to choose one to simulate a realistic mock dataset.  For now, we proceed assuming a fairly trivial confusion matrix giving a 50% chance of correct classification for each type and equal probabilities for all misclassifications.  TO DO: read this in from a file created by a realistic classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will need to take this from data of some kind, arbitrary for now\n",
    "conf_matrix = (0.25 + 0.25 * np.eye(3)) * frac_types[:, np.newaxis]\n",
    "true_rates = np.sum(conf_matrix, axis=1)\n",
    "obs_rates = np.sum(conf_matrix, axis=0)\n",
    "# print(conf_matrix, frac_types, true_rates, obs_rates)\n",
    "conf_matrix /= true_rates[:, np.newaxis]\n",
    "# print(conf_matrix)\n",
    "\n",
    "assert np.all(np.isclose(true_rates, frac_types))\n",
    "\n",
    "ln_conf_matrix = safe_log(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*$\\chi^{2}$ lightcurve parameter fitting*\n",
    "\n",
    "In order to produce $p(z, \\mu | t, \\underline{\\ell}_{i})$, we will again introduce the idea of using the true type $t_{i}'$ as a proxy for the lightcurve $\\underline{\\ell}_{i}$ and a classified type $t$ for the variable type $t$ that appears in the probability expressions.  If we do this, the quantity we want is really $p(z, \\mu | t, t'_{i})$.  We can obtain this knowing how lightcurve fitters, in general, estimate redshifts $z$ and distance moduli $\\mu$ under all possible combinations of $t_{i}'$ and $t$.  We will construct functions that aim to simulate the signatures of misclassification in the Hubble diagram [like the one here](http://iopscience.iop.org/article/10.1088/0004-637X/752/2/79/meta;jsessionid=620BA2E6C84CD7FD5AC88DDB72680B0C.ip-10-40-2-120#apj429245f3).  Currently, we use placeholder functions that will be replaced later on as information about the consequences of fitting with the wrong function becomes available.  The table below summarizes the fitting function for each true type given a classification of type $Ia$.  All other classified types are assumed to give a distribution that is uniform in $\\mu$ and Gaussian in $z$ according to the same prescription used for the photo-$z$ PDFs.\n",
    "\n",
    "| True Type $t_{i}'$ | Functional Form of posterior $p(z, \\mu | t, t'_{i})$ for $t=Ia$|\n",
    "| :----------------: | :------------------------------------------------------------: |\n",
    "| Ia | $\\vec{\\mathcal{N}}((z_{i}'', \\mu_{i}''), \\underline{\\Sigma}_{t_{i}'=Ia})$ where $\\underline{\\Sigma}_{t_{i}'=Ia}=(\\sigma^{2}_{z_{i},Ia}, \\sigma^{2}_{\\mu_{i},Ia})\\times\\underline{I}$ and $(z_{i}'', \\mu_{i}'')\\sim\\vec{\\mathcal{N}}((z_{i}', \\mu_{i}'), \\underline{\\Sigma}_{t_{i}'=Ia})$ |\n",
    "| Ibc | $\\vec{\\mathcal{N}}((z_{i}'', \\mu_{i}''), \\underline{\\Sigma}_{t_{i}'=Ibc})$ where $\\underline{\\Sigma}_{t_{i}'=Ibc}=(\\sigma^{2}_{z_{i},Ibc}, \\sigma^{2}_{\\mu_{i},Ibc})\\times\\underline{I}$ and $(z_{i}'', \\mu_{i}'')\\sim\\vec{\\mathcal{N}}((z_{i}', \\mu_{i}' - C_{Ibc}), \\underline{\\Sigma}_{t_{i}'=Ibc})$ for survey-wide constant $C_{Ibc}$ |\n",
    "| II | $\\vec{\\mathcal{N}}((z_{i}'', \\mu_{i}''), \\underline{\\Sigma}_{t_{i}'=II})$ where $\\underline{\\Sigma}_{t_{i}'=II}=(\\sigma^{2}_{z_{i},II}, \\sigma^{2}_{\\mu_{i},II})\\times\\underline{I}$ and $(z_{i}'', \\mu_{i}'')\\sim\\vec{\\mathcal{N}}((z_{i}', C_{II}), \\underline{\\Sigma}_{t_{i}'=II})$ for survey-wide constant $C_{II}$ |\n",
    "\n",
    "In a future revision, the values of $\\sigma^{2}_{z_{i},t}$ and $\\sigma^{2}_{\\mu_{i},t}$ for each type $t$ will be replaced by random variables themselves representing the intrinsic variation among lightcurves, but for now they are constant for all $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# must set nuisance parameters inherent in process of producing interim posteriors from lightcurves\n",
    "Ia_Ia_var = np.array([0.001, 0.02]) ** 2\n",
    "Ibc_Ia_delta = 0.25\n",
    "Ibc_Ia_var = np.array([0.001, 0.01]) ** 2\n",
    "II_Ia_delta = np.mean(mu_mids)\n",
    "II_Ia_var = np.array([0.002, 0.04]) ** 2\n",
    "\n",
    "# cake_shape = np.zeros((n_zs-1, n_mus-1))\n",
    "\n",
    "# definitely needs more work on what (z, mu) distributions are expected when lightcurves are fit with wrong templates\n",
    "# just made it flat for now\n",
    "\n",
    "def fit_Ia(z, mu, vb=False):\n",
    "#     cake = unity_hubble#np.zeros((n_types, n_zs-1, n_mus-1))\n",
    "    cake_Ia = sps.multivariate_normal(mean = np.array([z, mu]), cov = Ia_Ia_var * np.eye(2))\n",
    "    [z_samp, mu_samp] = cake_Ia.rvs()\n",
    "    cake_Ia = sps.multivariate_normal(mean = np.array([z_samp, mu_samp]), cov = Ia_Ia_var * np.eye(2))\n",
    "    cake = normalize_hubble(cake_Ia.pdf(z_mu_grid.reshape(((n_zs-1)*(n_mus-1), 2))).reshape((n_zs-1, n_mus-1)), vb=vb)\n",
    "    return cake\n",
    "    \n",
    "def fit_Ibc(z, mu, vb=False):\n",
    "#     cake = np.zeros((n_types, n_zs-1, n_mus-1))\n",
    "    cake_Ia = sps.multivariate_normal(mean = np.array([z, mu - Ibc_Ia_delta]), cov = Ibc_Ia_var * np.eye(2))\n",
    "    [z_samp, mu_samp] = cake_Ia.rvs()\n",
    "    cake_Ia = sps.multivariate_normal(mean = np.array([z_samp, mu_samp]), cov = Ibc_Ia_var * np.eye(2))\n",
    "    cake = normalize_hubble(cake_Ia.pdf(z_mu_grid.reshape(((n_zs-1)*(n_mus-1), 2))).reshape((n_zs-1, n_mus-1)), vb=vb)\n",
    "    return cake\n",
    "    \n",
    "def fit_II(z, mu, vb=False):\n",
    "#     cake = np.zeros((n_types, n_zs-1, n_mus-1))\n",
    "    cake_Ia = sps.multivariate_normal(mean = np.array([z, II_Ia_delta]), cov = II_Ia_var * np.eye(2))\n",
    "    [z_samp, mu_samp] = cake_Ia.rvs()\n",
    "    cake_Ia = sps.multivariate_normal(mean = np.array([z_samp, mu_samp]), cov = II_Ia_var * np.eye(2))\n",
    "    cake = normalize_hubble(cake_Ia.pdf(z_mu_grid.reshape(((n_zs-1)*(n_mus-1), 2))).reshape((n_zs-1, n_mus-1)), vb=vb)\n",
    "    return cake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_sigma = 0.03\n",
    "\n",
    "def fit_any(true_vals, vb=False):\n",
    "#     print(unity_one)\n",
    "    cake = np.zeros((n_types, n_zs-1, n_mus-1))#unity_one.copy()\n",
    "#     print(unity_one)\n",
    "    if true_vals['t'] == 'Ia':\n",
    "        cake[0] = fit_Ia(true_vals['z'], true_vals['mu'], vb=vb)\n",
    "        ln_conf = ln_conf_matrix[0]\n",
    "    if true_vals['t'] == 'Ibc':\n",
    "        cake[0] = fit_Ibc(true_vals['z'], true_vals['mu'], vb=vb)\n",
    "        ln_conf = ln_conf_matrix[1]\n",
    "    if true_vals['t'] == 'II':\n",
    "        cake[0] = fit_II(true_vals['z'], true_vals['mu'], vb=vb)\n",
    "        ln_conf = ln_conf_matrix[2]\n",
    "    if vb: print(np.exp(ln_conf))\n",
    "    dist = sps.norm(loc = true_vals['z'], scale = z_sigma)\n",
    "    z_means = dist.rvs(2)\n",
    "    layer_Ibc = sps.norm(loc = z_means[0], scale = z_sigma).pdf(z_mids)\n",
    "#     print(layer_Ibc)\n",
    "    layer_II = sps.norm(loc = z_means[1], scale = z_sigma).pdf(z_mids)\n",
    "#     print(layer_II)\n",
    "    cake[1] = normalize_hubble(unity_hubble * layer_Ibc[:, np.newaxis], vb=vb)\n",
    "    cake[2] = normalize_hubble(unity_hubble * layer_II[:, np.newaxis], vb=vb)\n",
    "#     cake = normalize_one(cake)\n",
    "    if not np.all(cake>=0.):\n",
    "        print(true_vals)\n",
    "        assert False\n",
    "    cake = reg_vals(safe_log(cake) + ln_conf[:, np.newaxis, np.newaxis])\n",
    "#     cake = safe_log(normalize_one(np.exp(cake)))\n",
    "    if vb: print(np.sum(np.sum(np.exp(cake) * z_difs[np.newaxis, :, np.newaxis] * mu_difs[np.newaxis, np.newaxis, :], axis=2), axis=1))\n",
    "    return cake\n",
    "\n",
    "def fit_all(catalog, vb=False):\n",
    "    dessert = []\n",
    "    i=0\n",
    "    for true_vals in catalog:\n",
    "        if vb: print(i)\n",
    "        thing = fit_any(true_vals, vb=vb)\n",
    "        try:\n",
    "            dessert.append(thing)\n",
    "        except AssertionError:\n",
    "            print('error '+str(thing))\n",
    "        i += 1\n",
    "    return np.array(dessert)\n",
    "\n",
    "sheet_cake = fit_all(true_params, vb=False)\n",
    "sheet_cake = reg_vals(safe_log(normalize_all(np.exp(sheet_cake), vb=False)))\n",
    "\n",
    "# assert(not np.any(np.isnan(hub_only_sheet_cake)))\n",
    "# sheet_cake = hub_only_sheet_cake + ln_conf_matrix[np.newaxis, :, np.newaxis, np.newaxis]\n",
    "# exp_sheet_cake = np.exp(sheet_cake)\n",
    "# sheet_cake_norm = np.sum(np.sum(np.sum(exp_sheet_cake * z_difs[np.newaxis, np.newaxis, :, np.newaxis] * mu_difs[np.newaxis, np.newaxis, np.newaxis, :], axis=3), axis=2), axis=1)\n",
    "# exp_sheet_cake /= sheet_cake_norm[:, np.newaxis, np.newaxis, np.newaxis]\n",
    "# # print(np.sum(exp_sheet_cake * z_difs[np.newaxis, np.newaxis, :, np.newaxis] * mu_difs[np.newaxis, np.newaxis, np.newaxis, :]))\n",
    "# assert np.isclose(np.sum(exp_sheet_cake * z_difs[np.newaxis, np.newaxis, :, np.newaxis] * mu_difs[np.newaxis, np.newaxis, np.newaxis, :]), n_sne)\n",
    "# sheet_cake = safe_log(exp_sheet_cake)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot a handful of these posteriors.  Each row is the log-posterior for a different supernova in the catalog.  The columns represent the log-posterior in the space of $z$ and $\\mu$ (Hubble diagram-space) for that supernova if it were classified as type $Ia$, type $Ibc$ and type $II$.  The true $z$ and $\\mu$ for each are also plotted.  The title of each panel gives the true type for each supernova.  We can see the effect on the posterior for $z$ and $\\mu$ based on each combination of true and classified type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp_sheet_cake = np.exp(sheet_cake)\n",
    "# pmin, pmax = 0., np.max(exp_sheet_cake)\n",
    "\n",
    "\n",
    "# happily, these look like what we see in contaminated hubble diagrams!\n",
    "fig = plt.subplots(figsize=(n_types*5, n_types*5))\n",
    "p = 0\n",
    "for s in posters:#range(n_sne)[:len(colors)]:\n",
    "    type_dist = np.sum(np.sum(np.exp(sheet_cake[s]) * mu_difs[np.newaxis, np.newaxis, :] * z_difs[np.newaxis, :, np.newaxis], axis=2), axis=1)\n",
    "    for t in range(n_types):\n",
    "        p += 1\n",
    "        plt.subplot(n_types, n_types, p)\n",
    "#         print(t, np.sum(np.exp(sheet_cake[s][t]) * z_difs[:, np.newaxis] * mu_difs[np.newaxis, :]))\n",
    "#         print(np.min(sheet_cake[s][t]), np.max(sheet_cake[s][t]), np.sum(sheet_cake[s][t]))\n",
    "        plt.pcolormesh(z_bins, mu_bins, sheet_cake[s][t].T, cmap='viridis', vmin=pmin, vmax=pmax)\n",
    "        plt.colorbar()#orientation='horizontal')\n",
    "        plt.scatter(true_params[s]['z'], true_params[s]['mu'], color='r')\n",
    "        plt.title(str(type_dist[t])+' true '+true_params[s]['t']+', class '+types[t])\n",
    "        plt.xlabel(r'$z$')\n",
    "        plt.ylabel(r'$\\mu$')\n",
    "        plt.axis([z_bins[0], z_bins[-1], mu_bins[0], mu_bins[-1]])\n",
    "\n",
    "plt.show()\n",
    "plt.savefig('plots/snlc_posteriors.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Supernova lightcurve selection function**\n",
    "\n",
    "$\\ln[p(t, z, \\mu | \\underline{\\alpha})]$\n",
    "\n",
    "We want to go directly to $p(t, z, \\mu | \\underline{\\alpha})$ knowing only $p(\\underline{\\ell} | \\vec{\\alpha})$ and $p(t, z, \\mu | \\underline{\\ell})$. We note that in the empirical approach, because $\\mu$ is determined by $z$ in the probailistic graphical model, we can say $p(t, z, \\mu | \\underline{\\alpha}) = p(t, z | \\underline{\\alpha}) = \\int\\ p(t, z | \\underline{\\ell})\\ p(\\underline{\\ell} | \\underline{\\alpha})\\ d\\underline{\\ell}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We emulate this using data from a realistic supernova simulation.\n",
    "# We want the number of supernovae as a function of redshift and SN type.\n",
    "# (The cadence studies with SNANA should have this.)\n",
    "# Using a realistic set of selection cuts, we want to calculate the recovered fraction\n",
    "# of SN as a function of SN type and redshift under a given survey strategy\n",
    "# relative to the \"best possible\" survey strategy.\n",
    "if not (os.path.exists('data/ratios_wfd.txt') and os.path.exists('data/ratios_ddf.txt')):\n",
    "    print('WARNING: No SN LC selection function data found in `data` directory, using flat SN LC selection function instead.')\n",
    "    sn_sel_fun_in = unity_zt.copy() # RH changed this\n",
    "    sn_sel_fun_in=sn_sel_fun_in[0:len(z_bins)]\n",
    "    #print(sn_sel_fun_in,len(z_bins))\n",
    "else:\n",
    "    with open('data/ratios_wfd.txt', 'r') as wfd_file:\n",
    "    #     wfd_file.next()\n",
    "        tuples = (line.split(None) for line in wfd_file)\n",
    "        wfddata = [[pair[k] for k in range(0,len(pair))] for pair in tuples]\n",
    "    n_sel_fun_zs = 6\n",
    "    zs_eval = np.array([float(wfddata[i][0]) for i in range(1, n_sel_fun_zs)])\n",
    "    wfd_data = np.array([np.array([int(wfddata[i][2 * j]) for j in range(1, (len(wfddata[i]))/2+1)]) for i in range(1, n_sel_fun_zs)])\n",
    "    wfd_data[np.isnan(wfd_data)] = 0.\n",
    "    # print(wfd_data)\n",
    "    with open('data/ratios_ddf.txt', 'r') as ddf_file:\n",
    "    #     ddf_file.next()\n",
    "        tuples = (line.split(None) for line in ddf_file)\n",
    "        ddfdata = [[pair[k] for k in range(0,len(pair))] for pair in tuples]\n",
    "    ddf_data = np.array([np.array([float(ddfdata[i][2 * j]) for j in range(1, (len(ddfdata[i]))/2+1)]) for i in range(1, n_sel_fun_zs)])\n",
    "    # print(ddf_data)\n",
    "    # these are the recovery rates\n",
    "    sn_sel_fun_in = np.transpose(wfd_data / ddf_data)\n",
    "    # sn_sel_fun = sn_sel_fun.T\n",
    "    # print(sn_sel_fun)\n",
    "# It's actually a big problem for the selection function to go to 0 or exceed 1.\n",
    "# Note: these need to be normalized by survey volume so are not valid at this time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to fix this to interpolate selection function to grid but broken for now\n",
    "# #purity = # real classified as real / all classified as real\n",
    "# #want real classified as real / all true real\n",
    "# #for now using purity as what we want\n",
    "# #should use really low purity for testing to check that it's doing something!\n",
    "# #need this to be # types * # z bins in shape\n",
    "sn_sel_fun_out = np.ones((n_types, n_zs-1))\n",
    "#print(np.shape(sn_sel_fun_out), np.shape(sn_sel_fun_in), zs_eval)\n",
    "#for t in range(n_types):\n",
    "#    interpolator = spi.interp1d(zs_eval, sn_sel_fun_in[t])\n",
    "#    interpolated = interpolator(z_mids)\n",
    "#    sn_sel_fun_out[t] = interpolated\n",
    "# uniform selection in type\n",
    "# sn_sel_fun = np.vstack((interpolated, interpolated, interpolated))#((interpolated, 1.*interpolated/3., 2.*interpolated/3.))\n",
    "#should divide this by DDF, for now say it's perfect\n",
    "# sn_sel_fun /= np.ones((n_types, n_zs))\n",
    "sn_sel_fun_out = np.ones((n_types, n_zs-1))\n",
    "sn_sel_fun_z = normalize_zt(sn_sel_fun_out, vb=True)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "for t in range(n_types):\n",
    "    plt.plot(z_mids, sn_sel_fun_z[t], label=types[t], color=colors[t])\n",
    "plt.xlabel(r'$z$')\n",
    "plt.ylabel('recovery fraction')\n",
    "plt.legend(loc='upper right')\n",
    "plt.savefig('plots/lc_sel_func.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#will need this for inference, should write to file and skeleton should read it in\n",
    "sn_sel_fun = normalize_one(sn_sel_fun_z[:, :, np.newaxis] * unity_one, vb=True)\n",
    "# sn_sel_fun_norm = np.sum(sn_sel_fun * z_difs[np.newaxis, :])\n",
    "# sn_sel_fun /= sn_sel_fun_norm\n",
    "# assert np.isclose(np.sum(sn_sel_fun * z_difs[np.newaxis, :]), 1.)\n",
    "ln_sn_selection_function = safe_log(sn_sel_fun)\n",
    "\n",
    "#ignore this because these are not probabilities!\n",
    "# sn_selection_function = np.ones((n_types, n_zs-1, n_mus-1))\n",
    "# sn_selection_function /= np.sum(sn_selection_function * z_difs[np.newaxis, :, np.newaxis] * mu_difs[np.newaxis, np.newaxis, :])\n",
    "# ln_sn_selection_function = safe_log(sn_selection_function)\n",
    "\n",
    "selfunmin, selfunmax = np.min(ln_sn_selection_function), np.max(ln_sn_selection_function)\n",
    "fig = plt.figure(figsize=(n_types*6, 5))\n",
    "p = 0\n",
    "for t in range(n_types):\n",
    "    p += 1\n",
    "    plt.subplot(1, n_types, p)\n",
    "    plt.pcolormesh(z_bins, mu_bins, ln_sn_selection_function[t].T, cmap='viridis', vmin=selfunmin, vmax=selfunmax)\n",
    "    plt.colorbar()\n",
    "    plt.title('log selection function for '+types[t])\n",
    "    plt.xlabel(r'$z$')\n",
    "    plt.ylabel(r'$\\mu$')\n",
    "#     plt.legend(loc='upper left')\n",
    "    plt.axis([z_bins[0], z_bins[-1], mu_bins[0], mu_bins[-1]])\n",
    "plt.savefig('plots/lc_sel_fun_full.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Supernova lightcurve interim prior**\n",
    "\n",
    "$\\ln[p(t, z, \\mu | \\underline{\\Phi}^{*})$\n",
    "\n",
    "To transform our posteriors into interim posteriors that accurately represent what we expect a real data analysis pipeline to produce, we must choose interim priors.  The interim prior represents the $p(t, z, \\mu)$ that is used in the estimation of log-posterior probabilities -- our assumptions about $p(t_{n}, z_{n}, \\mu_{n} | \\underline{\\ell}_{n})$ are parametrized by the interim prior parameters comprising $\\underline{\\Phi}^{*}$, so the closest we can get to the desired posteriors is the interim posteriors $\\ln[p(t_{n}, z_{n}, \\mu_{n} | \\underline{\\ell}_{n}, \\underline{\\Phi}^{*})].\n",
    "\n",
    "This section is about making the interim prior $\\ln[p(t, z, \\mu | \\underline{\\xi})]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# could replace this with non-flat\n",
    "# SN interim prior in z-dependent type proportion space\n",
    "sn_interim_nz = np.ones((n_types, n_zs-1))#this is flat, replace this with nontrivial interim prior on types and redshifts\n",
    "# sn_interim_nz /= np.sum(sn_interim_nz * z_difs[np.newaxis, :])\n",
    "# assert np.isclose(np.sum(sn_interim_nz * z_difs[np.newaxis, :]), 1.)\n",
    "sn_interim_nz = normalize_zt(sn_interim_nz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(n_types):\n",
    "    plt.plot(z_mids, sn_interim_nz[t], label=types[t], color=colors[t])\n",
    "plt.xlabel(r'$z$')\n",
    "plt.ylabel('redshift-dependent SN type proportions interim prior')\n",
    "plt.legend(loc='upper right')\n",
    "plt.savefig('plots/lc_int_pr.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn_interim_nz = normalize_one(unity_one * sn_interim_nz[:, :, np.newaxis])\n",
    "ln_sn_interim_nz = safe_log(sn_interim_nz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flat prior on redshift-dependent SNe proportions, replace with SDSS n(z)\n",
    "# interim_n_of_z = np.ones((n_types, n_zs-1))\n",
    "# interim_n_of_z /= np.sum(interim_n_of_z * z_difs[np.newaxis, :])\n",
    "# assert np.isclose(np.sum(interim_n_of_z * z_difs[np.newaxis, :]), 1.)\n",
    "# interim_ln_n_of_z = safe_log(interim_n_of_z)\n",
    "\n",
    "# read these off Planck plots\n",
    "interim_H0 = true_H0#70.0\n",
    "delta_H0 = 2.2# * 10.\n",
    "interim_Om0 = true_Om0#1. - 0.721\n",
    "delta_Om0 = 0.025# * 10.\n",
    "interim_Ode0 = true_Ode0\n",
    "delta_Ode0 = 0.025 * 10.\n",
    "interim_w0 = -1.0#true_w0\n",
    "delta_w0 = 0.05#0.2# * 10.\n",
    "interim_wa = 0.0#true_wa\n",
    "delta_wa = 0.05#1.5# * 10.\n",
    "interim_cosmo_hyperparams = np.array([interim_w0, interim_wa])\n",
    "interim_cosmo_hyperparam_sigmas = np.array([delta_w0, delta_wa])\n",
    "interim_cosmo_hyperparam_vars = (interim_cosmo_hyperparam_sigmas) * np.eye(n_hyperparams)\n",
    "interim_dist = sps.multivariate_normal(mean = interim_cosmo_hyperparams, cov = interim_cosmo_hyperparam_vars)\n",
    "interim_cosmo = cosmology.w0waCDM(interim_H0, interim_Om0, interim_Ode0, w0=interim_w0, wa=interim_wa)\n",
    "def inverter(z, mu):\n",
    "    #note: this inverter is slow! perhaps we could speed it up with interpolation over predefined grids?\n",
    "    def cosmo_helper(hyperparams):\n",
    "        return np.array([abs(cosmology.w0waCDM(interim_H0, interim_Om0, interim_Ode0, w0=hyperparams[0], wa=hyperparams[1]).distmod(z).value - mu)])\n",
    "    solved_cosmo = spo.minimize(cosmo_helper, interim_cosmo_hyperparams, method=\"Nelder-Mead\", options={\"maxfev\": 1e5, \"maxiter\":1e5})\n",
    "    prob = interim_dist.pdf(solved_cosmo.x)\n",
    "    return prob#max(prob, sys.float_info.epsilon)\n",
    "\n",
    "# interim_H0 = true_H0#70.0\n",
    "# delta_H0 = 2.2# * 10.\n",
    "# interim_Om0 = true_Om0#1. - 0.721\n",
    "# delta_Om0 = 0.025#*10\n",
    "# interim_cosmo_hyperparams = np.array([interim_H0, interim_Om0])\n",
    "# interim_cosmo_hyperparam_sigmas = np.array([delta_H0, delta_Om0])\n",
    "# interim_cosmo_hyperparam_vars =  interim_cosmo_hyperparam_sigmas * np.eye(n_hyperparams)\n",
    "# interim_dist = sps.multivariate_normal(mean = interim_cosmo_hyperparams, cov = interim_cosmo_hyperparam_vars)\n",
    "# interim_cosmo = cosmology.FlatLambdaCDM(H0=interim_H0, Om0=interim_Om0)\n",
    "# def inverter(z, mu):\n",
    "#     #note: this inverter is slow! perhaps we could speed it up with interpolation over predefined grids?\n",
    "#     def cosmo_helper(hyperparams):\n",
    "#         return np.array([abs(cosmology.FlatLambdaCDM(H0=hyperparams[0], Om0=hyperparams[1]).distmod(z).value - mu)])\n",
    "#     solved_cosmo = spo.minimize(cosmo_helper, interim_cosmo_hyperparams, method=\"Nelder-Mead\", options={\"maxfev\": 1e5, \"maxiter\":1e5})\n",
    "#     prob = interim_dist.pdf(solved_cosmo.x)\n",
    "#     assert(not np.isnan(prob))\n",
    "#     return prob#max(prob, sys.float_info.epsilon)\n",
    "\n",
    "# may have to change this if nontrivial covariances between hyperparameters=\n",
    "#interim_hyperparams['theta'] = np.array([interim_cosmo_hyperparams, interim_cosmo_hyperparam_sigmas])\n",
    "#interim_hyperparams['phi'] = interim_ln_n_of_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SN interim prior in Hubble diagram space\n",
    "sn_interim_hubble = np.zeros((n_zs-1, n_mus-1))\n",
    "for z in range(n_zs-1):\n",
    "    for mu in range(n_mus-1):\n",
    "        prob = inverter(z_mids[z], mu_mids[mu])\n",
    "        sn_interim_hubble[z][mu] = prob\n",
    "# print(np.sum(sn_interim_hubble * z_difs[:, np.newaxis] * mu_difs[np.newaxis, :]))\n",
    "sn_interim_hubble = normalize_hubble(sn_interim_hubble)\n",
    "ln_sn_interim_hubble = safe_log(sn_interim_hubble)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(5, 5))\n",
    "# plt.pcolormesh(z_bins, mu_bins, ln_sn_interim_hubble.T, cmap='viridis')\n",
    "# plt.plot(z_bins, [true_cosmo.distmod(z).value for z in z_bins], color='k', label='true Hubble relation')\n",
    "# plt.plot(z_bins, [interim_cosmo.distmod(z).value for z in z_bins], color='r', label='interim Hubble relation')\n",
    "# plt.colorbar()\n",
    "# plt.title('log interim cosmological prior')\n",
    "# plt.xlabel(r'$z$')\n",
    "# plt.ylabel(r'$\\mu$')\n",
    "# plt.legend(loc='upper left')\n",
    "# plt.axis([z_bins[0], z_bins[-1], mu_bins[0], mu_bins[-1]])\n",
    "# plt.savefig('plots/lc_interim_w0waCDM.png')\n",
    "\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "plt.pcolormesh(z_bins, mu_bins, ln_sn_interim_hubble.T, cmap='viridis', vmin=pmin, vmax=pmax)\n",
    "plt.plot(z_bins, [true_cosmo.distmod(z).value for z in z_bins], color='k', label='true Hubble relation')\n",
    "plt.plot(z_bins, [interim_cosmo.distmod(z).value for z in z_bins], color='r', label='interim Hubble relation')\n",
    "plt.colorbar()\n",
    "plt.title('log interim cosmological prior')\n",
    "plt.xlabel(r'$z$')\n",
    "plt.ylabel(r'$\\mu$')\n",
    "plt.legend(loc='upper left')\n",
    "plt.axis([z_bins[0], z_bins[-1], mu_bins[0], mu_bins[-1]])\n",
    "plt.savefig('plots/lc_interim_FlatLambdaCDM.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn_interim_hubble = normalize_one(unity_one * sn_interim_hubble[np.newaxis, :, :])\n",
    "ln_sn_interim_hubble = safe_log(sn_interim_hubble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln_sn_interim = reg_vals(ln_sn_interim_nz + ln_sn_interim_hubble)\n",
    "sn_interim = np.exp(ln_sn_interim)\n",
    "ln_sn_interim = safe_log(normalize_one(sn_interim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(n_types*6, 5))\n",
    "p = 0\n",
    "for t in range(n_types):\n",
    "    p += 1\n",
    "    plt.subplot(1, n_types, p)\n",
    "    plt.pcolormesh(z_bins, mu_bins, ln_sn_interim[t].T, cmap='viridis', vmin=pmin, vmax=pmax)\n",
    "    plt.plot(z_bins, [true_cosmo.distmod(z).value for z in z_bins], color='k', label='true Hubble relation')\n",
    "    plt.plot(z_bins, [interim_cosmo.distmod(z).value for z in z_bins], color='r', label='interim Hubble relation')\n",
    "    plt.colorbar()\n",
    "    plt.title('log interim prior for type '+types[t])\n",
    "    plt.xlabel(r'$z$')\n",
    "    plt.ylabel(r'$\\mu$')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.axis([z_bins[0], z_bins[-1], mu_bins[0], mu_bins[-1]])\n",
    "plt.savefig('plots/lc_interim_combo.png')\n",
    "\n",
    "# plt.pcolormesh(z_bins, mu_bins, ln_sn_interim[0].T, cmap='viridis')#, vmin = 0., vmax = 3.)\n",
    "# plt.plot(z_bins, [true_cosmo.distmod(z).value for z in z_bins], color='k', label='true Hubble relation')\n",
    "# plt.title('interim prior distribution')\n",
    "# plt.xlabel(r'$z$')\n",
    "# plt.ylabel(r'$\\mu$')\n",
    "# plt.legend(loc='lower right', fontsize='small')\n",
    "# plt.axis([z_bins[0], z_bins[-1], mu_bins[0], mu_bins[-1]])\n",
    "# plt.colorbar()\n",
    "# plt.savefig('plots/sn_interim_prior.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Host galaxy photometry probability distributions\n",
    "\n",
    "\\begin{equation*}\n",
    "\\ln[p(t_{i}, z_{i}, \\mu_{i} | \\vec{f}_{i}, \\vec{\\beta}, \\vec{\\varphi}^{*})]\\ =\\ \\ln[p(t_{i}, z_{i}, \\mu_{i} | \\vec{f}_{i})]\\ +\\ \\ln[p(t_{i}, z_{i}, \\mu_{i} | \\vec{\\beta})]\\ +\\ \\ln[p(t_{i}, z_{i}, \\mu_{i} | \\vec{\\varphi}^{*})]\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Host galaxy photometry log-posteriors**\n",
    "\n",
    "$\\ln[p(t, z, \\mu | \\vec{f}_{i})]$\n",
    "\n",
    "It is simplest to start with the log-posterior $\\ln[p(z | \\vec{f}_{i})]$ of host galaxy photometry $\\vec{f}_{i}$ as a function of redshift $z$.  In the absence of photometry, we will simulate this as $p(z | z'_{i})$.  We assume the simplest model in which photo-$z$ PDFs are Gaussians $\\mathcal{N}(z_{i}'', \\sigma_{i}^{2})$ where $z_{i}''\\sim\\mathcal{N}(z_{i}', \\sigma_{n}^{2})$.  We will also state that the variance is a constant $\\sigma_{i}\\equiv\\sigma$ for all host galaxies $i$.  [cite where 0.03 came from]  TO DO: allow $\\sigma_{i}^{2}$ to vary for realistic tests.\n",
    "\n",
    "*The choice of a Gaussian photo-$z$ PDF model is irrelevant -- any continuous function or linear combination thereof, as well as any discrete distribution, can be converted to the binned parametrization used here.  The cell below could easily be replaced with one that reads in more realistically modeled photo-$z$ likelihoods from a file.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# very simple p(z) model, simple gaussians, but binned parametrization permits arbitrary shapes\n",
    "pz_sigma = 0.03\n",
    "\n",
    "pzs, ln_pzs = [], []\n",
    "for s in range(n_sne):\n",
    "    dist = sps.norm(loc = true_params[s]['z'], scale = pz_sigma)\n",
    "    pz_mean = dist.rvs()\n",
    "    new_dist = sps.norm(loc = pz_mean, scale = pz_sigma)\n",
    "    pz = new_dist.pdf(z_mids)\n",
    "    #ln_pz = new_dist.logpdf(z_mids)\n",
    "    pzs.append(pz)\n",
    "    #ln_pzs.append(ln_pz)\n",
    "pzs = np.array(pzs)\n",
    "pzs = normalize_z(pzs)\n",
    "ln_pzs = safe_log(pzs)#np.array(ln_pzs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in range(n_sne)[:len(colors)-1]:\n",
    "    plt.step(z_bins[1:], pzs[s], color=colors[s], alpha=0.5)\n",
    "    plt.vlines(true_params[s]['z'], 0., max(pzs[s]), color=colors[s])\n",
    "plt.xlabel(r'$z$')\n",
    "plt.ylabel(r'$p(z)$')\n",
    "plt.title(r'host galaxy $p(z)$ distributions')\n",
    "plt.savefig('plots/host_pzs.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Host galaxy photometry selection function**\n",
    "\n",
    "$\\ln[p(t, z, \\mu | \\vec{\\beta})]$\n",
    "\n",
    "The selection function can be represented by $\\ln[p(z | \\vec{\\beta})]$.  A selection function in which $\\vec{\\beta}$ is comprised of magnitude limits in all photometric bands is commonly imposed on galaxy surveys, such that $p(\\vec{f} | \\vec{\\beta})$ is known (and often is a step function in as many dimensions as there are filters).  Furthermore, we have a reasonably good idea of what $p(z | \\vec{f})$ is.  The selection function we want is just $\\int\\ p(z | \\vec{f})\\ p(\\vec{f} | \\vec{\\beta})\\ d\\vec{f}$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We emulate this using data from a realistic galaxy simulation.\n",
    "# We want the number of galaxies as a function of redshift, SED type, and luminosity.\n",
    "# (Buzzard, for example, includes this.)\n",
    "# Using a realistic set of magnitude limits, we want to calculate the recovered fraction\n",
    "# of galaxies as a function of redshift, SED type, and luminosity.\n",
    "# Then we just integrate over SED type and luminosity to get $p(z | \\vec{\\beta})$\n",
    "\n",
    "# pz_selfun = np.ones(n_zs-1)\n",
    "# pz_selfun /= np.sum(pz_selfun * z_difs)\n",
    "# assert np.isclose(np.sum(pz_selfun * z_difs), 1.)\n",
    "# ln_pz_selfun = safe_log(pz_selfun)\n",
    "\n",
    "host_sel_fun = np.ones(n_zs-1)\n",
    "host_sel_fun = normalize_z(host_sel_fun)\n",
    "# host_sel_fun_norm = np.sum(host_sel_fun * z_difs)\n",
    "# host_sel_fun /= host_sel_fun_norm\n",
    "# assert np.isclose(np.sum(host_sel_fun * z_difs), 1.)\n",
    "ln_host_selection_function = safe_log(host_sel_fun)\n",
    "\n",
    "#interim_hyperparams['pz_sel_fun'] = ln_host_selection_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(z_mids, host_sel_fun, color='k')\n",
    "plt.xlabel(r'$z$')\n",
    "plt.ylabel(r'$p(z)$')\n",
    "plt.title(r'host galaxy redshift selection function')\n",
    "plt.savefig('plots/pz_sel_fun.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Host galaxy photometry interim prior**\n",
    "\n",
    "$\\ln[p(t, z, \\mu | \\vec{\\varphi}^{*})]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the SDSS DR7 one instead\n",
    "# separate interim prior from LC fitter and photo-z PDFs: this is for photo-z PDFs, flat for now, replace with SDSS n(z)\n",
    "pz_interim = np.ones(n_zs-1)\n",
    "pz_interim = normalize_z(pz_interim)\n",
    "# pz_interim /= np.sum(pz_interim * z_difs)\n",
    "# assert np.isclose(np.sum(pz_interim * z_difs), 1.)\n",
    "ln_pz_interim = safe_log(pz_interim)\n",
    "\n",
    "#interim_hyperparams = {}\n",
    "#interim_hyperparams['pz_int_pr'] = ln_pz_interim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(z_mids, pz_interim, color='k')\n",
    "plt.xlabel(r'$z$')\n",
    "plt.ylabel(r'$p(z)$')\n",
    "plt.title(r'interim redshift distribution function')\n",
    "plt.savefig('plots/pz_interim_prior.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting everything together\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln_host_probs = reg_vals(ln_pzs + ln_host_selection_function[np.newaxis, :] + ln_pz_interim[np.newaxis, :])\n",
    "host_probs = np.exp(ln_host_probs)[:, np.newaxis, :, np.newaxis] * unity_all\n",
    "host_probs = normalize_all(host_probs)\n",
    "ln_host_probs = safe_log(host_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for s in range(n_sne):\n",
    "#     for hub in host_probs[s]:\n",
    "#         print(true_params[s], np.sum(hub * z_difs[:, np.newaxis] * mu_difs[np.newaxis, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln_sn_probs = reg_vals(sheet_cake + ln_sn_selection_function[np.newaxis, :] + ln_sn_interim[np.newaxis, :])\n",
    "sn_probs = np.exp(ln_sn_probs)\n",
    "sn_probs = normalize_all(sn_probs)\n",
    "ln_sn_probs = safe_log(sn_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for s in range(n_sne):\n",
    "#     for hub in sn_probs[s]:\n",
    "#         print(true_params[s], np.sum(hub * z_difs[:, np.newaxis] * mu_difs[np.newaxis, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interim_ln_posteriors = reg_vals(ln_host_probs + ln_sn_probs)\n",
    "interim_posts = np.exp(interim_ln_posteriors)\n",
    "interim_posts = normalize_all(interim_posts)\n",
    "interim_ln_posteriors = safe_log(interim_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for s in range(n_sne):\n",
    "#     for hub in interim_posts[s]:\n",
    "#         print(true_params[s], np.sum(hub * z_difs[:, np.newaxis] * mu_difs[np.newaxis, :]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write this to a file for the inference notebook to read in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write true hyperparameters just to check\n",
    "\n",
    "sn_id = ['CID_%i'%n for n in np.arange(0,n_sne,1)]\n",
    "\n",
    "truth = {}\n",
    "binned_n_of_z = np.zeros((n_types, n_zs-1))\n",
    "for t in range(n_types):\n",
    "    cdfs = true_n_of_z[t].cdf(z_bins)\n",
    "    binned_n_of_z[t] = (cdfs[1:] - cdfs[:-1])\n",
    "binned_n_of_z = frac_types[:, np.newaxis] * np.array(binned_n_of_z)# / z_range\n",
    "binned_n_of_z /= np.sum(binned_n_of_z * z_difs[np.newaxis, :])\n",
    "assert np.isclose(np.sum(binned_n_of_z * z_difs[np.newaxis, :]), 1.)\n",
    "truth['phi'] = binned_n_of_z\n",
    "truth['theta'] = true_hyperparams\n",
    "truth['data'] = true_params\n",
    "truth['id']=sn_id \n",
    "                     \n",
    "with open('data/truth.hkl', 'w') as true_file:\n",
    "    hickle.dump(truth, true_file)\n",
    "\n",
    "# write axes (types, z_bins, mu_bins)\n",
    "# write interim prior (interim_ln_prior)\n",
    "# write interim posteriors (ln_interim_posteriors)\n",
    "\n",
    "\n",
    "\n",
    "output = {'types': types, 'z_bins': z_bins, 'mu_bins': mu_bins}\n",
    "output['ln host selection function'] = ln_host_selection_function\n",
    "output['host interim ln prior'] = ln_pz_interim\n",
    "output['ln sn selection function'] = ln_sn_selection_function\n",
    "output['sn interim ln prior'] = ln_sn_interim\n",
    "output['ln selection function'] = safe_log(normalize_one(np.exp(reg_vals(output['ln host selection function'][np.newaxis, :, np.newaxis] + output['ln sn selection function']))))\n",
    "output['interim ln prior'] = safe_log(normalize_one(np.exp(reg_vals(output['host interim ln prior'][np.newaxis, :, np.newaxis] + output['sn interim ln prior']))))\n",
    "output['ln prior info'] = safe_log(normalize_one(np.exp(reg_vals(output['interim ln prior'] + output['ln selection function']))))\n",
    "output['interim ln posteriors'] = interim_ln_posteriors\n",
    "output['id'] = sn_id\n",
    "\n",
    "with open('data/data.hkl', 'w') as out_file:\n",
    "    hickle.dump(output, out_file)\n",
    "    \n",
    "    \n",
    "true_id=truth['id']\n",
    "true_zs = [true_param['z'] for true_param in true_params]\n",
    "true_mus = [true_param['mu'] for true_param in true_params]\n",
    "\n",
    "for s in range(n_sne):\n",
    "    print(true_id[s], true_zs[s], true_mus[s], sn_id[s], z_bins[s], mu_bins[s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what these final results look like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are going to get a lot narrower\n",
    "fig = plt.figure(figsize=(n_types*5, n_types*5))\n",
    "p = 0\n",
    "for s in posters:\n",
    "    print(true_id[s], true_zs[s], true_mus[s], sn_id[s], z_bins[s], mu_bins[s])\n",
    "    type_dist = np.sum(np.sum(np.exp(interim_ln_posteriors[s]) * mu_difs[np.newaxis, np.newaxis, :] * z_difs[np.newaxis, :, np.newaxis], axis=2), axis=1)\n",
    "    for t in range(n_types):\n",
    "        p += 1\n",
    "        plt.subplot(n_types, n_types, p)\n",
    "        plt.pcolormesh(z_bins, mu_bins, interim_ln_posteriors[s][t].T, cmap='viridis', vmin = pmin, vmax = pmax)\n",
    "        plt.colorbar()\n",
    "        plt.scatter(true_params[s]['z'], true_params[s]['mu'], color='k', marker='o')\n",
    "        plt.scatter(true_zs[s], true_mus[s], color='r', marker='s')\n",
    "        plt.title(str(type_dist[t])+' true '+true_params[s]['t']+', class '+types[t])\n",
    "        plt.xlabel(r'$z$')\n",
    "        plt.ylabel(r'$\\mu$')\n",
    "        plt.axis([z_bins[0], z_bins[-1], mu_bins[0], mu_bins[-1]])\n",
    "plt.savefig('plots/out_interim_posteriors.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ignore everything after this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the combined likelihoods for when lightcurves and host galaxy photometry are available.  For the same supernovae as in the previous plot, you can see how the constraints in $z$ become much narrower when the redshift likelihoods are included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should be visible in all types, but the color scaling is bad for Ibc & II\n",
    "fig = plt.figure(figsize=(n_types*5, len(colors)*5))\n",
    "p = 0\n",
    "for s in range(n_sne)[:len(colors)]:\n",
    "    for t in range(n_types):\n",
    "        p += 1\n",
    "        plt.subplot(len(colors), n_types, p)\n",
    "        plt.pcolormesh(z_bins, mu_bins, ln_likelihoods[s][t].T, cmap='viridis')#, vmin = 0., vmax = 3.)\n",
    "        plt.colorbar()\n",
    "        plt.scatter(true_params[s]['z'], true_params[s]['mu'], color='r')\n",
    "        plt.title('true '+true_params[s]['t']+', class '+types[t])\n",
    "        plt.xlabel(r'$z$')\n",
    "        plt.ylabel(r'$\\mu$')\n",
    "        plt.axis([z_bins[0], z_bins[-1], mu_bins[0], mu_bins[-1]])\n",
    "plt.savefig('plots/full_posteriors.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making interim posteriors\n",
    "\n",
    "The likelihoods we just constructed are all well and good, and they really do exist in nature.  However, they are not things we observers are in general able to obtain.  To use them in inference, we would need to regularly integrate over the entire space of data, which is rarely something we know how to do.  When we run a code that produces the probability distribution of unobservable parameters, it really calculates a posterior, not a likelihood; because its inputs are data, it must be conditioning its estimate on that information.  \n",
    "\n",
    "In addition to that, any way of estimating our unobservable parameters from data also conditions their posteriors on other information, namely interim priors and selection functions.  In reality, when we perform classification, lightcurve fitting, and photo-$z$ PDF estimation, we are finding interim posteriors $p(t_{n}, z_{n}, \\mu_{n} | \\underline{\\ell}_{n}, \\vec{f}_{n}, \\underline{\\xi}, \\vec{\\alpha}, \\vec{\\beta})$ instead of likelihoods $p(\\underline{\\ell}_{n}, \\vec{f}_{n} | t_{n}, z_{n}, \\mu_{n})$, due to the assumptions about the distributions of our latent variables and the propagation of selection effects on the data.  In our case, choices of the interim hyperpriors $\\underline{\\xi}$ will translate directly into a prior belief about the `3D` distribution $p(t, z, \\mu | \\underline{\\xi}, \\vec{\\alpha}, \\vec{\\beta})$ that is independent of any observations (and thus independent of $n$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must now define a function that produces the posterior probability of a supernova taking the value of a particular pair $(z, \\mu)$ under a given set of cosmological parameters $\\vec{\\Omega}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this function, we can construct an interim prior probability, which has the dimensions of a single `3D` probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interim_ln_prior = ln_sn_interim + ln_pz_interim[np.newaxis, :, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # note the approximation of cdf[z_min, z_max] = pdf[z_mid]\n",
    "# interim_sheet = np.zeros((n_zs-1, n_mus-1))\n",
    "# for z in range(n_zs-1):\n",
    "#     for mu in range(n_mus-1):\n",
    "#         ln_prob = inverter(z_mids[z], mu_mids[mu])\n",
    "#         interim_sheet[z][mu] = ln_prob\n",
    "# interim_ln_prior = reg_vals(interim_ln_n_of_z[:, np.newaxis] + interim_sheet[np.newaxis, :])\n",
    "# interim_prior = np.exp(interim_ln_prior)\n",
    "# interim_prior /= np.sum(interim_prior * z_difs[np.newaxis, :, np.newaxis] * mu_difs[np.newaxis, np.newaxis, :])\n",
    "# interim_ln_prior = safe_log(interim_prior)\n",
    "# assert np.isclose(np.sum(interim_prior * z_difs[np.newaxis, :, np.newaxis] * mu_difs[np.newaxis, np.newaxis, :]), 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the log of the combined interim prior and show that it is reasonable but not too restrictive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # the interim prior and truth are way too close to each other. . . but that's realistic\n",
    "# plt.pcolormesh(z_bins, mu_bins, interim_ln_prior[0].T, cmap='viridis')#, vmin = 0., vmax = 3.)\n",
    "# plt.plot(z_bins, [true_cosmo.distmod(z).value for z in z_bins], color='k', label='true Hubble relation')\n",
    "# plt.title('interim prior distribution')\n",
    "# plt.xlabel(r'$z$')\n",
    "# plt.ylabel(r'$\\mu$')\n",
    "# plt.legend(loc='lower right', fontsize='small')\n",
    "# plt.axis([z_bins[0], z_bins[-1], mu_bins[0], mu_bins[-1]])\n",
    "# plt.colorbar()\n",
    "# plt.savefig('plots/interim_prior.png')\n",
    "fig = plt.figure(figsize=(n_types*6, 5))\n",
    "p = 0\n",
    "for t in range(n_types):\n",
    "    p += 1\n",
    "    plt.subplot(1, n_types, p)\n",
    "    plt.pcolormesh(z_bins, mu_bins, interim_ln_prior[t].T, cmap='viridis')\n",
    "    plt.plot(z_bins, [true_cosmo.distmod(z).value for z in z_bins], color='k', label='true Hubble relation')\n",
    "    plt.plot(z_bins, [interim_cosmo.distmod(z).value for z in z_bins], color='r', label='interim Hubble relation')\n",
    "    plt.colorbar()\n",
    "    plt.title('log interim prior')\n",
    "    plt.xlabel(r'$z$')\n",
    "    plt.ylabel(r'$\\mu$')\n",
    "    plt.axis([z_bins[0], z_bins[-1], mu_bins[0], mu_bins[-1]])\n",
    "plt.savefig('plots/full_interim_prior.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection Functions\n",
    "\n",
    "We have not yet included selection effects in this treatment.  The graphical model included known hyperparameters $\\vec{\\alpha}$ and $\\vec{\\beta}$ to represent the selection function for supernova lightcurves and host galaxy photometry.  If these hyperparameters are known, then we can safely say we know $p(\\underline{\\ell}_{n} | \\vec{\\alpha})$ and $p(\\vec{f}_{n} | \\vec{\\beta})$ for all supernovae $n$ in our catalog.  However, what we really need is $p(t_{n}, z_{n}, \\mu_{n} | \\vec{\\alpha}, \\vec{\\beta})$.  We obtain these using an approach similar to <a href=\"https://github.com/ixkael/Photoz-tools/blob/master/Photoz%20galaxy%20survey%20mock%20and%20N(z)%20inference.ipynb\">that of Boris Leistedt</a>.  *Actually, I'm not really sure how to relate what we're doing, which we discussed with Boris, to what's in his notebook.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wrap up the simulation/emulation of the selection functions by combining the terms as follows: $\\ln[p(t, z, \\mu | \\vec{\\alpha}, \\vec{\\beta})] = \\ln[p(t, z, \\mu | \\vec{\\alpha})] + \\ln[p(z | \\vec{\\beta})]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put together those terms\n",
    "\n",
    "selection_sheet = np.zeros((n_types, n_zs-1, n_mus-1))\n",
    "ln_selection_function = reg_vals(ln_sn_selection_function[:, np.newaxis] + ln_host_selection_function[np.newaxis, :, np.newaxis] + selection_sheet)\n",
    "selection_function = np.exp(ln_selection_function)\n",
    "sel_fun_norm = np.sum(selection_function * z_difs[np.newaxis, :, np.newaxis] * mu_difs[np.newaxis, np.newaxis, :] / n_types)\n",
    "selection_function /= sel_fun_norm\n",
    "assert np.isclose(np.sum(selection_function * z_difs[np.newaxis, :, np.newaxis] * mu_difs[np.newaxis, np.newaxis, :] / n_types), 1.)\n",
    "ln_selction_function = safe_log(selection_function)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (not)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
